{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing of additive state functionals via variational approximations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codebase\n",
    "*You can fold all this part but you need to run it if you want to run experiments below*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "\n",
    "*Uncommented an run this cell if you want to relaunch all cells in Colab*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from copy import deepcopy\n",
    "\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "from jax import lax, numpy as jnp, random, config, jit, vmap, value_and_grad\n",
    "from jax.scipy.stats.multivariate_normal import logpdf as jax_gaussian_logpdf\n",
    "import optax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from backward_ica.elbo import linear_gaussian_elbo\n",
    "from backward_ica.hmm import LinearGaussianHMM\n",
    "from backward_ica.kalman import filter as kalman_filter, smooth as kalman_smooth\n",
    "from backward_ica.misc import parameters_from_raw_parameters\n",
    "\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "key = random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameters_from_raw_parameters(raw_model):\n",
    "\n",
    "    model = deepcopy(raw_model)\n",
    "    \n",
    "    model['prior']['cov'] = jnp.diag(raw_model['prior']['cov'] ** 2) \n",
    "    model['transition']['weight'] = jnp.diag(model['transition']['weight'])\n",
    "\n",
    "    model['transition']['cov'] = jnp.diag(raw_model['transition']['cov'] ** 2)\n",
    "    model['emission']['cov'] = jnp.diag(raw_model['emission']['cov'] ** 2)\n",
    "                    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kalman "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(current_state_mean, current_state_cov, transition):\n",
    "    predictive_mean = transition['weight'] @ current_state_mean + transition['bias']\n",
    "    predictive_cov = transition['weight'] @ current_state_cov @ transition['weight'].T + transition['cov']\n",
    "    return predictive_mean, predictive_cov\n",
    "\n",
    "def update(predictive_mean, predictive_cov, observation, emission):\n",
    "    predicted_observation_mean = emission['weight'] @ predictive_mean + emission['bias']\n",
    "    predicted_observation_cov = emission['weight'] @ predictive_cov @ emission['weight'].T + emission['cov']\n",
    "    kalman_gain = predictive_cov  @ emission['weight'].T @ jnp.linalg.inv(predicted_observation_cov)\n",
    "\n",
    "    corrected_state_mean = predictive_mean + kalman_gain @ (observation - predicted_observation_mean)\n",
    "    corrected_state_cov = predictive_cov - kalman_gain @ emission['weight'] @ predictive_cov\n",
    "\n",
    "    return corrected_state_mean, corrected_state_cov\n",
    "\n",
    "def filter_step(current_state_mean, current_state_cov, observation, transition, emission):\n",
    "    predicted_mean, predicted_cov = predict(current_state_mean, current_state_cov, transition)\n",
    "    filtered_mean, filtered_cov = update(predicted_mean, predicted_cov, observation, emission)\n",
    "    return predicted_mean, predicted_cov, filtered_mean, filtered_cov\n",
    "\n",
    "def init(observation, prior, emission):\n",
    "    init_filtering_mean, init_filtering_cov = update(prior['mean'], prior['cov'], observation, emission)\n",
    "    return prior['mean'], prior['cov'], init_filtering_mean, init_filtering_cov\n",
    "\n",
    "def log_likelihood_term(predictive_mean, predictive_cov, observation, emission):\n",
    "    return jax_gaussian_logpdf(x=observation, \n",
    "                        mean=emission['weight'] @ predictive_mean + emission['bias'], \n",
    "                        cov=emission['weight'] @ predictive_cov @ emission['weight'].T + emission['cov'])\n",
    "\n",
    "\n",
    "def filter(observations, model):\n",
    "    init_predictive_mean, init_predictive_cov, init_filtering_mean, init_filtering_cov = init(observations[0], model['prior'], model['emission'])\n",
    "    loglikelihood = log_likelihood_term(init_predictive_mean, init_predictive_cov, observations[0], model['emission'])\n",
    "\n",
    "    def _step(carry, x):\n",
    "        loglikelihood, filtering_mean, filtering_cov, transition, emission  = carry\n",
    "        predictive_mean, predictive_cov, filtering_mean, filtering_cov = filter_step(current_state_mean=filtering_mean,\n",
    "                                                                                    current_state_cov=filtering_cov,\n",
    "                                                                                    observation=x,\n",
    "                                                                                    transition=transition,\n",
    "                                                                                    emission=emission)\n",
    "\n",
    "        loglikelihood += log_likelihood_term(predictive_mean, predictive_cov, x, emission)\n",
    "\n",
    "        return (loglikelihood, filtering_mean, filtering_cov, transition, emission), (predictive_mean, predictive_cov, filtering_mean, filtering_cov)\n",
    "\n",
    "    (loglikelihood, *_), (predictive_means, predictive_covs, filtering_means, filtering_covs) = lax.scan(f=_step, \n",
    "                                init=(loglikelihood, init_filtering_mean, init_filtering_cov, model['transition'], model['emission']), \n",
    "                                xs=observations[1:])\n",
    "\n",
    "    predictive_means = jnp.concatenate((init_predictive_mean[None,:], predictive_means))\n",
    "    predictive_covs = jnp.concatenate((init_predictive_cov[None,:], predictive_covs))\n",
    "    filtering_means =  jnp.concatenate((init_filtering_mean[None,:], filtering_means))\n",
    "    filtering_covs =  jnp.concatenate((init_filtering_cov[None,:], filtering_covs))\n",
    "\n",
    "    return predictive_means, predictive_covs, filtering_means, filtering_covs, loglikelihood\n",
    "\n",
    "\n",
    "def smooth_step(carry, x):\n",
    "    next_smoothing_mean, next_smoothing_cov, transition_matrix = carry \n",
    "    filtering_mean, filtering_cov, next_predictive_mean, next_predictive_cov = x  \n",
    "    \n",
    "    C = filtering_cov @ transition_matrix @ jnp.linalg.inv(next_predictive_cov)\n",
    "    smoothing_mean = filtering_mean + C @ (next_smoothing_mean - next_predictive_mean)\n",
    "    smoothing_cov = filtering_cov + C @ (next_smoothing_cov - next_predictive_cov) @ C.T\n",
    "\n",
    "    return (smoothing_mean, smoothing_cov, transition_matrix), (smoothing_mean, smoothing_cov)\n",
    "\n",
    "\n",
    "def smooth(observations, model):\n",
    "\n",
    "    predictive_means, predictive_covs, filtering_means, filtering_covs = filter(observations, model)[:-1]\n",
    "\n",
    "    last_smoothing_mean, last_smoothing_cov = filtering_means[-1], filtering_covs[-1]\n",
    "\n",
    "    _, (smoothing_means, smoothing_covs) = lax.scan(f=smooth_step,\n",
    "                                            init=(last_smoothing_mean, last_smoothing_cov, model['transition']['weight']),\n",
    "                                            xs=(filtering_means[:-1], \n",
    "                                                filtering_covs[:-1],\n",
    "                                                predictive_means[1:],\n",
    "                                                predictive_covs[1:]),\n",
    "                                            reverse=True)\n",
    "\n",
    "    smoothing_means = jnp.concatenate((smoothing_means, last_smoothing_mean[None,:]))\n",
    "    smoothing_covs = jnp.concatenate((smoothing_covs, last_smoothing_cov[None,:]))\n",
    "\n",
    "    return smoothing_means, smoothing_covs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HMMs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearGaussianHMM:\n",
    "\n",
    "    def get_random_model(key, state_dim=2, obs_dim=2):\n",
    "        default_state_cov = 1e-2*jnp.ones(state_dim)\n",
    "        default_emission_cov = 1e-2*jnp.ones(obs_dim)\n",
    "\n",
    "        key, *subkeys = random.split(key, 2)\n",
    "        prior_mean = random.uniform(subkeys[0], shape=(state_dim,))\n",
    "        prior_cov = default_state_cov\n",
    "\n",
    "        key, *subkeys = random.split(key, 3)\n",
    "        transition_weight = random.uniform(subkeys[0], shape=(state_dim,))\n",
    "        transition_bias = random.uniform(subkeys[1], shape=(state_dim,))\n",
    "        transition_cov = default_state_cov\n",
    "\n",
    "        key, *subkeys = random.split(key, 3)\n",
    "        emission_weight = random.uniform(subkeys[0], shape=(obs_dim,state_dim))\n",
    "        emission_bias = random.uniform(subkeys[1], shape=(obs_dim,))\n",
    "        emission_cov = default_emission_cov\n",
    "\n",
    "        return {'prior':{'mean':prior_mean, 'cov':prior_cov}, \n",
    "                'transition': {'weight':transition_weight, 'bias': transition_bias,'cov':transition_cov},\n",
    "                'emission': {'weight':emission_weight, 'bias': emission_bias,'cov':emission_cov}}\n",
    "\n",
    "    def sample_single_step_joint(carry, x):\n",
    "        key, previous_state, transition, emission = carry\n",
    "        key, *subkeys = random.split(key, 3)\n",
    "\n",
    "        new_state = random.multivariate_normal(key=subkeys[0], \n",
    "                                        mean=transition['weight'] @ previous_state + transition['bias'],\n",
    "                                        cov=transition['cov'])\n",
    "\n",
    "        new_obs = random.multivariate_normal(key=subkeys[1], \n",
    "                                    mean=emission['weight'] @ new_state + emission['bias'],\n",
    "                                    cov=emission['cov'])\n",
    "\n",
    "        return (key, new_state, transition, emission), (new_state, new_obs)\n",
    "\n",
    "    def sample_joint_sequence(key, model, length):\n",
    "\n",
    "        key, *subkeys = random.split(key, 3)\n",
    "        init_state = random.multivariate_normal(key=subkeys[0], \n",
    "                                        mean=model['prior']['mean'],\n",
    "                                        cov=model['prior']['cov'])\n",
    "\n",
    "        init_obs = random.multivariate_normal(key=subkeys[1], \n",
    "                                        mean=model['emission']['weight'] @ init_state + model['emission']['bias'], \n",
    "                                        cov=model['emission']['cov'])\n",
    "\n",
    "        (key, *_), (state_samples, obs_samples) = lax.scan(f=LinearGaussianHMM.sample_single_step_joint, \n",
    "                                                init=(key, init_state, model['transition'], model['emission']), \n",
    "                                                length=length-1,\n",
    "                                                xs=None)\n",
    "        \n",
    "        state_samples = jnp.concatenate((init_state[None,:], state_samples))\n",
    "        obs_samples = jnp.concatenate((init_obs[None,:], obs_samples))\n",
    "\n",
    "        return state_samples, obs_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELBOs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kalman_filter_step = filter_step\n",
    "kalman_init = init \n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "\n",
    "def _create_quad_form(A, b, Omega):\n",
    "    return {'A':A, 'b':b, 'Omega':Omega}\n",
    "\n",
    "def _create_filtering(mean, cov):\n",
    "    return {'mean':mean, 'cov':cov}\n",
    "\n",
    "def _create_backward(A, a, cov):\n",
    "    return {'A':A, 'a':a, 'cov':cov}\n",
    "\n",
    "def _update_quad_forms_at_index(quad_forms, quad_form, index):\n",
    "    quad_forms['A'] = quad_forms['A'].at[index].set(quad_form['A'])\n",
    "    quad_forms['b'] = quad_forms['b'].at[index].set(quad_form['b'])\n",
    "    quad_forms['Omega'] = quad_forms['Omega'].at[index].set(quad_form['Omega'])\n",
    "    return quad_forms\n",
    "\n",
    "def _constant_terms_from_log_gaussian(dim, det_cov):\n",
    "    return -0.5*(dim * jnp.log(2*jnp.pi) + jnp.log(det_cov))\n",
    "\n",
    "def _eval_quad_form(quad_form, x):\n",
    "    common_term = quad_form['A'] @ x + quad_form['b']\n",
    "    return common_term.T @ quad_form['Omega'] @ common_term\n",
    "\n",
    "def _expect_obs_term_under_backward(obs_term, q_backward):\n",
    "    return _expect_quad_form_under_backward(obs_term, q_backward)\n",
    "\n",
    "def _expect_obs_term_under_filtering(obs_term, q_filtering):\n",
    "    return _expect_quad_form_under_filtering(obs_term, q_filtering)\n",
    "\n",
    "def _get_obs_term(observation, p_emission):\n",
    "    \n",
    "    return _create_quad_form(A=p_emission['weight'],\n",
    "                            b=p_emission['bias'] - observation,\n",
    "                            Omega=- 0.5*p_emission['prec'])\n",
    "\n",
    "def _init_filtering(observation, q_prior, q_emission):\n",
    "    return _create_filtering(*kalman_init(observation, q_prior, q_emission)[2:])\n",
    "\n",
    "def _update_filtering(observation, q_filtering, q_transition, q_emission):\n",
    "    return _create_filtering(*kalman_filter_step(q_filtering['mean'], q_filtering['cov'], observation, q_transition, q_emission)[2:])\n",
    "\n",
    "def _update_backward(q_filtering, q_transition):\n",
    "\n",
    "    filtering_prec = jnp.linalg.inv(q_filtering['cov'])\n",
    "\n",
    "    backward_prec = q_transition['weight'].T @ q_transition['prec'] @ q_transition['weight'] + filtering_prec\n",
    "\n",
    "    cov = jnp.linalg.inv(backward_prec)\n",
    "\n",
    "    common_term = q_transition['weight'].T @ q_transition['prec'] \n",
    "    A = cov @ common_term\n",
    "    a = cov @ (filtering_prec @ q_filtering['mean'] - common_term @  q_transition['bias'])\n",
    "\n",
    "    return _create_backward(A=A,a=a,cov=cov)\n",
    "\n",
    "def _integrate_previous_terms(integrate_up_to, quad_forms, nonlinear_term, q_backward):\n",
    "\n",
    "    quad_forms_transition, quad_forms_emission = quad_forms\n",
    "\n",
    "    masks = jnp.arange(start=0, stop=quad_forms_transition['A'].shape[0]) <= integrate_up_to\n",
    "    _expect_quad_forms_under_backward_masked = vmap(_expect_quad_form_under_backward_masked, in_axes=(0,0,None))\n",
    "    \n",
    "    constants0, quad_forms_transition = _expect_quad_forms_under_backward_masked(masks, quad_forms_transition, q_backward)\n",
    "\n",
    "    masks = masks.at[integrate_up_to].set(False)\n",
    "    constants1, quad_forms_emission = _expect_quad_forms_under_backward_masked(masks, quad_forms_emission, q_backward)\n",
    "\n",
    "    constants2, quad_form = _expect_obs_term_under_backward(nonlinear_term, q_backward)\n",
    "\n",
    "    quad_forms_emission = _update_quad_forms_at_index(quad_forms_emission, \n",
    "                                                    quad_form, \n",
    "                                                    index=integrate_up_to)\n",
    "\n",
    "    return jnp.sum(constants0) + jnp.sum(constants1) + constants2, quad_forms_transition, quad_forms_emission\n",
    "\n",
    "\n",
    "def _init_V(observation, p):\n",
    "\n",
    "    constants_V = _constant_terms_from_log_gaussian(p['transition']['cov'].shape[0], jnp.linalg.det(p['prior']['cov'])) + \\\n",
    "                  _constant_terms_from_log_gaussian(p['emission']['cov'].shape[0], p['emission']['det_cov'])\n",
    "    \n",
    "    init_transition_term = {'A':jnp.eye(p['transition']['cov'].shape[0]), 'b':-p['prior']['mean'], 'Omega':-0.5 * jnp.linalg.inv(p['prior']['cov'])}\n",
    "    nonlinear_term = _get_obs_term(observation, p['emission'])\n",
    "\n",
    "    return constants_V, init_transition_term, nonlinear_term \n",
    "\n",
    "def _expect_transition_quad_form_under_backward(q_backward, p_transition):\n",
    "    # expectation of the quadratic form that appears in the log of the state transition density\n",
    "\n",
    "    A=p_transition['weight'] @ q_backward['A'] - jnp.eye(p_transition['cov'].shape[0])\n",
    "    b=p_transition['weight'] @ q_backward['a'] + p_transition['bias']\n",
    "    Omega = -0.5*p_transition['prec']\n",
    "\n",
    "    return {'A':A, 'b':b, 'Omega':Omega} \n",
    "\n",
    "def _no_integration(quad_form, q_backward):\n",
    "    return 0.0, quad_form\n",
    "\n",
    "def _expect_quad_form_under_backward(quad_form, q_backward):\n",
    "    Sigma = quad_form['A'] @ q_backward['cov'] @ quad_form['A'].T\n",
    "    constant = jnp.trace(quad_form['Omega'] @ Sigma)\n",
    "    A = quad_form['A'] @ q_backward['A']\n",
    "    b = quad_form['A'] @ q_backward['a'] + quad_form['b']\n",
    "    Omega = quad_form['Omega']\n",
    "    return constant, {'A':A, 'b':b, 'Omega':Omega}\n",
    "\n",
    "def _expect_quad_form_under_backward_masked(mask, quad_form, q_backward):\n",
    "    # if mask: \n",
    "    #     return _expect_quad_form_under_backward(quad_form, q_backward)\n",
    "\n",
    "    # return _no_integration(quad_form, q_backward)\n",
    "    return lax.cond(mask, _expect_quad_form_under_backward, _no_integration, quad_form, q_backward)\n",
    "\n",
    "def _update_V(observation, integrate_up_to, quad_forms, nonlinear_term, q_backward, p):\n",
    "\n",
    "    constants, quad_forms_transition, quad_forms_emission =  _integrate_previous_terms(integrate_up_to, quad_forms, nonlinear_term, q_backward)\n",
    "\n",
    "    dim_z, dim_x = p['transition']['cov'].shape[0], p['emission']['cov'].shape[0]\n",
    "    constants +=  _constant_terms_from_log_gaussian(dim_x, p['emission']['det_cov']) \\\n",
    "                +   _constant_terms_from_log_gaussian(dim_z, p['transition']['det_cov']) \\\n",
    "                + - _constant_terms_from_log_gaussian(dim_z, jnp.linalg.det(q_backward['cov'])) \\\n",
    "                +  0.5 * dim_z \\\n",
    "                + - 0.5 * jnp.trace(p['transition']['prec'] @ p['transition']['weight'] @ q_backward['cov'] @ p['transition']['weight'].T)\n",
    "\n",
    "    quad_form = _expect_transition_quad_form_under_backward(q_backward, p['transition'])\n",
    "\n",
    "\n",
    "    quad_forms_transition = _update_quad_forms_at_index(quad_forms_transition, \n",
    "                                                    quad_form, \n",
    "                                                    index=integrate_up_to+1)\n",
    "    \n",
    "    nonlinear_term = _get_obs_term(observation, p['emission'])\n",
    "\n",
    "    return constants, [quad_forms_transition, quad_forms_emission], nonlinear_term\n",
    "\n",
    "def _expect_quad_form_under_filtering(quad_form, q_filtering):\n",
    "    return jnp.trace(quad_form['Omega'] @ quad_form['A'] @ q_filtering['cov'] @ quad_form['A'].T) + _eval_quad_form(quad_form, q_filtering['mean'])\n",
    "\n",
    "def _expect_V_under_filtering(constants_V, quad_forms, nonlinear_term, q_filtering):\n",
    "    result = constants_V\n",
    "\n",
    "    quad_forms_transition, quad_forms_emission = quad_forms\n",
    "    quad_forms_emission['A'] = quad_forms_emission['A'][:-1]\n",
    "    quad_forms_emission['b'] = quad_forms_emission['b'][:-1]\n",
    "    quad_forms_emission['Omega'] = quad_forms_emission['Omega'][:-1]\n",
    "\n",
    "    expect_quad_forms_under_filtering = vmap(_expect_quad_form_under_filtering, in_axes=(0, None))\n",
    "\n",
    "    result += jnp.sum(expect_quad_forms_under_filtering(quad_forms_transition, q_filtering)) \\\n",
    "            + jnp.sum(expect_quad_forms_under_filtering(quad_forms_emission, q_filtering)) \\\n",
    "            + _expect_obs_term_under_filtering(nonlinear_term, q_filtering)\n",
    "\n",
    "    result += 0.5*q_filtering['mean'].shape[0]\n",
    "\n",
    "    return result\n",
    "\n",
    "def init(observations, p, q):\n",
    "\n",
    "    constants_V, init_transition_term, nonlinear_term = _init_V(observations[0], p)\n",
    "    q_filtering = _init_filtering(observations[0], q['prior'], q['emission'])\n",
    "    dim_z = p['transition']['cov'].shape[0]\n",
    "\n",
    "    num_terms = len(observations)\n",
    "    As = jnp.empty(shape=(num_terms, dim_z, dim_z))\n",
    "    bs = jnp.empty(shape=(num_terms, dim_z))\n",
    "    Omegas = jnp.empty(shape=(num_terms, dim_z, dim_z))\n",
    "\n",
    "    As = As.at[0].set(init_transition_term['A'])\n",
    "    bs  = bs.at[0].set(init_transition_term['b'])\n",
    "    Omegas = Omegas.at[0].set(init_transition_term['Omega'])\n",
    "\n",
    "    quad_forms_transition = _create_quad_form(A=As, b=bs, Omega=Omegas)\n",
    "\n",
    "    dim_x = p['emission']['cov'].shape[0]\n",
    "\n",
    "    quad_forms_emission = _create_quad_form(A=jnp.empty(shape=(num_terms, dim_x, dim_z)),\n",
    "                                            b=jnp.empty(shape=(num_terms, dim_x)),\n",
    "                                            Omega=jnp.empty(shape=(num_terms, dim_x, dim_x)))\n",
    "            \n",
    "    quad_forms = [quad_forms_transition, quad_forms_emission]\n",
    "    return constants_V, quad_forms, nonlinear_term, q_filtering\n",
    "\n",
    "def _update(observation, integrate_up_to, quad_forms, nonlinear_term, q_filtering, p, q):\n",
    "    q_backward = _update_backward(q_filtering, q['transition'])\n",
    "    constants, quad_forms, nonlinear_term = _update_V(observation, \n",
    "                                                    integrate_up_to,\n",
    "                                                    quad_forms, \n",
    "                                                    nonlinear_term, \n",
    "                                                    q_backward, \n",
    "                                                    p)\n",
    "\n",
    "    q_filtering = _update_filtering(observation, q_filtering, q['transition'], q['emission'])\n",
    "\n",
    "    return constants, quad_forms, nonlinear_term, q_filtering\n",
    "\n",
    "def prepare_parameters(model):\n",
    "\n",
    "    model = parameters_from_raw_parameters(model)\n",
    "\n",
    "    model['transition']['prec'] =  jnp.linalg.inv(model['transition']['cov'])\n",
    "    model['transition']['det_cov'] = jnp.linalg.det(model['transition']['cov'])\n",
    "\n",
    "    model['emission']['prec'] =  jnp.linalg.inv(model['emission']['cov'])\n",
    "    model['emission']['det_cov'] = jnp.linalg.det(model['emission']['cov'])\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "def linear_gaussian_elbo(p_raw, q_raw, observations):\n",
    "\n",
    "    p = prepare_parameters(p_raw)\n",
    "    q = prepare_parameters(q_raw)\n",
    "    \n",
    "    constants_V, quad_forms, nonlinear_term, q_filtering = init(observations, p, q)\n",
    "\n",
    "    observations = observations[1:]\n",
    "\n",
    "    integrate_up_to_array = jnp.arange(start=0, stop=len(observations))\n",
    "\n",
    "    \n",
    "    # for observation, integrate_up_to in zip(observations, integrate_up_to_array):\n",
    "    #     new_constants, quad_forms, nonlinear_term, q_filtering = _update(observation, integrate_up_to, quad_forms, nonlinear_term, q_filtering, p, q)\n",
    "    #     constants_V += new_constants\n",
    "\n",
    "    def V_step(carry, x):\n",
    "\n",
    "        observation, integrate_up_to = x \n",
    "        quad_forms, nonlinear_term, q_filtering, p, q = carry \n",
    "\n",
    "        new_constants, quad_forms, nonlinear_term, q_filtering = _update(observation, \n",
    "                                                                    integrate_up_to,\n",
    "                                                                    quad_forms, \n",
    "                                                                    nonlinear_term, \n",
    "                                                                    q_filtering, \n",
    "                                                                    p, \n",
    "                                                                    q)\n",
    "\n",
    "        return (quad_forms, nonlinear_term, q_filtering, p, q), new_constants\n",
    "\n",
    "    (quad_forms, nonlinear_term, q_filtering, p, q), constants = lax.scan(f=V_step, \n",
    "                                init=(quad_forms, nonlinear_term, q_filtering, p, q),\n",
    "                                xs=(observations, integrate_up_to_array))\n",
    "\n",
    "    constants_V += jnp.sum(constants) \n",
    "    constants_V += -_constant_terms_from_log_gaussian(p['transition']['cov'].shape[0], jnp.linalg.det(q_filtering['cov']))\n",
    "\n",
    "    return _expect_V_under_filtering(constants_V, quad_forms, nonlinear_term, q_filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction\n",
    "\n",
    "This notebook is comprised of a series of experiments that attempt to recover expectations $\\mathbb{E}[h(z_{1:t})|x_{1:t}]$ via variational approximations, when the process $(z_t, x_t)_{t \\ge 1}$ is an HMM. The main metric $\\ell$ all along is the MSE against the true states when $h$ is a plain sum, ie\n",
    "\n",
    "$$\\ell = \\sum_{t=1}^T  \\left(z_t^* - \\mathbb{E}_{q(z_t|x_{1:T})}[z_t] \\right)^2$$\n",
    "\n",
    "where $q(z_t|x_{1:T}) = q_T(z_t)$ is the marginal smoothing distribution at $t$.\n",
    "\n",
    "In all the following, we assume that the variational smoothing distribution factorizes, for any $1 \\leq t \\leq T$, as $$q_\\phi(z_{1:t}|x_{1:t}) = q_\\phi(z_t|x_{1:t}) \\prod_{s=1}^{t-1} q_\\phi(z_s|z_{s+1},x_{1:s})$$ where \n",
    "- $q_\\phi(z_t|x_{1:t}) \\sim \\mathcal{N}(\\mu_{1:t}, \\Sigma_{1:t})$ is the *variational filtering distribution*,\n",
    "- $q_\\phi(z_s|z_{s+1},x_{1:s}) \\sim \\mathcal{N}(\\overleftarrow{\\mu}_{1:t}(z_{s+1}), \\overleftarrow{\\Sigma}_{1:t})$ is the *variational backward distribution*.\n",
    "\n",
    "In the following, we make several assumptions on both $p_\\theta$ and $q_\\phi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. $p_\\theta$ is a linear Gaussian HMM\n",
    "\n",
    "First we assume that observation sequences $x_{1:T}$ arise from $p_\\theta(z_{1:T},x_{1:T})$ defined as\n",
    "$$z_t = A_\\theta z_{t-1} + a_\\theta + \\eta_\\theta$$ \n",
    "$$x_t = B_\\theta z_t + b_\\theta + \\epsilon_\\theta$$\n",
    "\n",
    "where $\\eta_\\theta \\sim \\mathcal{N}(0,Q_\\theta)$ and $\\epsilon_\\theta \\sim \\mathcal{N}(0,R_\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. a. $q_\\phi$ is a linear Gaussian HMM\n",
    "\n",
    "We start by recovering $p_\\theta$ when $q_\\phi$ is in the family of the true $p_\\theta$. We can do this by prescribing the model for $q_\\phi$ in forward time with a similar HMM structure as $p_\\theta$ (but random initial parameters). In this case exceptionally, the parameters of the filtering distributions are linked in closed-form to the forward model via the Kalman filtering recursions. Furthermore, the mean of the variational backward is a linear form in $z_{t+1}$, ie. $$\\overleftarrow{\\mu}_{1:t}(z_{t+1}) = \\overleftarrow{A}_{1:t} z_{t+1} + \\overleftarrow{a}_{1:t}$$\n",
    "\n",
    "where $\\overleftarrow{A}_{1:t}$ and $\\overleftarrow{a}_{1:t}$ are given in closed-form via the Kalman smoothing recursions.\n",
    "\n",
    "For this experiment, not only should the expectations be correctly recovered, but parameters in $\\phi$ and $\\theta$ may be identifiable in some cases (depending on the conditioning of $p_\\theta$). We also know that in this case the best estimate of $z_{1:t}^*$ for any sequence is obtained via the Kalman smoothing recursions applied with the true parameters $\\theta$, so we have an optimal estimator to compare to. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking that $\\mathcal{L}(\\theta, \\theta) = \\log p_\\theta$ \n",
    "*Remark: here $\\log p$ is computed with Kalman*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Number of observations per sequence: 16"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Number of sequences: 30"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "Average $\\log p_\\theta(x)$ across sequences $x$: 142.91"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "Average $|\\mathcal{L}(\\theta,\\theta)- \\log p_\\theta(x)|$ across sequences: 0.0"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state_dim, obs_dim = 2, 3\n",
    "num_sequences = 30\n",
    "length = 16\n",
    "display(Markdown(f'Number of observations per sequence: {length}'))\n",
    "display(Markdown(f'Number of sequences: {num_sequences}'))\n",
    "\n",
    "key, *subkeys = random.split(key, 3)\n",
    "\n",
    "p_raw = LinearGaussianHMM.get_random_model(key=subkeys[0], state_dim=state_dim, obs_dim=obs_dim)\n",
    "\n",
    "p = parameters_from_raw_parameters(p_raw)\n",
    "\n",
    "linear_gaussian_sampler = vmap(LinearGaussianHMM.sample_joint_sequence, in_axes=(0, None, None))\n",
    "key, *subkeys = random.split(key, num_sequences+1)\n",
    "state_sequences, obs_sequences = linear_gaussian_sampler(jnp.array(subkeys), p, length)\n",
    "\n",
    "\n",
    "filter_obs_sequences = vmap(kalman_filter, in_axes=(0, None))\n",
    "elbo_sequences = jit(vmap(linear_gaussian_elbo, in_axes=(None, None, 0)))\n",
    "\n",
    "average_evidence_across_sequences = jnp.mean(filter_obs_sequences(obs_sequences, p)[-1])\n",
    "display(Latex(f'Average $\\log p_\\\\theta(x)$ across sequences $x$: {average_evidence_across_sequences:.2f}'))\n",
    "average_elbo_across_sequences_with_true_model = jnp.mean(elbo_sequences(p_raw, p_raw, obs_sequences))\n",
    "display(Latex('Average $|\\mathcal{L}(\\\\theta,\\\\theta)- \\log p_\\\\theta(x)|$ across sequences: ' + f'{jnp.abs(average_elbo_across_sequences_with_true_model-average_evidence_across_sequences)}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizing $\\phi$ when $\\theta$ is fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "Average $|\\mathcal{L}(\\theta,\\phi)- \\log p_\\theta(x)|$ across sequences:77710.14"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "optimizer = optax.adam(learning_rate=-1e-3)\n",
    "\n",
    "@jit\n",
    "def step(p_raw, q_raw, opt_state, batch):\n",
    "    loss_value, grads = value_and_grad(linear_gaussian_elbo, argnums=1)(p_raw, q_raw, batch)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, q_raw)\n",
    "    q_raw = optax.apply_updates(q_raw, updates)\n",
    "    return p_raw, q_raw, opt_state, -loss_value\n",
    "\n",
    "q_raw = LinearGaussianHMM.get_random_model(key=subkeys[1], state_dim=state_dim, obs_dim=obs_dim)\n",
    "average_elbo_across_sequences_with_init_q = jnp.mean(elbo_sequences(p_raw, q_raw, obs_sequences))\n",
    "display(Latex('Average $|\\mathcal{L}(\\\\theta,\\\\phi)- \\log p_\\\\theta(x)|$ across sequences:' +  f'{jnp.abs(average_evidence_across_sequences-average_elbo_across_sequences_with_init_q):.2f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fit(p_raw, q_raw, optimizer: optax.GradientTransformation) -> optax.Params:\n",
    "    opt_state = optimizer.init(q_raw)\n",
    "\n",
    "    eps = jnp.inf\n",
    "    old_mean_epoch_elbo = -average_elbo_across_sequences_with_init_q\n",
    "    epoch_nb = 0\n",
    "    mean_elbos = [old_mean_epoch_elbo - average_evidence_across_sequences]\n",
    "    while eps > 1e-2:\n",
    "        epoch_elbo = 0.0\n",
    "        for batch in obs_sequences: \n",
    "            p_raw, q_raw, opt_state, elbo_value = step(p_raw, q_raw, opt_state, batch)\n",
    "            epoch_elbo += elbo_value\n",
    "        mean_epoch_elbo = epoch_elbo/len(obs_sequences)\n",
    "        eps = jnp.abs(mean_epoch_elbo - old_mean_epoch_elbo)\n",
    "        epoch_nb+=1\n",
    "        mean_elbos.append(mean_epoch_elbo - average_evidence_across_sequences)\n",
    "        old_mean_epoch_elbo = mean_epoch_elbo\n",
    "    return q_raw, mean_elbos\n",
    "\n",
    "\n",
    "fitted_q_raw, mean_elbos = fit(p_raw, q_raw, optimizer)\n",
    "\n",
    "plt.plot(mean_elbos)\n",
    "plt.xlabel('Epoch nb'), \n",
    "plt.ylabel('$|\\mathcal{L}(\\\\theta,\\\\phi)- \\log p_\\\\theta(x)|$')\n",
    "fitted_q = parameters_from_raw_parameters(fitted_q_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing expectations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE(E_q(h(z)), z_true): [0.0873562  0.08952309]\n",
      "MSE(E_p(h(z)), z_true): [0.02279203 0.0420618 ]\n"
     ]
    }
   ],
   "source": [
    "def squared_error_expectation_against_true_states(states, observations, approximate_linear_gaussian_model, additive_functional):\n",
    "    smoothed_states, _ = kalman_smooth(observations, approximate_linear_gaussian_model)\n",
    "    return jnp.sqrt((additive_functional(smoothed_states) - additive_functional(states)) ** 2)\n",
    "\n",
    "additive_functional = partial(jnp.sum, axis=0)\n",
    "mse_in_expectations = vmap(squared_error_expectation_against_true_states, in_axes=(0,0, None, None))\n",
    "print('MSE(E_q(h(z)), z_true):', jnp.mean(mse_in_expectations(state_sequences, obs_sequences, fitted_q, additive_functional), axis=0))\n",
    "print('MSE(E_p(h(z)), z_true):', jnp.mean(mse_in_expectations(state_sequences, obs_sequences, p, additive_functional), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. b. $q_\\phi$ is prescribed only through its filtering and backward distributions whose parameters are estimated by neural networks\n",
    "\n",
    "We can keep the assumption that $\\overleftarrow{\\mu}_{1:t}(z_{t+1}) = \\overleftarrow{A}_{1:t} z_{t+1} + \\overleftarrow{a}_{1:t}$ to allow the recursion on the ELBO, but predict $\\mu_{1:t}, \\Sigma_{1:t},  \\overleftarrow{A}_{1:t}$ and $\\overleftarrow{a}_{1:t}$ with neural networks (e.g. recurrent networks updated sequentially with new observations $x_t$). \n",
    "\n",
    "*TODO*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. $p_\\theta$ is an HMM with a linear transition but a nonlinear emission\n",
    "\n",
    "We now assume that $p_\\theta$ has a nonlinear emission distribution, ie. $x_t  = f_\\theta(z_t) + \\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. a. $q_\\phi$ is a linear Gaussian HMM \n",
    "We keep a linear gaussian distribution for $q_\\phi$, but we add a mapping to compute the expectation of the emission term from $p_\\theta$. We need to approximate the following quantity:\n",
    "\n",
    "$$\\mathbb{E}_{q(z_t|z_{t+1}, x_{1:t})}\\left[(x_t - f_\\theta(z_t))^T R^{{\\theta}^{-1}}(x_t - f_\\theta(z_t))\\right]$$\n",
    "\n",
    "And similarly for the last expectation under the filtering distribution: \n",
    "\n",
    "$$\\mathbb{E}_{q(z_T|x_{1:T})}\\left[(x_T - f_\\theta(z_T))^T R^{{\\theta}^{-1}}(x_T - f_\\theta(z_T))\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. a. i. Sketch of a method using the expected value of quadratic forms \n",
    "\n",
    "\n",
    "If we know the expectation $\\mu$ and variance $\\Sigma$ of a random variable $v$ with distribution $p_v$ (which need not be Gaussian), the following holds:\n",
    "\n",
    "$$\\mathbb{E}_{v \\sim p_v}\\left[(x - v)^T \\Omega (x - v)\\right] = tr(\\Omega \\Sigma) + (\\mu - x)^T \\Omega (\\mu - x)$$\n",
    "\n",
    "Suppose we a have neural network which approximates the mean and variance of $v \\sim f_\\theta(z)$ when $z \\sim p_z$, given parameters of $p_z$. Denote $\\tilde{\\mu}_v$ and $\\tilde{\\Sigma}_v$ these estimated means and variances. For the filtering case, we feed the network with filtering mean and covariance at $T$ to obtain an estimate of $\\tilde{\\mu}_v$ and $\\tilde{\\Sigma}_v$, then:\n",
    "\n",
    "$$\\mathbb{E}_{q(z_T|x_{1:T})}\\left[(x_T - f_\\theta(z_T))^T R^{{\\theta}^{-1}}(x_T - f_\\theta(z_T))\\right] = tr(\\Omega \\tilde{\\Sigma}_v) + (\\tilde{\\mu}_v - x)^T R^{{\\theta}^{-1}} (\\tilde{\\mu}_v - x)$$\n",
    "\n",
    "For the backwards case this is not as simple, because: $\\overleftarrow{\\mu}_{1:t}$ is a function of $z_{t+1}$, therefore $\\mathbb{E}_{q(z_t|z_{t+1}, x_{1:t})}[f_\\theta(z_t)]$ and $\\mathbb{V}_{q(z_t|z_{t+1}, x_{1:t})}[f_\\theta(z_t)]$ are also functions of $z_{t+1}$. \n",
    "\n",
    "We can still attempt to use one network for both the fitlering and the backwards via the following scheme: \n",
    "\n",
    "- Build a neural network $g_\\alpha(A, a, \\Sigma)$ which outputs $\\tilde{A}_v, \\tilde{a}_v$ and $\\tilde{\\Sigma}_v$\n",
    "- For the backwards case, use $A = \\overleftarrow{A}_{1:t}, a = \\overleftarrow{a}_{1:t}$ and $\\Sigma = \\overleftarrow{\\Sigma}_{1:t}$, and consider that $\\tilde{\\mu}_v = \\tilde{A}_vz_{t+1} + \\tilde{a}_v$. \n",
    "- For the filtering case, use $A = 0, a = a_{1:t}$ and $\\Sigma = \\Sigma_{1:t}$, and consider that $\\tilde{\\mu}_v = \\tilde{a}_v$ (without using the output $\\tilde{A}_v$).\n",
    "\n",
    "Since the variance $\\mathbb{V}_{q(z_t|z_{t+1}, x_{1:t})}[f_\\theta(z_t)]$ is a function of $z_{t+1}$, the term in $tr(\\Omega \\tilde{\\Sigma}_v)$ breaks the quadratic form and prevents computations in the next steps. This method might still be useful if: \n",
    "\n",
    "- We suppose that $\\tilde{\\Sigma}_v \\approx \\tilde{\\Sigma}_v' z_{t+1} z_{t+1}^T$ therefore reintroducing the quadratic assumption.\n",
    "- We let go of the online recursion and perform computations online, therefore allowing more flexible estimates outside the quadratic family "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. a. ii. Using a sampling based approach and a neural network similar to [Johnson et al.](https://arxiv.org/pdf/1603.06277.pdf)\n",
    "\n",
    "At any time $t$, the predictive distribution $q(z_{t+1}|x_{1:t})$ is easily available via a Kalman predict step. Therefore a sample from the variational backward is easily obtained by sampling:\n",
    "\n",
    "$$\\zeta \\sim q(z_{t+1}|x_{1:t})$$ \n",
    "then \n",
    "\n",
    "$$\\xi \\sim q(z_t|z_{t+1}=\\zeta ,x_{1:t})$$\n",
    "\n",
    "such that $(x_t - f_\\theta(\\xi))^T R^{{\\theta}^{-1}}(x_t - f_\\theta(\\xi))$ is the one-sample Monte Carlo estimate of $\\mathbb{E}_{q(z_t|z_{t+1}, x_{1:t})}\\left[(x_t - f_\\theta(z_t))^T R^{{\\theta}^{-1}}(x_t - f_\\theta(z_t))\\right]$. \n",
    "\n",
    "However we are interested in the terms $v, W$ of the quadratic approximation $(z_{t+1} - v)^T W (z_{t+1} - v)$ of this expectation. We could build additional neural networks with parameters $\\alpha$ such that $$\\mathbb{E}_{q(z_t|z_{t+1}, x_{1:t})}\\left[(x_t - f_\\theta(z_t))^T R^{{\\theta}^{-1}}(x_t - f_\\theta(z_t))\\right] \\approx [z_{t+1} - v_\\alpha(\\xi)]^T W_\\alpha(\\xi) [z_{t+1} - v_\\alpha(\\xi)]$$\n",
    "\n",
    "This requires fitting $\\alpha$ at a fixed $(\\theta, \\phi)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. a. iii. Using [Johnson et al.](https://arxiv.org/pdf/1603.06277.pdf) as is"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c528143767b35424e5fa616681845f3b9656c31f35cafe040129ce40f24d14f5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('backward_ica')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
