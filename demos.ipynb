{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on backward variational ICA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Uncomment and run the following cell if you're using Collab***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/mchagneux/backward_ica.git\n",
    "# !mv backward_ica/* ./"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(2.1316282073e-14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathis/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/nn/modules/container.py:597: UserWarning: Setting attributes on ParameterDict is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from src.eval import mse_expectation_against_true_states\n",
    "from src.kalman import Kalman, NumpyKalman\n",
    "from src.hmm import AdditiveGaussianHMM, LinearGaussianHMM\n",
    "from src.elbo import LinearGaussianELBO\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "torch.set_default_dtype(torch.float64) \n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "torch.set_printoptions(precision=10)\n",
    "\n",
    "## sanity checks\n",
    "hmm = LinearGaussianHMM(state_dim=2, obs_dim=2)\n",
    "states, observations = hmm.sample_joint_sequence(10)\n",
    "\n",
    "for param in hmm.model.parameters():param.requires_grad = False\n",
    "likelihood_torch = Kalman(hmm.model).filter(observations)[4] #kalman with torch operators \n",
    "likelihood_numpy = NumpyKalman(hmm.model).filter(observations.numpy())[2] #kalman with numpy operators \n",
    "likelihood_via_elbo = LinearGaussianELBO(hmm.model, hmm.model)(observations) #elbo\n",
    "\n",
    "# both should be close to 0\n",
    "print(likelihood_numpy - likelihood_torch)\n",
    "print(likelihood_numpy - likelihood_via_elbo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. $p$ and $q$ are a linear Gaussian HMMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathis/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/nn/modules/container.py:597: UserWarning: Setting attributes on ParameterDict is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True evidence accross all sequences: tensor(289.4399950207)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(39769.4221639078)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(14211.7123990185)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(6425.1472680504)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(3396.7449257098)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1857.4977526532)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1574.9162045078)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1361.8359411717)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1178.1037451402)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1019.4819551965)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(898.8964313511)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(798.5644992976)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(713.5487374184)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(642.9804792664)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(585.2156668418)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(538.0579497728)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(499.7240083360)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(468.6475498095)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(443.5161071838)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(423.1394621295)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(406.4976428458)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(392.7702558584)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(381.2844660679)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(371.5084802624)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(363.0262414180)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(355.5183373688)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(348.7474036802)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(342.5341213950)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(336.7473841626)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(331.2958530132)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(326.1166399098)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(321.1693504650)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(316.4310550468)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(311.8920566961)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(307.5530312791)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(303.4223684695)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(299.5140969529)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(295.8459041866)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(292.4369746066)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(289.3055408074)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(286.4659091378)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(283.9249616761)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(281.6781935749)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(279.7055721869)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(277.9677793324)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(276.4037272222)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(274.9305393540)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(273.4471744729)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(271.8422888938)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(270.0057138648)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(267.8415024589)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(265.2796263870)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(262.2836455174)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(258.8529518496)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(255.0198288991)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(250.8427964487)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(246.3981389978)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(241.7712437029)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(237.0487744671)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(232.3121505636)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(227.6324762932)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(223.0669871687)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mathis/repos/backward_ica/demos.ipynb Cell 5'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mathis/repos/backward_ica/demos.ipynb#ch0000002?line=30'>31</a>\u001b[0m \u001b[39mfor\u001b[39;00m observations \u001b[39min\u001b[39;00m observation_sequences: \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mathis/repos/backward_ica/demos.ipynb#ch0000002?line=31'>32</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mathis/repos/backward_ica/demos.ipynb#ch0000002?line=32'>33</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39melbo(observations)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mathis/repos/backward_ica/demos.ipynb#ch0000002?line=33'>34</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mathis/repos/backward_ica/demos.ipynb#ch0000002?line=34'>35</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/backward_ica/src/elbo.py:163\u001b[0m, in \u001b[0;36mLinearGaussianELBO.forward\u001b[0;34m(self, observations)\u001b[0m\n\u001b[1;32m    <a href='file:///~/repos/backward_ica/src/elbo.py?line=160'>161</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_transition_det_cov \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdet(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtransition\u001b[39m.\u001b[39mcov)\n\u001b[1;32m    <a href='file:///~/repos/backward_ica/src/elbo.py?line=161'>162</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_emission_prec \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39minverse(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39memission\u001b[39m.\u001b[39mcov)\n\u001b[0;32m--> <a href='file:///~/repos/backward_ica/src/elbo.py?line=162'>163</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_emission_det_cov \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mdet(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49memission\u001b[39m.\u001b[39;49mcov)\n\u001b[1;32m    <a href='file:///~/repos/backward_ica/src/elbo.py?line=165'>166</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_model_transition_prec \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39minverse(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_model\u001b[39m.\u001b[39mtransition\u001b[39m.\u001b[39mcov)\n\u001b[1;32m    <a href='file:///~/repos/backward_ica/src/elbo.py?line=166'>167</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_model_transition_det_cov \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdet(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv_model\u001b[39m.\u001b[39mtransition\u001b[39m.\u001b[39mcov)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hmm = LinearGaussianHMM(state_dim=2, obs_dim=2) # pick some true model p \n",
    "for param in hmm.model.parameters(): param.requires_grad = False # not learning the parameters of the true model for now \n",
    "\n",
    "\n",
    "\n",
    "# sampling 10 sequences from the hmm \n",
    "samples = [hmm.sample_joint_sequence(8) for _ in range(10)] \n",
    "state_sequences = [sample[0] for sample in samples]\n",
    "observation_sequences = [sample[1] for sample in samples] \n",
    "\n",
    "\n",
    "# the variational model is a random LGMM with same dimensions, and we will not learn the covariances for now \n",
    "v_model = LinearGaussianHMM.get_random_model(2,2)\n",
    "v_model.prior.parametrizations.cov.original.requires_grad = False\n",
    "v_model.transition.parametrizations.cov.original.requires_grad = False \n",
    "v_model.emission.parametrizations.cov.original.requires_grad = False \n",
    "\n",
    "# the elbo object with p and q as arguments\n",
    "elbo = LinearGaussianELBO(hmm.model, v_model)\n",
    "\n",
    "# optimize the parameters of the ELBO (but theta deactivated above)\n",
    "optimizer = torch.optim.Adam(params=elbo.parameters(), lr=1e-2)\n",
    "true_evidence_all_sequences = sum(Kalman(hmm.model).filter(observations)[-1] for observations in observation_sequences)\n",
    "\n",
    "print('True evidence accross all sequences:', true_evidence_all_sequences)\n",
    "\n",
    "eps = torch.inf\n",
    "# optimizing model \n",
    "while eps > 0.1:\n",
    "    epoch_loss = 0.0\n",
    "    for observations in observation_sequences: \n",
    "        optimizer.zero_grad()\n",
    "        loss = -elbo(observations)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += -loss\n",
    "    with torch.no_grad():\n",
    "        eps = torch.abs(true_evidence_all_sequences - epoch_loss)\n",
    "        print('Average of \"L(theta, phi) - log(p_theta(x))\":', eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expectation when smooth with true model: tensor(0.0123707586)\n",
      "Expectation when smooth with variational model: tensor(0.1057920861)\n"
     ]
    }
   ],
   "source": [
    "# checking expectations under approximate model \n",
    "with torch.no_grad():\n",
    "    additive_functional = partial(torch.sum, dim=0)\n",
    "    smoothed_with_true_model = mse_expectation_against_true_states(state_sequences, observation_sequences, hmm.model, additive_functional)\n",
    "    smoothed_with_approximate_model = mse_expectation_against_true_states(state_sequences, observation_sequences, v_model, additive_functional)\n",
    "\n",
    "    print('Expectation when smooth with true model:',smoothed_with_true_model)\n",
    "    print('Expectation when smooth with variational model:',smoothed_with_approximate_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. $p$ has a non linear emission model $x_t = f_\\theta(z_t) + \\epsilon_t$, but $q$ is still a linear Gaussian HMM"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c528143767b35424e5fa616681845f3b9656c31f35cafe040129ce40f24d14f5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('backward_ica')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
