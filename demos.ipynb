{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on backward variational ICA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Uncomment and run the following cell if you're using Collab***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf *\n",
    "# !git clone https://github.com/mchagneux/backward_ica.git\n",
    "# !mv backward_ica/* ./\n",
    "# !rm -rf backward_ica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "from src.elbo import linear_gaussian_elbo\n",
    "from src.hmm import LinearGaussianHMM\n",
    "from src import kalman\n",
    "from jax.random import PRNGKey\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import optax\n",
    "from jax import random\n",
    "from src.misc import *\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "# jax.config.update(\"jax_debug_nans\", True)\n",
    "key = PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook is comprised of a series of experiments that attempt to recover expectations $\\mathbb{E}[h(z_{1:t})|x_{1:t}]$ via variational approximations, when the process $(z_t, x_t)_{t \\ge 1}$ is an HMM. The main metric $\\ell$ all along is the MSE against the true states when $h$ is a plain sum, ie\n",
    "\n",
    "$$\\ell = \\left(\\sum_{t=1}^T z_t^* - \\sum_{t=1}^T \\mathbb{E}_{q_T(z_t)}[z_t] \\right)^2$$\n",
    "\n",
    "where $q_T(z_t) = q(z_t|x_{1:T})$ is the marginal smoothing distribution at $t$.\n",
    "\n",
    "In all the following, we assume that the variational smoothing distribution factorizes as $q_\\phi(z_{1:t}|x_{1:t}) = q_\\phi(z_t|x_{1:t}) \\prod_{s=1}^{t-1} q_\\phi(z_s|z_{s+1},x_{1:s})$. We always assume that $$q_\\phi(z_t|x_{1:t}) \\sim \\mathcal{N}(\\mu_{1:t}, \\Sigma_{1:t})$$ and \n",
    "\n",
    "$$q_\\phi(z_s|z_{s+1},x_{1:s}) \\sim \\mathcal{N}(\\overleftarrow{\\mu}_{1:t}(z_{s+1}), \\overleftarrow{\\Sigma}_{1:t})$$\n",
    "\n",
    "In the following, we make several assumptions on both $p_\\theta$ and $q_\\phi$.\n",
    "\n",
    "\n",
    "In this case, not only should the expectations be correctly recovered, but parameters in $\\phi$ and $\\theta$ should be identifiable. We also know that in this case the best estimate of $z_{1:t}^*$ for any sequence is obtained via the Kalman smoothing recursions applied with parameters $\\theta$ on the observations $x_{1:t}$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Gaussian HMM \n",
    "\n",
    "First we assume that observation sequences $x_{1:T}$ arise from $p_\\theta(z_{1:t},x_{1:t})$ defined as\n",
    "$$z_t = A_\\theta z_{t-1} + a_\\theta + \\eta_\\theta$$ \n",
    "$$x_t = B_\\theta z_t + b_\\theta + \\epsilon_\\theta$$\n",
    "\n",
    "where $\\eta_\\theta \\sim \\mathcal{N}(0,Q_\\theta)$ and $\\epsilon_\\theta \\sim \\mathcal{N}(0,R_\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. a. Approximated by a linear Gaussian HMM\n",
    "\n",
    "We start by recovering $p_\\theta$ when $q_\\phi$ is in the family of the true p. We do this by prescribing the p for $q_\\phi$ in forward time with a similar HMM structure as $p_\\theta$ (but random initial parameters), and in this case the parameters of the filtering backward distributions exist via Kalman recursions and closed-form definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference mean evidence Kalman and mean ELBO when q=p: 2.842170943040401e-14\n",
      "Different mean evidence and mean ELBO when q=q0: 63926.53136294946\n",
      "Change in average elbo from last iteration: 36966.195978419404\n",
      "Change in average elbo from last iteration: 361.8131073848094\n",
      "Change in average elbo from last iteration: 31.633543857033487\n",
      "Change in average elbo from last iteration: 2.264612917233592\n",
      "Change in average elbo from last iteration: 4.431943899362466\n",
      "Change in average elbo from last iteration: 4.236690914804512\n",
      "Change in average elbo from last iteration: 3.8156304158587773\n",
      "Change in average elbo from last iteration: 3.4448717667738435\n",
      "Change in average elbo from last iteration: 2.923155833980985\n",
      "Change in average elbo from last iteration: 2.831830018943066\n",
      "Change in average elbo from last iteration: 2.4411804673708843\n",
      "Change in average elbo from last iteration: 2.358355181386372\n",
      "Change in average elbo from last iteration: 1.9797915903304073\n",
      "Change in average elbo from last iteration: 1.7490912256507158\n",
      "Change in average elbo from last iteration: 1.4370674252821658\n",
      "Change in average elbo from last iteration: 1.0291701016200605\n",
      "Change in average elbo from last iteration: 0.7530997084557143\n",
      "Change in average elbo from last iteration: 0.5614388912262172\n",
      "Change in average elbo from last iteration: 0.436568306796012\n",
      "Change in average elbo from last iteration: 0.3588068450353248\n",
      "Change in average elbo from last iteration: 0.31136569936553116\n",
      "Change in average elbo from last iteration: 0.2774743800279076\n",
      "Change in average elbo from last iteration: 0.25510495319320015\n",
      "Change in average elbo from last iteration: 0.23822278245700446\n",
      "Change in average elbo from last iteration: 0.22532295829649307\n",
      "Change in average elbo from last iteration: 0.21641689191080005\n",
      "Change in average elbo from last iteration: 0.2123368869569724\n",
      "Change in average elbo from last iteration: 0.21387360676053202\n",
      "Change in average elbo from last iteration: 0.22139164741479966\n",
      "Change in average elbo from last iteration: 0.23550064239516333\n",
      "Change in average elbo from last iteration: 0.2558291534893865\n",
      "Change in average elbo from last iteration: 0.2605381214820457\n",
      "Change in average elbo from last iteration: 0.45465910903518125\n",
      "Change in average elbo from last iteration: 0.4036263516935801\n",
      "Change in average elbo from last iteration: 0.45558270475353524\n",
      "Change in average elbo from last iteration: 0.3328313328956156\n",
      "Change in average elbo from last iteration: 0.8176984912612468\n",
      "Change in average elbo from last iteration: 0.33562172561348547\n",
      "Change in average elbo from last iteration: 0.11470512974440794\n",
      "Change in average elbo from last iteration: 0.05015643410088444\n"
     ]
    }
   ],
   "source": [
    "state_dim, obs_dim = 2, 2 \n",
    "num_sequences = 20\n",
    "length = 8\n",
    "\n",
    "key, *subkeys = random.split(key,3)\n",
    "\n",
    "p_raw = LinearGaussianHMM.get_random_model(key=subkeys[0], state_dim=state_dim, obs_dim=obs_dim)\n",
    "q_raw = LinearGaussianHMM.get_random_model(key=subkeys[1], state_dim=state_dim, obs_dim=obs_dim)\n",
    "\n",
    "p = actual_model_from_raw_parameters(p_raw)\n",
    "q = actual_model_from_raw_parameters(q_raw)\n",
    "\n",
    "linear_gaussian_sampler = jax.vmap(LinearGaussianHMM.sample_joint_sequence, in_axes=(0, None, None))\n",
    "key, *subkeys = random.split(key, num_sequences+1)\n",
    "state_sequences, obs_sequences = linear_gaussian_sampler(jnp.array(subkeys), p, length)\n",
    "\n",
    "\n",
    "filter_obs_sequences = jax.vmap(kalman.filter, in_axes=(0, None))\n",
    "elbo_sequences = jax.vmap(linear_gaussian_elbo, in_axes=(None, None, 0))\n",
    "\n",
    "average_evidence_across_sequences = jnp.mean(filter_obs_sequences(obs_sequences, p)[-1])\n",
    "average_elbo_across_sequences_with_true_model = jnp.mean(elbo_sequences(p_raw, p_raw, obs_sequences))\n",
    "print('Difference mean evidence Kalman and mean ELBO when q=p:', jnp.abs(average_evidence_across_sequences-average_elbo_across_sequences_with_true_model))\n",
    "average_elbo_across_sequences_with_init_q = jnp.mean(elbo_sequences(p_raw, q_raw, obs_sequences))\n",
    "print('Different mean evidence and mean ELBO when q=q0:', jnp.abs(average_evidence_across_sequences-average_elbo_across_sequences_with_init_q))\n",
    "\n",
    "\n",
    "optimizer = optax.adam(learning_rate=-1e-3)\n",
    "\n",
    "loss = lambda p_raw, q_raw, observations: linear_gaussian_elbo(p_raw, q_raw, observations)\n",
    "\n",
    "def fit(p_raw, q_raw, optimizer: optax.GradientTransformation) -> optax.Params:\n",
    "    opt_state = optimizer.init(q_raw)\n",
    "\n",
    "    @jax.jit\n",
    "    def step(p_raw, q_raw, opt_state, batch):\n",
    "        loss_value, grads = jax.value_and_grad(loss, argnums=1)(p_raw, q_raw, batch)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state, q_raw)\n",
    "        q_raw = optax.apply_updates(q_raw, updates)\n",
    "        return p_raw, q_raw, opt_state, -loss_value\n",
    "\n",
    "    eps = jnp.inf\n",
    "    old_mean_epoch_loss = -average_elbo_across_sequences_with_init_q\n",
    "    epoch_nb = 0\n",
    "    while eps > 1e-2:\n",
    "        epoch_loss = 0.0\n",
    "        for batch in obs_sequences: \n",
    "            p_raw, q_raw, opt_state, loss_value = step(p_raw, q_raw, opt_state, batch)\n",
    "            epoch_loss += loss_value\n",
    "        mean_epoch_loss = epoch_loss/len(obs_sequences)\n",
    "        eps = jnp.abs(mean_epoch_loss - old_mean_epoch_loss)\n",
    "        if epoch_nb % 10 == 0: print('Change in average elbo from last iteration:', eps)\n",
    "        epoch_nb+=1\n",
    "        old_mean_epoch_loss = mean_epoch_loss\n",
    "    return q_raw \n",
    "\n",
    "\n",
    "fitted_q_raw = fit(p_raw, q_raw, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### need to be changed to jax version \n",
    "# # checking expectations under approximate p when the additive functional is just the sum \n",
    "# with torch.no_grad():\n",
    "#     additive_functional = partial(torch.sum, dim=0)\n",
    "#     smoothed_with_true_model = mse_expectation_against_true_states(state_sequences, observation_sequences, hmm.model, additive_functional)\n",
    "#     smoothed_with_approximate_model = mse_expectation_against_true_states(state_sequences, observation_sequences, q, additive_functional)\n",
    "\n",
    "#     print('MSE when smoothed with p:',smoothed_with_true_model)\n",
    "#     print('MSE when smoothed with q:',smoothed_with_approximate_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. b. Using a neural network to compute the backward parameters instead of Kalman recursions\n",
    "We make the same assumptions on $p_\\theta$ but now we attempt to recover the backward parameters via neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A nonlinear emission p\n",
    "\n",
    "We now assume that $p_\\theta$ has a nonlinear emission distribution, ie. $x_t  = f_\\theta(z_t) + \\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. a. Approximated by a linear Gaussian p.\n",
    "We keep a linear gaussian distribution for $q_\\phi$, but we add a mapping to compute the expectation of the emission term from $p_\\theta$. We need to approximate the following quantity:\n",
    "\n",
    "$$\\mathbb{E}_{q(z_t|z_{t+1}, x_{1:t})}\\left[(x_t - f_\\theta(z_t))^T R^{{\\theta}^{-1}}(x_t - f_\\theta(z_t))\\right]$$\n",
    "\n",
    "And similarly for the last expectation under the filtering distribution: \n",
    "\n",
    "$$\\mathbb{E}_{q(z_T|x_{1:T})}\\left[(x_T - f_\\theta(z_T))^T R^{{\\theta}^{-1}}(x_T - f_\\theta(z_T))\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. a. i. A sampling-free approach. \n",
    "\n",
    "\n",
    "If we know the expectation $\\mu$ and variance $\\Sigma$ of a random variable $v$ (which need not be Gaussian):\n",
    "\n",
    "$$\\mathbb{E}_{v}\\left[(x - v)^T \\Omega (x - v)\\right] = tr(\\Sigma \\Omega) + (\\mu - x)^T \\Omega (\\mu - x)$$\n",
    "\n",
    "Suppose we a have neural network which approximates the mean and variance of $v \\sim f_\\theta(z)$ when $z \\sim p_z$, given parameters of $p_z$. Denote $\\tilde{\\mu}$ and $\\tilde{\\Sigma}$ these means and variances estimated by this network. For the filtering case, we feed the network with filtering mean and covariance at $T$ to obtain an estimate of $\\tilde{\\mu}$ and $\\tilde{\\Sigma}$, then:\n",
    "\n",
    "$$\\mathbb{E}_{q(z_T|x_{1:T})}\\left[(x_T - f_\\theta(z_T))^T R^{{\\theta}^{-1}}(x_T - f_\\theta(z_T))\\right] = tr(\\tilde{\\Sigma} \\Omega) + (\\tilde{\\mu} - x)^T R^{{\\theta}^{-1}} (\\tilde{\\mu} - x)$$\n",
    "\n",
    "For the backwards case this is not as simple, because: $\\overleftarrow{\\mu}_{1:t}$ is a function of $z_{t+1}$, therefore $\\mathbb{E}_{q(z_t|z_{t+1}, x_{1:t})}[f_\\theta(z_t)]$ and $\\mathbb{V}_{q(z_t|z_{t+1}, x_{1:t})}[f_\\theta(z_t)]$ are also functions of $z_{t+1}$. \n",
    "\n",
    "We still attempt to use one network for both the fitlering and the backwards via the following scheme: \n",
    "\n",
    "- Build a neural network $g_\\alpha(A, a, \\Sigma)$ which outputs $\\tilde{A}, \\tilde{a}$ and $\\tilde{\\Sigma}$\n",
    "- For the backwards case, use $A = \\overleftarrow{A}_{1:t}, a = \\overleftarrow{a}_{1:t}$ and $\\Sigma = \\overleftarrow{\\Sigma}_{1:t}$, and consider that $\\tilde{\\mu} = \\tilde{A}z_{t+1} + \\tilde{a}$, while $\\tilde{\\Sigma}$ does not depend on $z_{t+1}$ (which is knowingly false). In this case, the quadratic form build for $\\tilde{A}$ and $\\tilde{a}$ is a quadratic form in $z_{t+1}$ as wanted.\n",
    "- For the backwards case, use $A = 0, a = a_{1:t}$ and $\\Sigma = \\Sigma_{1:t}$, and consider that $\\tilde{\\mu} = \\tilde{a}$ (without using the output $\\tilde{A}$).\n",
    "\n",
    "*This method, which is tried below: fails to learn anything as of now.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. a. i. The Johnson trick\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c528143767b35424e5fa616681845f3b9656c31f35cafe040129ce40f24d14f5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('backward_ica')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
