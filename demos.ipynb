{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on backward variational ICA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Uncomment and run the following cell if you're using Collab***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf *\n",
    "# !git clone https://github.com/mchagneux/backward_ica.git\n",
    "# !mv backward_ica/* ./\n",
    "# !rm -rf backward_ica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n",
      "tensor(-1.4211e-14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathis/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/nn/modules/container.py:597: UserWarning: Setting attributes on ParameterDict is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from src.eval import mse_expectation_against_true_states\n",
    "from src.kalman import Kalman, NumpyKalman\n",
    "from src.hmm import AdditiveGaussianHMM, LinearGaussianHMM\n",
    "from src.elbo import get_appropriate_elbo\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "torch.set_default_dtype(torch.float64) \n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "# torch.set_printoptions(precision=10)\n",
    "\n",
    "## sanity checks\n",
    "hmm = LinearGaussianHMM(state_dim=2, obs_dim=2)\n",
    "states, observations = hmm.sample_joint_sequence(10)\n",
    "\n",
    "for param in hmm.model.parameters():param.requires_grad = False\n",
    "likelihood_torch = Kalman(hmm.model).filter(observations)[4] #kalman with torch operators \n",
    "likelihood_numpy = NumpyKalman(hmm.model).filter(observations.numpy())[2] #kalman with numpy operators \n",
    "fully_linear_gaussian_elbo = get_appropriate_elbo('linear_gaussian','linear_emission')\n",
    "likelihood_via_elbo = fully_linear_gaussian_elbo(hmm.model, hmm.model)(observations) #elbo\n",
    "\n",
    "# both should be close to 0\n",
    "print(likelihood_numpy - likelihood_torch)\n",
    "print(likelihood_numpy - likelihood_via_elbo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook is comprised of a series of experiments that attempt to recover expectations $\\mathbb{E}[h(z_{1:t})|x_{1:t}]$ via variational approximations, when the process $(z_t, x_t)_{t \\ge 1}$ is an HMM. The main metric $\\ell$ all along is the MSE against the true states when $h$ is a plain sum, ie\n",
    "\n",
    "$$\\ell = \\left(\\sum_{t=1}^T z_t^* - \\sum_{t=1}^T \\mathbb{E}_{q_T(z_t)}[z_t] \\right)^2$$\n",
    "\n",
    "where $q_T(z_t) = q(z_t|x_{1:T})$ is the marginal smoothing distribution at $t$.\n",
    "\n",
    "In all the following, we assume that the variational smoothing distribution factorizes as $q_\\phi(z_{1:t}|x_{1:t}) = q_\\phi(z_t|x_{1:t}) \\prod_{s=1}^{t-1} q_\\phi(z_s|z_{s+1},x_{1:s})$. We always assume that $$q_\\phi(z_t|x_{1:t}) \\sim \\mathcal{N}(\\mu_{1:t}, \\Sigma_{1:t})$$ and \n",
    "\n",
    "$$q_\\phi(z_s|z_{s+1},x_{1:s}) \\sim \\mathcal{N}(\\overleftarrow{\\mu}_{1:t}(z_{s+1}), \\overleftarrow{\\Sigma}_{1:t})$$\n",
    "\n",
    "In the following, we make several assumptions on both $p_\\theta$ and $q_\\phi$.\n",
    "\n",
    "\n",
    "In this case, not only should the expectations be correctly recovered, but parameters in $\\phi$ and $\\theta$ should be identifiable. We also know that in this case the best estimate of $z_{1:t}^*$ for any sequence is obtained via the Kalman smoothing recursions applied with parameters $\\theta$ on the observations $x_{1:t}$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Gaussian HMM \n",
    "\n",
    "First we assume that observation sequences $x_{1:T}$ arise from $p_\\theta(z_{1:t},x_{1:t})$ defined as\n",
    "$$z_t = A_\\theta z_{t-1} + a_\\theta + \\eta_\\theta$$ \n",
    "$$x_t = B_\\theta z_t + b_\\theta + \\epsilon_\\theta$$\n",
    "\n",
    "where $\\eta_\\theta \\sim \\mathcal{N}(0,Q_\\theta)$ and $\\epsilon_\\theta \\sim \\mathcal{N}(0,R_\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. a. Approximated by a linear Gaussian HMM\n",
    "\n",
    "We start by recovering $p_\\theta$ when $q_\\phi$ is in the family of the true model. We do this by prescribing the model for $q_\\phi$ in forward time with a similar HMM structure as $p_\\theta$ (but random initial parameters), and in this case the parameters of the filtering backward distributions exist via Kalman recursions and closed-form definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True evidence accross all sequences: tensor(285.0427)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(2670.9395)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(736.5879)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(674.3642)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(451.8646)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(281.1790)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(196.5590)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(133.7245)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(104.3276)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(93.4044)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(89.8911)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(86.5933)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(81.8028)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(76.1348)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(70.5672)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(65.9291)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(62.4507)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(59.7904)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(57.5015)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(55.3578)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(53.3514)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(51.5350)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(49.9125)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(48.4420)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(47.0836)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(45.8185)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(44.6382)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(43.5328)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(42.4904)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(41.5014)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(40.5587)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(39.6572)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(38.7926)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(37.9609)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(37.1590)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(36.3847)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(35.6360)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(34.9113)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(34.2094)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(33.5293)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(32.8700)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(32.2308)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(31.6110)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(31.0102)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(30.4277)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(29.8632)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(29.3161)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(28.7860)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(28.2726)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(27.7753)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(27.2937)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(26.8275)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(26.3761)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(25.9391)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(25.5161)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(25.1067)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(24.7102)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(24.3264)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(23.9546)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(23.5945)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(23.2456)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(22.9073)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(22.5793)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(22.2611)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(21.9521)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(21.6520)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(21.3604)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(21.0767)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(20.8006)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(20.5316)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(20.2694)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(20.0135)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(19.7636)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(19.5194)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(19.2805)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(19.0465)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(18.8172)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(18.5923)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(18.3714)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(18.1543)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(17.9407)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(17.7305)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(17.5233)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(17.3190)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(17.1173)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(16.9181)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(16.7211)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(16.5263)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(16.3335)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(16.1425)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(15.9531)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(15.7654)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(15.5791)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(15.3941)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(15.2104)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(15.0279)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(14.8464)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(14.6659)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(14.4864)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(14.3078)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(14.1299)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(13.9529)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(13.7766)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(13.6010)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(13.4261)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(13.2519)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(13.0782)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(12.9052)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(12.7328)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(12.5610)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(12.3897)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(12.2191)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(12.0490)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(11.8796)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(11.7108)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(11.5426)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(11.3751)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(11.2083)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(11.0422)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(10.8768)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(10.7122)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(10.5484)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(10.3854)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(10.2234)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(10.0622)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(9.9020)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(9.7429)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(9.5848)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(9.4278)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(9.2719)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(9.1172)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(8.9638)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(8.8117)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(8.6610)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(8.5116)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(8.3636)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(8.2172)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(8.0722)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(7.9288)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(7.7871)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(7.6469)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(7.5084)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(7.3717)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(7.2367)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(7.1034)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(6.9719)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(6.8423)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(6.7144)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(6.5884)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(6.4643)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(6.3420)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(6.2216)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(6.1031)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(5.9864)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(5.8717)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(5.7587)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(5.6477)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(5.5385)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(5.4312)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(5.3257)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(5.2220)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(5.1202)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(5.0201)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(4.9219)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(4.8253)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(4.7306)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(4.6375)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(4.5462)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(4.4566)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(4.3686)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(4.2823)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(4.1976)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(4.1145)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(4.0330)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(3.9531)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(3.8747)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(3.7978)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(3.7225)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(3.6486)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(3.5762)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(3.5053)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(3.4358)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(3.3676)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(3.3009)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(3.2355)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(3.1715)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(3.1088)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(3.0474)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(2.9873)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(2.9284)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(2.8708)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(2.8144)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(2.7591)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(2.7051)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(2.6522)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(2.6004)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(2.5498)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(2.5002)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(2.4517)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(2.4043)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(2.3578)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(2.3124)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(2.2679)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(2.2243)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(2.1817)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(2.1401)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(2.0993)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(2.0593)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(2.0202)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.9819)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.9444)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.9078)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.8718)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.8366)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.8022)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.7684)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.7353)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.7029)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.6711)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.6400)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.6095)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.5796)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.5503)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.5216)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.4934)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.4658)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.4387)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.4121)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.3860)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.3604)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.3353)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.3107)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.2865)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.2628)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.2395)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.2167)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.1942)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.1722)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.1505)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.1293)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.1084)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.0879)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.0678)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.0481)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.0287)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(1.0096)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.9909)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.9725)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.9545)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.9369)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.9195)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.9025)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.8859)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.8696)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.8537)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.8380)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.8228)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.8079)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.7933)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.7791)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.7652)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.7516)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.7384)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.7255)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.7129)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.7007)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.6887)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.6771)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.6657)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.6547)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.6439)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.6334)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.6231)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.6131)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.6034)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.5939)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.5846)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.5755)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.5666)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.5579)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.5494)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.5411)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.5330)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.5250)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.5172)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.5096)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.5021)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.4947)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.4875)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.4804)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.4735)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.4667)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.4600)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.4534)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.4469)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.4405)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.4342)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.4280)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.4220)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.4160)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.4101)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.4043)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.3986)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.3930)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.3874)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.3819)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.3766)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.3712)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.3660)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.3608)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.3557)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.3507)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.3457)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.3408)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.3360)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.3312)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.3265)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.3218)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.3173)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.3127)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.3082)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.3038)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.2994)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.2951)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.2908)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.2866)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.2825)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.2784)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.2743)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.2703)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.2663)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.2624)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.2585)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.2547)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.2509)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.2471)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.2434)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.2398)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.2362)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.2326)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.2291)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.2256)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.2221)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.2187)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.2154)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.2120)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.2088)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.2055)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.2023)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1991)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1960)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1929)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1898)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1868)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1838)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1809)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1780)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1751)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1722)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1694)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1667)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1639)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1612)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1585)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1559)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1533)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1507)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1482)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1456)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1432)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1407)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1383)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1359)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1336)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1312)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1289)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1267)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1244)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1222)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1201)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1179)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1158)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1137)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1117)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1096)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1076)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1056)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1037)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.1018)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(0.0999)\n"
     ]
    }
   ],
   "source": [
    "hmm = LinearGaussianHMM(state_dim=2, obs_dim=2) # pick some true model p \n",
    "for param in hmm.model.parameters(): param.requires_grad = False # not learning the parameters of the true model for now \n",
    "\n",
    "\n",
    "\n",
    "# sampling 10 sequences from the hmm \n",
    "samples = [hmm.sample_joint_sequence(8) for _ in range(10)] \n",
    "state_sequences = [sample[0] for sample in samples]\n",
    "observation_sequences = [sample[1] for sample in samples] \n",
    "\n",
    "\n",
    "# the variational model is a random LGMM with same dimensions, and we will not learn the covariances for now\n",
    "v_model = LinearGaussianHMM.get_random_model(2,2)\n",
    "v_model.prior.parametrizations.cov.original.requires_grad = False\n",
    "v_model.transition.parametrizations.cov.original.requires_grad = False \n",
    "v_model.emission.parametrizations.cov.original.requires_grad = False \n",
    "\n",
    "# the elbo object with p and q as arguments\n",
    "elbo = fully_linear_gaussian_elbo(hmm.model, v_model)\n",
    "\n",
    "# optimize the parameters of the ELBO (but theta deactivated above)\n",
    "optimizer = torch.optim.Adam(params=elbo.parameters(), lr=1e-2)\n",
    "true_evidence_all_sequences = sum(Kalman(hmm.model).filter(observations)[-1] for observations in observation_sequences)\n",
    "\n",
    "print('True evidence accross all sequences:', true_evidence_all_sequences)\n",
    "\n",
    "eps = torch.inf\n",
    "# optimizing model \n",
    "while eps > 0.1:\n",
    "    epoch_loss = 0.0\n",
    "    for observations in observation_sequences: \n",
    "        optimizer.zero_grad()\n",
    "        loss = -elbo(observations)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += -loss\n",
    "    with torch.no_grad():\n",
    "        eps = torch.abs(true_evidence_all_sequences - epoch_loss)\n",
    "        print('Average of \"L(theta, phi) - log(p_theta(x))\":', eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE when smoothed with true model: tensor(0.0149)\n",
      "MSE when smoothed with variational model: tensor(0.0151)\n"
     ]
    }
   ],
   "source": [
    "# checking expectations under approximate model when the additive functional is just the sum \n",
    "with torch.no_grad():\n",
    "    additive_functional = partial(torch.sum, dim=0)\n",
    "    smoothed_with_true_model = mse_expectation_against_true_states(state_sequences, observation_sequences, hmm.model, additive_functional)\n",
    "    smoothed_with_approximate_model = mse_expectation_against_true_states(state_sequences, observation_sequences, v_model, additive_functional)\n",
    "\n",
    "    print('MSE when smoothed with true model:',smoothed_with_true_model)\n",
    "    print('MSE when smoothed with variational model:',smoothed_with_approximate_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. b. Using a neural network to compute the backward parameters instead of Kalman recursions\n",
    "We make the same assumptions on $p_\\theta$ but now we attempt to recover the backward parameters via neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A nonlinear emission model\n",
    "\n",
    "We now assume that $p_\\theta$ has a nonlinear emission distribution, ie. $x_t  = f_\\theta(z_t) + \\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. a. Approximated by a linear Gaussian model.\n",
    "We keep a linear gaussian distribution for $q_\\phi$, but we add a mapping to compute the expectation of the emission term from $p_\\theta$. We need to approximate the following quantity:\n",
    "\n",
    "$$\\mathbb{E}_{q(z_t|z_{t+1}, x_{1:t})}\\left[(x_t - f_\\theta(z_t))^T R^{{\\theta}^{-1}}(x_t - f_\\theta(z_t))\\right]$$\n",
    "\n",
    "And similarly for the last expectation under the filtering distribution: \n",
    "\n",
    "$$\\mathbb{E}_{q(z_T|x_{1:T})}\\left[(x_T - f_\\theta(z_T))^T R^{{\\theta}^{-1}}(x_T - f_\\theta(z_T))\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. a. i. A sampling-free approach. \n",
    "\n",
    "\n",
    "If we know the expectation $\\mu$ and variance $\\Sigma$ of a random variable $v$ (which need not be Gaussian):\n",
    "\n",
    "$$\\mathbb{E}_{v}\\left[(x - v)^T \\Omega (x - v)\\right] = tr(\\Sigma \\Omega) + (\\mu - x)^T \\Omega (\\mu - x)$$\n",
    "\n",
    "Suppose we a have neural network which approximates the mean and variance of $v \\sim f_\\theta(z)$ when $z \\sim p_z$, given parameters of $p_z$. Denote $\\tilde{\\mu}$ and $\\tilde{\\Sigma}$ these means and variances estimated by this network. For the filtering case, we feed the network with filtering mean and covariance at $T$ to obtain an estimate of $\\tilde{\\mu}$ and $\\tilde{\\Sigma}$, then:\n",
    "\n",
    "$$\\mathbb{E}_{q(z_T|x_{1:T})}\\left[(x_T - f_\\theta(z_T))^T R^{{\\theta}^{-1}}(x_T - f_\\theta(z_T))\\right] = tr(\\tilde{\\Sigma} \\Omega) + (\\tilde{\\mu} - x)^T R^{{\\theta}^{-1}} (\\tilde{\\mu} - x)$$\n",
    "\n",
    "For the backwards case this is not as simple, because: $\\overleftarrow{\\mu}_{1:t}$ is a function of $z_{t+1}$, therefore $\\mathbb{E}_{q(z_t|z_{t+1}, x_{1:t})}[f_\\theta(z_t)]$ and $\\mathbb{V}_{q(z_t|z_{t+1}, x_{1:t})}[f_\\theta(z_t)]$ are also functions of $z_{t+1}$. \n",
    "\n",
    "We still attempt to use one network for both the fitlering and the backwards via the following scheme: \n",
    "\n",
    "- Build a neural network $g_\\alpha(A, a, \\Sigma)$ which outputs $\\tilde{A}, \\tilde{a}$ and $\\tilde{\\Sigma}$\n",
    "- For the backwards case, use $A = \\overleftarrow{A}_{1:t}, a = \\overleftarrow{a}_{1:t}$ and $\\Sigma = \\overleftarrow{\\Sigma}_{1:t}$, and consider that $\\tilde{\\mu} = \\tilde{A}z_{t+1} + \\tilde{a}$, while $\\tilde{\\Sigma}$ does not depend on $z_{t+1}$ (which is knowingly false). In this case, the quadratic form build for $\\tilde{A}$ and $\\tilde{a}$ is a quadratic form in $z_{t+1}$ as wanted.\n",
    "- For the backwards case, use $A = 0, a = a_{1:t}$ and $\\Sigma = \\Sigma_{1:t}$, and consider that $\\tilde{\\mu} = \\tilde{a}$ (without using the output $\\tilde{A}$).\n",
    "\n",
    "*This method, which is tried below: fails to learn anything as of now.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathis/repos/backward_ica/src/elbo.py:45: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  self.cov_tilde_approximator = nn.Sequential(nn.Linear(in_features=input_shape, out_features=(obs_dim * (obs_dim + 1)) // 2, bias=True), nn.ReLU())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(1.8077e+16, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(909259.7224, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(30076905.0721, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(96596.4490, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(138113.5901, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(95605.0150, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(2.9038e+12, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(951313.3204, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(1.4477e+11, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(4.2146e+11, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(4987929.0718, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(1.8323e+13, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(9.2182e+09, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(1.0273e+08, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(4.2118e+12, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(143361.5022, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(6.9418e+09, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(35551587.5425, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(12505520.8165, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(229045.7456, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(2137743.6233, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(62248773.2948, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(1.7414e+09, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(8.0716e+11, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(6.1788e+11, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(6.6977e+11, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(2.8797e+19, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(8.7267e+17, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(6.1720e+11, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(2.1661e+10, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(1.9947e+11, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(1.0512e+10, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(5.2044e+11, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(1.1491e+13, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(1.3829e+12, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(6.3243e+16, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(1.4822e+17, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(8.1068e+10, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(2.1089e+11, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(2859304.2048, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(8.8377e+16, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(5.9812e+09, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(5.7745e+10, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(2.5444e+17, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(1.4587e+13, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(9.6285e+10, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(5.1034e+16, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(9.1516e+09, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(1.3663e+12, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(1.0240e+12, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(2.9379e+12, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(7.1763e+09, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(2.2025e+11, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(6.0870e+16, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(2.6277e+10, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(1.8339e+14, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(2.2711e+10, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(7.6971e+09, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(7.9148e+16, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(4.4498e+11, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(1.6470e+17, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(1.3228e+11, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(15421105.7894, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(2.6086e+11, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(3.1697e+16, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(1.6321e+11, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(3.2022e+16, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(1.1085e+11, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(1.4171e+17, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(3.3541e+11, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(3.1681e+10, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mathis/repos/backward_ica/demos.ipynb Cell 15'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mathis/repos/backward_ica/demos.ipynb#ch0000013?line=32'>33</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mathis/repos/backward_ica/demos.ipynb#ch0000013?line=33'>34</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39melbo(observations)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mathis/repos/backward_ica/demos.ipynb#ch0000013?line=34'>35</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mathis/repos/backward_ica/demos.ipynb#ch0000013?line=35'>36</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mathis/repos/backward_ica/demos.ipynb#ch0000013?line=36'>37</a>\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m-\u001b[39mloss\n",
      "File \u001b[0;32m~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/_tensor.py?line=297'>298</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/_tensor.py?line=298'>299</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/_tensor.py?line=299'>300</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/_tensor.py?line=300'>301</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/_tensor.py?line=304'>305</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/_tensor.py?line=305'>306</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/_tensor.py?line=306'>307</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/autograd/__init__.py?line=150'>151</a>\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/autograd/__init__.py?line=151'>152</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/autograd/__init__.py?line=153'>154</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/autograd/__init__.py?line=154'>155</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/autograd/__init__.py?line=155'>156</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hmm = AdditiveGaussianHMM(state_dim=2, obs_dim=2) # we now take an hmm wih \n",
    "\n",
    "# sampling 10 sequences from the hmm \n",
    "samples = [hmm.sample_joint_sequence(8) for _ in range(10)] \n",
    "state_sequences = [sample[0] for sample in samples]\n",
    "observation_sequences = [sample[1] for sample in samples] \n",
    "\n",
    "\n",
    "# the variational model is a random LGMM with same dimensions, and we will not learn the covariances for now\n",
    "v_model = LinearGaussianHMM.get_random_model(2,2)\n",
    "v_model.prior.parametrizations.cov.original.requires_grad = False\n",
    "v_model.transition.parametrizations.cov.original.requires_grad = False \n",
    "v_model.emission.parametrizations.cov.original.requires_grad = False \n",
    "\n",
    "\n",
    "elbo_nonlinear_emission = get_appropriate_elbo(variational_model_description='linear_gaussian', true_model_description='nonlinear_emission')\n",
    "\n",
    "elbo = elbo_nonlinear_emission(hmm.model, v_model)\n",
    "\n",
    "# print(elbo_nonlinear_emission(observation_sequences[0]))\n",
    "\n",
    "\n",
    "\n",
    "# optimize the parameters of the ELBO (but theta deactivated above)\n",
    "optimizer = torch.optim.Adam(params=elbo.parameters(), lr=1e-2)\n",
    "\n",
    "\n",
    "eps = torch.inf\n",
    "# optimizing model \n",
    "while True:\n",
    "    epoch_loss = 0.0\n",
    "    for observations in observation_sequences: \n",
    "        optimizer.zero_grad()\n",
    "        loss = -elbo(observations)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += -loss\n",
    "    with torch.no_grad():\n",
    "        print(\"Loss:\", epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. a. i. Sampling and the Johnson trick.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c528143767b35424e5fa616681845f3b9656c31f35cafe040129ce40f24d14f5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('backward_ica')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
