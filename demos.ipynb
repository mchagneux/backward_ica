{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on backward variational ICA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Uncomment and run the following cell if you're using Collab***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf *\n",
    "# !git clone https://github.com/mchagneux/backward_ica.git\n",
    "# !mv backward_ica/* ./\n",
    "# !rm -rf backward_ica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "from jax import numpy as jnp, random, config, jit, vmap, value_and_grad\n",
    "import optax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from backward_ica.elbo import linear_gaussian_elbo\n",
    "from backward_ica.hmm import LinearGaussianHMM\n",
    "from backward_ica.kalman import filter as kalman_filter, smooth as kalman_smooth\n",
    "from backward_ica.misc import parameters_from_raw_parameters\n",
    "\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "key = random.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook is comprised of a series of experiments that attempt to recover expectations $\\mathbb{E}[h(z_{1:t})|x_{1:t}]$ via variational approximations, when the process $(z_t, x_t)_{t \\ge 1}$ is an HMM. The main metric $\\ell$ all along is the MSE against the true states when $h$ is a plain sum, ie\n",
    "\n",
    "$$\\ell = \\left(\\sum_{t=1}^T z_t^* - \\sum_{t=1}^T \\mathbb{E}_{q_T(z_t)}[z_t] \\right)^2$$\n",
    "\n",
    "where $q_T(z_t) = q(z_t|x_{1:T})$ is the marginal smoothing distribution at $t$.\n",
    "\n",
    "In all the following, we assume that the variational smoothing distribution factorizes as $q_\\phi(z_{1:t}|x_{1:t}) = q_\\phi(z_t|x_{1:t}) \\prod_{s=1}^{t-1} q_\\phi(z_s|z_{s+1},x_{1:s})$. We always assume that $$q_\\phi(z_t|x_{1:t}) \\sim \\mathcal{N}(\\mu_{1:t}, \\Sigma_{1:t})$$ and \n",
    "\n",
    "$$q_\\phi(z_s|z_{s+1},x_{1:s}) \\sim \\mathcal{N}(\\overleftarrow{\\mu}_{1:t}(z_{s+1}), \\overleftarrow{\\Sigma}_{1:t})$$\n",
    "\n",
    "In the following, we make several assumptions on both $p_\\theta$ and $q_\\phi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Gaussian HMM \n",
    "\n",
    "First we assume that observation sequences $x_{1:T}$ arise from $p_\\theta(z_{1:t},x_{1:t})$ defined as\n",
    "$$z_t = A_\\theta z_{t-1} + a_\\theta + \\eta_\\theta$$ \n",
    "$$x_t = B_\\theta z_t + b_\\theta + \\epsilon_\\theta$$\n",
    "\n",
    "where $\\eta_\\theta \\sim \\mathcal{N}(0,Q_\\theta)$ and $\\epsilon_\\theta \\sim \\mathcal{N}(0,R_\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. a. Approximated by a linear Gaussian HMM\n",
    "\n",
    "We start by recovering $p_\\theta$ when $q_\\phi$ is in the family of the true $p_\\theta$. We can do this by prescribing the model for $q_\\phi$ in forward time with a similar HMM structure as $p_\\theta$ (but random initial parameters), and in this case exceptionally, the parameters of the backward and filtering distributions are linked in closed-formed to the forward model via the Kalman filtering and smoothing recursions.\n",
    "\n",
    "For this experiment, not only should the expectations be correctly recovered, but parameters in $\\phi$ and $\\theta$ may be identifiable in some cases (depending on the conditioning of $p_\\theta$). We also know that in this case the best estimate of $z_{1:t}^*$ for any sequence is obtained via the Kalman smoothing recursions applied with the true parameters $\\theta$ on the observations $x_{1:t}$, so we have an optimal estimator to compare to. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking that $\\mathcal{L}(\\theta, \\theta) = \\log p_\\theta$ \n",
    "*Remark: here $\\log p$ is computed with Kalman*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Number of observations per sequence: 16"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Number of sequences: 30"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/latex": [
       "Average $\\log p_\\theta(x)$ across sequences $x$: 142.68"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Argument to inv must have shape [..., n, n], got (2,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/mathis/repos/backward_ica/demos.ipynb Cell 10'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mathis/repos/backward_ica/demos.ipynb#ch0000025?line=20'>21</a>\u001b[0m average_evidence_across_sequences \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mmean(filter_obs_sequences(obs_sequences, p)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mathis/repos/backward_ica/demos.ipynb#ch0000025?line=21'>22</a>\u001b[0m display(Latex(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAverage $\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mlog p_\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39mtheta(x)$ across sequences $x$: \u001b[39m\u001b[39m{\u001b[39;00maverage_evidence_across_sequences\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mathis/repos/backward_ica/demos.ipynb#ch0000025?line=22'>23</a>\u001b[0m average_elbo_across_sequences_with_true_model \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mmean(elbo_sequences(p_raw, p_raw, obs_sequences))\n",
      "    \u001b[0;31m[... skipping hidden 17 frame]\u001b[0m\n",
      "File \u001b[0;32m~/repos/backward_ica/backward_ica/elbo.py:202\u001b[0m, in \u001b[0;36mlinear_gaussian_elbo\u001b[0;34m(p_raw, q_raw, observations)\u001b[0m\n\u001b[1;32m    <a href='file:///~/repos/backward_ica/backward_ica/elbo.py?line=199'>200</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlinear_gaussian_elbo\u001b[39m(p_raw, q_raw, observations):\n\u001b[0;32m--> <a href='file:///~/repos/backward_ica/backward_ica/elbo.py?line=201'>202</a>\u001b[0m     p \u001b[39m=\u001b[39m prepare_parameters(p_raw)\n\u001b[1;32m    <a href='file:///~/repos/backward_ica/backward_ica/elbo.py?line=202'>203</a>\u001b[0m     q \u001b[39m=\u001b[39m prepare_parameters(q_raw)\n\u001b[1;32m    <a href='file:///~/repos/backward_ica/backward_ica/elbo.py?line=204'>205</a>\u001b[0m     constants_V, quad_forms, nonlinear_term, q_filtering \u001b[39m=\u001b[39m init(observations, p, q)\n",
      "File \u001b[0;32m~/repos/backward_ica/backward_ica/elbo.py:191\u001b[0m, in \u001b[0;36mprepare_parameters\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    <a href='file:///~/repos/backward_ica/backward_ica/elbo.py?line=186'>187</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprepare_parameters\u001b[39m(model):\n\u001b[1;32m    <a href='file:///~/repos/backward_ica/backward_ica/elbo.py?line=188'>189</a>\u001b[0m     model \u001b[39m=\u001b[39m parameters_from_raw_parameters(model)\n\u001b[0;32m--> <a href='file:///~/repos/backward_ica/backward_ica/elbo.py?line=190'>191</a>\u001b[0m     model[\u001b[39m'\u001b[39m\u001b[39mtransition\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mprec\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m  jnp\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49minv(model[\u001b[39m'\u001b[39;49m\u001b[39mtransition\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mcov\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    <a href='file:///~/repos/backward_ica/backward_ica/elbo.py?line=191'>192</a>\u001b[0m     model[\u001b[39m'\u001b[39m\u001b[39mtransition\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mdet_cov\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mdet(model[\u001b[39m'\u001b[39m\u001b[39mtransition\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcov\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    <a href='file:///~/repos/backward_ica/backward_ica/elbo.py?line=193'>194</a>\u001b[0m     model[\u001b[39m'\u001b[39m\u001b[39memission\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mprec\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m  jnp\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39minv(model[\u001b[39m'\u001b[39m\u001b[39memission\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcov\u001b[39m\u001b[39m'\u001b[39m])\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "File \u001b[0;32m~/envs/backward_ica/lib/python3.8/site-packages/jax/_src/numpy/linalg.py:392\u001b[0m, in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    <a href='file:///~/envs/backward_ica/lib/python3.8/site-packages/jax/_src/numpy/linalg.py?line=387'>388</a>\u001b[0m \u001b[39m@_wraps\u001b[39m(np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39minv)\n\u001b[1;32m    <a href='file:///~/envs/backward_ica/lib/python3.8/site-packages/jax/_src/numpy/linalg.py?line=388'>389</a>\u001b[0m \u001b[39m@jit\u001b[39m\n\u001b[1;32m    <a href='file:///~/envs/backward_ica/lib/python3.8/site-packages/jax/_src/numpy/linalg.py?line=389'>390</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minv\u001b[39m(a):\n\u001b[1;32m    <a href='file:///~/envs/backward_ica/lib/python3.8/site-packages/jax/_src/numpy/linalg.py?line=390'>391</a>\u001b[0m   \u001b[39mif\u001b[39;00m jnp\u001b[39m.\u001b[39mndim(a) \u001b[39m<\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m a\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m a\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]:\n\u001b[0;32m--> <a href='file:///~/envs/backward_ica/lib/python3.8/site-packages/jax/_src/numpy/linalg.py?line=391'>392</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///~/envs/backward_ica/lib/python3.8/site-packages/jax/_src/numpy/linalg.py?line=392'>393</a>\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mArgument to inv must have shape [..., n, n], got \u001b[39m\u001b[39m{\u001b[39;00ma\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='file:///~/envs/backward_ica/lib/python3.8/site-packages/jax/_src/numpy/linalg.py?line=393'>394</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m solve(\n\u001b[1;32m    <a href='file:///~/envs/backward_ica/lib/python3.8/site-packages/jax/_src/numpy/linalg.py?line=394'>395</a>\u001b[0m     a, lax\u001b[39m.\u001b[39mbroadcast(jnp\u001b[39m.\u001b[39meye(a\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], dtype\u001b[39m=\u001b[39mlax\u001b[39m.\u001b[39mdtype(a)), a\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]))\n",
      "\u001b[0;31mValueError\u001b[0m: Argument to inv must have shape [..., n, n], got (2,)."
     ]
    }
   ],
   "source": [
    "state_dim, obs_dim = 2, 3\n",
    "num_sequences = 30\n",
    "length = 16\n",
    "display(Markdown(f'Number of observations per sequence: {length}'))\n",
    "display(Markdown(f'Number of sequences: {num_sequences}'))\n",
    "\n",
    "key, *subkeys = random.split(key,3)\n",
    "\n",
    "p_raw = LinearGaussianHMM.get_random_model(key=subkeys[0], state_dim=state_dim, obs_dim=obs_dim)\n",
    "\n",
    "p = parameters_from_raw_parameters(p_raw)\n",
    "\n",
    "linear_gaussian_sampler = vmap(LinearGaussianHMM.sample_joint_sequence, in_axes=(0, None, None))\n",
    "key, *subkeys = random.split(key, num_sequences+1)\n",
    "state_sequences, obs_sequences = linear_gaussian_sampler(jnp.array(subkeys), p, length)\n",
    "\n",
    "\n",
    "filter_obs_sequences = vmap(kalman_filter, in_axes=(0, None))\n",
    "elbo_sequences = jit(vmap(linear_gaussian_elbo, in_axes=(None, None, 0)))\n",
    "\n",
    "average_evidence_across_sequences = jnp.mean(filter_obs_sequences(obs_sequences, p)[-1])\n",
    "display(Latex(f'Average $\\log p_\\\\theta(x)$ across sequences $x$: {average_evidence_across_sequences:.2f}'))\n",
    "average_elbo_across_sequences_with_true_model = jnp.mean(elbo_sequences(p_raw, p_raw, obs_sequences))\n",
    "# display(Latex('Average $|\\mathcal{L}(\\\\theta,\\\\theta)- \\log p_\\\\theta(x)|$ across sequences: ' + f'{jnp.abs(average_evidence_across_sequences-av"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizing $\\phi$ when $\\theta$ is fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def step(p_raw, q_raw, opt_state, batch):\n",
    "    loss_value, grads = value_and_grad(linear_gaussian_elbo, argnums=1)(p_raw, q_raw, batch)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, q_raw)\n",
    "    q_raw = optax.apply_updates(q_raw, updates)\n",
    "    return p_raw, q_raw, opt_state, -loss_value\n",
    "\n",
    "q_raw = LinearGaussianHMM.get_random_model(key=subkeys[1], state_dim=state_dim, obs_dim=obs_dim)\n",
    "average_elbo_across_sequences_with_init_q = jnp.mean(elbo_sequences(p_raw, q_raw, obs_sequences))\n",
    "display(Latex('Average $|\\mathcal{L}(\\\\theta,\\\\phi)- \\log p_\\\\theta(x)|$ across sequences:' +  f'{jnp.abs(average_evidence_across_sequences-average_elbo_across_sequences_with_init_q):.2f}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adam(learning_rate=-1e-3)\n",
    "\n",
    "def fit(p_raw, q_raw, optimizer: optax.GradientTransformation) -> optax.Params:\n",
    "    opt_state = optimizer.init(q_raw)\n",
    "\n",
    "    eps = jnp.inf\n",
    "    old_mean_epoch_elbo = -average_elbo_across_sequences_with_init_q\n",
    "    epoch_nb = 0\n",
    "    mean_elbos = [old_mean_epoch_elbo - average_evidence_across_sequences]\n",
    "    while eps > 1e-2:\n",
    "        epoch_elbo = 0.0\n",
    "        for batch in obs_sequences: \n",
    "            p_raw, q_raw, opt_state, elbo_value = step(p_raw, q_raw, opt_state, batch)\n",
    "            epoch_elbo += elbo_value\n",
    "        mean_epoch_elbo = epoch_elbo/len(obs_sequences)\n",
    "        eps = jnp.abs(mean_epoch_elbo - old_mean_epoch_elbo)\n",
    "        epoch_nb+=1\n",
    "        mean_elbos.append(mean_epoch_elbo - average_evidence_across_sequences)\n",
    "        old_mean_epoch_elbo = mean_epoch_elbo\n",
    "    return q_raw, mean_elbos\n",
    "\n",
    "\n",
    "fitted_q_raw, mean_elbos = fit(p_raw, q_raw, optimizer)\n",
    "\n",
    "plt.plot(mean_elbos)\n",
    "plt.xlabel('Epoch nb'), \n",
    "plt.ylabel('$|\\mathcal{L}(\\\\theta,\\\\phi)- \\log p_\\\\theta(x)|$')\n",
    "fitted_q = actual_model_from_raw_parameters(fitted_q_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing expectations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squared_error_expectation_against_true_states(states, observations, approximate_linear_gaussian_model, additive_functional):\n",
    "    smoothed_states, _ = kalman_smooth(observations, approximate_linear_gaussian_model)\n",
    "    return jnp.sqrt((additive_functional(smoothed_states) - additive_functional(states)) ** 2)\n",
    "\n",
    "additive_functional = partial(jnp.sum, axis=0)\n",
    "mse_in_expectations = vmap(squared_error_expectation_against_true_states, in_axes=(0,0, None, None))\n",
    "print('MSE(E_q(h(z)), z_true):', jnp.mean(mse_in_expectations(state_sequences, obs_sequences, fitted_q, additive_functional), axis=0))\n",
    "print('MSE(E_p(h(z)), z_true):', jnp.mean(mse_in_expectations(state_sequences, obs_sequences, p, additive_functional), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing values of $\\phi$ and $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = 4\n",
    "def convert_to_plain_python_dict(model):\n",
    "    prior = {k: np.around(np.array(v),precision) for k,v in model.prior._asdict().items()}\n",
    "    transition = {k: np.around(np.array(v),precision) for k,v in model.transition._asdict().items()}\n",
    "    emission = {k: np.around(np.array(v),precision) for k,v in model.emission._asdict().items()}\n",
    "    return {'prior':prior, 'transition':transition, 'emission':emission}\n",
    "    \n",
    "\n",
    "print('theta')\n",
    "display(pd.DataFrame(convert_to_plain_python_dict(p)).T)\n",
    "print('phi')\n",
    "display(pd.DataFrame(convert_to_plain_python_dict(fitted_q)).T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It seems that expectations are recovered quite well even though some parameters differ.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. b. Using a neural network to compute the backward parameters instead of Kalman recursions\n",
    "We make the same assumptions on $p_\\theta$ but now we attempt to recover the backward parameters via neural network.\n",
    "\n",
    "*TODO*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A nonlinear emission p\n",
    "\n",
    "We now assume that $p_\\theta$ has a nonlinear emission distribution, ie. $x_t  = f_\\theta(z_t) + \\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. a. Approximated by a linear Gaussian p.\n",
    "We keep a linear gaussian distribution for $q_\\phi$, but we add a mapping to compute the expectation of the emission term from $p_\\theta$. We need to approximate the following quantity:\n",
    "\n",
    "$$\\mathbb{E}_{q(z_t|z_{t+1}, x_{1:t})}\\left[(x_t - f_\\theta(z_t))^T R^{{\\theta}^{-1}}(x_t - f_\\theta(z_t))\\right]$$\n",
    "\n",
    "And similarly for the last expectation under the filtering distribution: \n",
    "\n",
    "$$\\mathbb{E}_{q(z_T|x_{1:T})}\\left[(x_T - f_\\theta(z_T))^T R^{{\\theta}^{-1}}(x_T - f_\\theta(z_T))\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. a. i. A sampling-free approach. \n",
    "\n",
    "\n",
    "If we know the expectation $\\mu$ and variance $\\Sigma$ of a random variable $v$ (which need not be Gaussian):\n",
    "\n",
    "$$\\mathbb{E}_{v}\\left[(x - v)^T \\Omega (x - v)\\right] = tr(\\Sigma \\Omega) + (\\mu - x)^T \\Omega (\\mu - x)$$\n",
    "\n",
    "Suppose we a have neural network which approximates the mean and variance of $v \\sim f_\\theta(z)$ when $z \\sim p_z$, given parameters of $p_z$. Denote $\\tilde{\\mu}$ and $\\tilde{\\Sigma}$ these means and variances estimated by this network. For the filtering case, we feed the network with filtering mean and covariance at $T$ to obtain an estimate of $\\tilde{\\mu}$ and $\\tilde{\\Sigma}$, then:\n",
    "\n",
    "$$\\mathbb{E}_{q(z_T|x_{1:T})}\\left[(x_T - f_\\theta(z_T))^T R^{{\\theta}^{-1}}(x_T - f_\\theta(z_T))\\right] = tr(\\tilde{\\Sigma} \\Omega) + (\\tilde{\\mu} - x)^T R^{{\\theta}^{-1}} (\\tilde{\\mu} - x)$$\n",
    "\n",
    "For the backwards case this is not as simple, because: $\\overleftarrow{\\mu}_{1:t}$ is a function of $z_{t+1}$, therefore $\\mathbb{E}_{q(z_t|z_{t+1}, x_{1:t})}[f_\\theta(z_t)]$ and $\\mathbb{V}_{q(z_t|z_{t+1}, x_{1:t})}[f_\\theta(z_t)]$ are also functions of $z_{t+1}$. \n",
    "\n",
    "We still attempt to use one network for both the fitlering and the backwards via the following scheme: \n",
    "\n",
    "- Build a neural network $g_\\alpha(A, a, \\Sigma)$ which outputs $\\tilde{A}, \\tilde{a}$ and $\\tilde{\\Sigma}$\n",
    "- For the backwards case, use $A = \\overleftarrow{A}_{1:t}, a = \\overleftarrow{a}_{1:t}$ and $\\Sigma = \\overleftarrow{\\Sigma}_{1:t}$, and consider that $\\tilde{\\mu} = \\tilde{A}z_{t+1} + \\tilde{a}$, while $\\tilde{\\Sigma}$ does not depend on $z_{t+1}$ (which is knowingly false). In this case, the quadratic form build for $\\tilde{A}$ and $\\tilde{a}$ is a quadratic form in $z_{t+1}$ as wanted.\n",
    "- For the backwards case, use $A = 0, a = a_{1:t}$ and $\\Sigma = \\Sigma_{1:t}$, and consider that $\\tilde{\\mu} = \\tilde{a}$ (without using the output $\\tilde{A}$).\n",
    "\n",
    "*This method, which is tried below: fails to learn anything as of now.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### not reimplemented in JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. a. i. The Johnson trick\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c528143767b35424e5fa616681845f3b9656c31f35cafe040129ce40f24d14f5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('backward_ica')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
