{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on backward variational ICA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Uncomment and run the following cell if you're using Collab***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf *\n",
    "# !git clone https://github.com/mchagneux/backward_ica.git\n",
    "# !mv backward_ica/* ./\n",
    "# !rm -rf backward_ica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.1054e-15)\n",
      "tensor(7.1054e-15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/infres/chagneux/anaconda3/envs/backward_ica/lib/python3.9/site-packages/torch/nn/modules/container.py:597: UserWarning: Setting attributes on ParameterDict is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from src.eval import mse_expectation_against_true_states\n",
    "from src.kalman import Kalman, NumpyKalman\n",
    "from src.hmm import AdditiveGaussianHMM, LinearGaussianHMM\n",
    "from src.elbo import get_appropriate_elbo\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "torch.set_default_dtype(torch.float64) \n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "# torch.set_printoptions(precision=10)\n",
    "\n",
    "## sanity checks\n",
    "hmm = LinearGaussianHMM(state_dim=2, obs_dim=2)\n",
    "states, observations = hmm.sample_joint_sequence(10)\n",
    "\n",
    "for param in hmm.model.parameters():param.requires_grad = False\n",
    "likelihood_torch = Kalman(hmm.model).filter(observations)[4] #kalman with torch operators \n",
    "likelihood_numpy = NumpyKalman(hmm.model).filter(observations.numpy())[2] #kalman with numpy operators \n",
    "fully_linear_gaussian_elbo = get_appropriate_elbo('linear_gaussian','linear_emission')\n",
    "likelihood_via_elbo = fully_linear_gaussian_elbo(hmm.model, hmm.model)(observations) #elbo\n",
    "\n",
    "# both should be close to 0\n",
    "print(likelihood_numpy - likelihood_torch)\n",
    "print(likelihood_numpy - likelihood_via_elbo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook is comprised of a series of experiments that attempt to recover expectations $\\mathbb{E}[h(z_{1:t})|x_{1:t}]$ via variational approximations, when the process $(z_t, x_t)_{t \\ge 1}$ is an HMM. The main metric $\\ell$ all along is the MSE against the true states when $h$ is a plain sum, ie\n",
    "\n",
    "$$\\ell = \\left(\\sum_{t=1}^T z_t^* - \\sum_{t=1}^T \\mathbb{E}_{q_T(z_t)}[z_t] \\right)^2$$\n",
    "\n",
    "where $q_T(z_t) = q(z_t|x_{1:T})$ is the marginal smoothing distribution at $t$.\n",
    "\n",
    "In all the following, we assume that the variational smoothing distribution factorizes as $q_\\phi(z_{1:t}|x_{1:t}) = q_\\phi(z_t|x_{1:t}) \\prod_{s=1}^{t-1} q_\\phi(z_s|z_{s+1},x_{1:s})$. We always assume that $$q_\\phi(z_t|x_{1:t}) \\sim \\mathcal{N}(\\mu_{1:t}, \\Sigma_{1:t})$$ and \n",
    "\n",
    "$$q_\\phi(z_s|z_{s+1},x_{1:s}) \\sim \\mathcal{N}(\\overleftarrow{\\mu}_{1:t}(z_{s+1}), \\overleftarrow{\\Sigma}_{1:t})$$\n",
    "\n",
    "In the following, we make several assumptions on both $p_\\theta$ and $q_\\phi$.\n",
    "\n",
    "\n",
    "In this case, not only should the expectations be correctly recovered, but parameters in $\\phi$ and $\\theta$ should be identifiable. We also know that in this case the best estimate of $z_{1:t}^*$ for any sequence is obtained via the Kalman smoothing recursions applied with parameters $\\theta$ on the observations $x_{1:t}$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Gaussian HMM \n",
    "\n",
    "First we assume that observation sequences $x_{1:T}$ arise from $p_\\theta(z_{1:t},x_{1:t})$ defined as\n",
    "$$z_t = A_\\theta z_{t-1} + a_\\theta + \\eta_\\theta$$ \n",
    "$$x_t = B_\\theta z_t + b_\\theta + \\epsilon_\\theta$$\n",
    "\n",
    "where $\\eta_\\theta \\sim \\mathcal{N}(0,Q_\\theta)$ and $\\epsilon_\\theta \\sim \\mathcal{N}(0,R_\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. a. Approximated by a linear Gaussian HMM\n",
    "\n",
    "We start by recovering $p_\\theta$ when $q_\\phi$ is in the family of the true p. We do this by prescribing the p for $q_\\phi$ in forward time with a similar HMM structure as $p_\\theta$ (but random initial parameters), and in this case the parameters of the filtering backward distributions exist via Kalman recursions and closed-form definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True evidence accross all sequences: tensor(284.5419)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(2265.7776)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(828.8245)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(310.4879)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(157.9945)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(143.0278)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(128.4308)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(115.4511)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(109.9892)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(105.7958)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(102.5639)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(99.7887)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(96.9263)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(94.3767)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(91.6941)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(89.0908)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(86.4193)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(83.7521)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(81.0582)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(78.3620)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(75.6632)\n",
      "Average of \"L(theta, phi) - log(p_theta(x))\": tensor(72.9740)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/infres/chagneux/repos/backward_ica/demos.ipynb Cell 9'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu2/home/infres/chagneux/repos/backward_ica/demos.ipynb#ch0000008vscode-remote?line=30'>31</a>\u001b[0m \u001b[39mfor\u001b[39;00m observations \u001b[39min\u001b[39;00m observation_sequences: \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu2/home/infres/chagneux/repos/backward_ica/demos.ipynb#ch0000008vscode-remote?line=31'>32</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpu2/home/infres/chagneux/repos/backward_ica/demos.ipynb#ch0000008vscode-remote?line=32'>33</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39melbo(observations)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu2/home/infres/chagneux/repos/backward_ica/demos.ipynb#ch0000008vscode-remote?line=33'>34</a>\u001b[0m     loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpu2/home/infres/chagneux/repos/backward_ica/demos.ipynb#ch0000008vscode-remote?line=34'>35</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/backward_ica/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/backward_ica/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/backward_ica/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/backward_ica/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/backward_ica/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///~/anaconda3/envs/backward_ica/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/backward_ica/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/backward_ica/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/backward_ica/src/elbo.py:138\u001b[0m, in \u001b[0;36mBackwardELBO.forward\u001b[0;34m(self, observations)\u001b[0m\n\u001b[1;32m    <a href='file:///~/repos/backward_ica/src/elbo.py?line=134'>135</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit(observations[\u001b[39m0\u001b[39m])\n\u001b[1;32m    <a href='file:///~/repos/backward_ica/src/elbo.py?line=136'>137</a>\u001b[0m \u001b[39mfor\u001b[39;00m observation \u001b[39min\u001b[39;00m observations[\u001b[39m1\u001b[39m:]:\n\u001b[0;32m--> <a href='file:///~/repos/backward_ica/src/elbo.py?line=137'>138</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update(observation)\n\u001b[1;32m    <a href='file:///~/repos/backward_ica/src/elbo.py?line=138'>139</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconstants_V \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m-\u001b[39m_constant_terms_from_log_gaussian(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdim_z, torch\u001b[39m.\u001b[39mdet(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfiltering_params\u001b[39m.\u001b[39mcov))\n\u001b[1;32m    <a href='file:///~/repos/backward_ica/src/elbo.py?line=139'>140</a>\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expect_V_under_filtering()\n",
      "File \u001b[0;32m~/repos/backward_ica/src/elbo.py:114\u001b[0m, in \u001b[0;36mBackwardELBO._update\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    <a href='file:///~/repos/backward_ica/src/elbo.py?line=111'>112</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update\u001b[39m(\u001b[39mself\u001b[39m, observation):\n\u001b[1;32m    <a href='file:///~/repos/backward_ica/src/elbo.py?line=112'>113</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_backward()\n\u001b[0;32m--> <a href='file:///~/repos/backward_ica/src/elbo.py?line=113'>114</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_filtering(observation)\n\u001b[1;32m    <a href='file:///~/repos/backward_ica/src/elbo.py?line=114'>115</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_V(observation)\n",
      "File \u001b[0;32m~/repos/backward_ica/src/elbo.py:212\u001b[0m, in \u001b[0;36mLinearGaussianQ._update_filtering\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    <a href='file:///~/repos/backward_ica/src/elbo.py?line=210'>211</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_filtering\u001b[39m(\u001b[39mself\u001b[39m, observation):\n\u001b[0;32m--> <a href='file:///~/repos/backward_ica/src/elbo.py?line=211'>212</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfiltering_params \u001b[39m=\u001b[39m Filtering(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkalman\u001b[39m.\u001b[39;49mfilter_step(\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfiltering_params, observation)[\u001b[39m2\u001b[39m:])\n",
      "File \u001b[0;32m~/repos/backward_ica/src/kalman.py:29\u001b[0m, in \u001b[0;36mKalman.filter_step\u001b[0;34m(self, current_state_mean, current_state_covariance, observation)\u001b[0m\n\u001b[1;32m     <a href='file:///~/repos/backward_ica/src/kalman.py?line=27'>28</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfilter_step\u001b[39m(\u001b[39mself\u001b[39m, current_state_mean, current_state_covariance, observation):\n\u001b[0;32m---> <a href='file:///~/repos/backward_ica/src/kalman.py?line=28'>29</a>\u001b[0m     predicted_mean, predicted_cov \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(current_state_mean, current_state_covariance)\n\u001b[1;32m     <a href='file:///~/repos/backward_ica/src/kalman.py?line=29'>30</a>\u001b[0m     filtered_mean, filtered_cov \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(predicted_mean, predicted_cov, observation)\n\u001b[1;32m     <a href='file:///~/repos/backward_ica/src/kalman.py?line=30'>31</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m predicted_mean, predicted_cov, filtered_mean, filtered_cov\n",
      "File \u001b[0;32m~/repos/backward_ica/src/kalman.py:15\u001b[0m, in \u001b[0;36mKalman.predict\u001b[0;34m(self, current_state_mean, current_state_covariance)\u001b[0m\n\u001b[1;32m     <a href='file:///~/repos/backward_ica/src/kalman.py?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, current_state_mean, current_state_covariance):\n\u001b[1;32m     <a href='file:///~/repos/backward_ica/src/kalman.py?line=13'>14</a>\u001b[0m     predicted_state_mean \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtransition\u001b[39m.\u001b[39mmap(current_state_mean)\n\u001b[0;32m---> <a href='file:///~/repos/backward_ica/src/kalman.py?line=14'>15</a>\u001b[0m     predicted_state_covariance \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtransition\u001b[39m.\u001b[39mmap\u001b[39m.\u001b[39mweight \u001b[39m@\u001b[39m current_state_covariance \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtransition\u001b[39m.\u001b[39mmap\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mT \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtransition\u001b[39m.\u001b[39mcov\n\u001b[1;32m     <a href='file:///~/repos/backward_ica/src/kalman.py?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m predicted_state_mean, predicted_state_covariance\n",
      "File \u001b[0;32m~/anaconda3/envs/backward_ica/lib/python3.9/site-packages/torch/nn/modules/module.py:1164\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/backward_ica/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1160'>1161</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_is_full_backward_hook\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/backward_ica/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1161'>1162</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_full_backward_hook \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> <a href='file:///~/anaconda3/envs/backward_ica/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1163'>1164</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tensor, \u001b[39m'\u001b[39m\u001b[39mModule\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/backward_ica/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1164'>1165</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   <a href='file:///~/anaconda3/envs/backward_ica/lib/python3.9/site-packages/torch/nn/modules/module.py?line=1165'>1166</a>\u001b[0m         _parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hmm = LinearGaussianHMM(state_dim=2, obs_dim=2) # pick some true p p \n",
    "for param in hmm.model.parameters(): param.requires_grad = False # not learning the parameters of the true p for now \n",
    "\n",
    "\n",
    "\n",
    "# sampling 10 sequences from the hmm \n",
    "samples = [hmm.sample_joint_sequence(8) for _ in range(10)] \n",
    "state_sequences = [sample[0] for sample in samples]\n",
    "observation_sequences = [sample[1] for sample in samples] \n",
    "\n",
    "\n",
    "# the variational p is a random LGMM with same dimensions, and we will not learn the covariances for now\n",
    "q = LinearGaussianHMM.get_random_model(2,2)\n",
    "q.prior.parametrizations.cov.original.requires_grad = False\n",
    "q.transition.parametrizations.cov.original.requires_grad = False \n",
    "q.emission.parametrizations.cov.original.requires_grad = False \n",
    "\n",
    "# the elbo object with p and q as arguments\n",
    "elbo = fully_linear_gaussian_elbo(hmm.model, q)\n",
    "\n",
    "# optimize the parameters of the ELBO (but theta deactivated above)\n",
    "optimizer = torch.optim.Adam(params=elbo.parameters(), lr=1e-2)\n",
    "true_evidence_all_sequences = sum(Kalman(hmm.model).filter(observations)[-1] for observations in observation_sequences)\n",
    "\n",
    "print('True evidence accross all sequences:', true_evidence_all_sequences)\n",
    "\n",
    "eps = torch.inf\n",
    "# optimizing p \n",
    "while eps > 0.1:\n",
    "    epoch_loss = 0.0\n",
    "    for observations in observation_sequences: \n",
    "        optimizer.zero_grad()\n",
    "        loss = -elbo(observations)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += -loss\n",
    "    with torch.no_grad():\n",
    "        eps = torch.abs(true_evidence_all_sequences - epoch_loss)\n",
    "        print('Average of \"L(theta, phi) - log(p_theta(x))\":', eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE when smoothed with p: tensor(0.0165)\n",
      "MSE when smoothed with q: tensor(0.0165)\n"
     ]
    }
   ],
   "source": [
    "# checking expectations under approximate p when the additive functional is just the sum \n",
    "with torch.no_grad():\n",
    "    additive_functional = partial(torch.sum, dim=0)\n",
    "    smoothed_with_true_model = mse_expectation_against_true_states(state_sequences, observation_sequences, hmm.model, additive_functional)\n",
    "    smoothed_with_approximate_model = mse_expectation_against_true_states(state_sequences, observation_sequences, q, additive_functional)\n",
    "\n",
    "    print('MSE when smoothed with p:',smoothed_with_true_model)\n",
    "    print('MSE when smoothed with q:',smoothed_with_approximate_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. b. Using a neural network to compute the backward parameters instead of Kalman recursions\n",
    "We make the same assumptions on $p_\\theta$ but now we attempt to recover the backward parameters via neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. A nonlinear emission p\n",
    "\n",
    "We now assume that $p_\\theta$ has a nonlinear emission distribution, ie. $x_t  = f_\\theta(z_t) + \\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. a. Approximated by a linear Gaussian p.\n",
    "We keep a linear gaussian distribution for $q_\\phi$, but we add a mapping to compute the expectation of the emission term from $p_\\theta$. We need to approximate the following quantity:\n",
    "\n",
    "$$\\mathbb{E}_{q(z_t|z_{t+1}, x_{1:t})}\\left[(x_t - f_\\theta(z_t))^T R^{{\\theta}^{-1}}(x_t - f_\\theta(z_t))\\right]$$\n",
    "\n",
    "And similarly for the last expectation under the filtering distribution: \n",
    "\n",
    "$$\\mathbb{E}_{q(z_T|x_{1:T})}\\left[(x_T - f_\\theta(z_T))^T R^{{\\theta}^{-1}}(x_T - f_\\theta(z_T))\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. a. i. A sampling-free approach. \n",
    "\n",
    "\n",
    "If we know the expectation $\\mu$ and variance $\\Sigma$ of a random variable $v$ (which need not be Gaussian):\n",
    "\n",
    "$$\\mathbb{E}_{v}\\left[(x - v)^T \\Omega (x - v)\\right] = tr(\\Sigma \\Omega) + (\\mu - x)^T \\Omega (\\mu - x)$$\n",
    "\n",
    "Suppose we a have neural network which approximates the mean and variance of $v \\sim f_\\theta(z)$ when $z \\sim p_z$, given parameters of $p_z$. Denote $\\tilde{\\mu}$ and $\\tilde{\\Sigma}$ these means and variances estimated by this network. For the filtering case, we feed the network with filtering mean and covariance at $T$ to obtain an estimate of $\\tilde{\\mu}$ and $\\tilde{\\Sigma}$, then:\n",
    "\n",
    "$$\\mathbb{E}_{q(z_T|x_{1:T})}\\left[(x_T - f_\\theta(z_T))^T R^{{\\theta}^{-1}}(x_T - f_\\theta(z_T))\\right] = tr(\\tilde{\\Sigma} \\Omega) + (\\tilde{\\mu} - x)^T R^{{\\theta}^{-1}} (\\tilde{\\mu} - x)$$\n",
    "\n",
    "For the backwards case this is not as simple, because: $\\overleftarrow{\\mu}_{1:t}$ is a function of $z_{t+1}$, therefore $\\mathbb{E}_{q(z_t|z_{t+1}, x_{1:t})}[f_\\theta(z_t)]$ and $\\mathbb{V}_{q(z_t|z_{t+1}, x_{1:t})}[f_\\theta(z_t)]$ are also functions of $z_{t+1}$. \n",
    "\n",
    "We still attempt to use one network for both the fitlering and the backwards via the following scheme: \n",
    "\n",
    "- Build a neural network $g_\\alpha(A, a, \\Sigma)$ which outputs $\\tilde{A}, \\tilde{a}$ and $\\tilde{\\Sigma}$\n",
    "- For the backwards case, use $A = \\overleftarrow{A}_{1:t}, a = \\overleftarrow{a}_{1:t}$ and $\\Sigma = \\overleftarrow{\\Sigma}_{1:t}$, and consider that $\\tilde{\\mu} = \\tilde{A}z_{t+1} + \\tilde{a}$, while $\\tilde{\\Sigma}$ does not depend on $z_{t+1}$ (which is knowingly false). In this case, the quadratic form build for $\\tilde{A}$ and $\\tilde{a}$ is a quadratic form in $z_{t+1}$ as wanted.\n",
    "- For the backwards case, use $A = 0, a = a_{1:t}$ and $\\Sigma = \\Sigma_{1:t}$, and consider that $\\tilde{\\mu} = \\tilde{a}$ (without using the output $\\tilde{A}$).\n",
    "\n",
    "*This method, which is tried below: fails to learn anything as of now.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mathis/repos/backward_ica/src/elbo.py:45: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  self.cov_tilde_approximator = nn.Sequential(nn.Linear(in_features=input_shape, out_features=(obs_dim * (obs_dim + 1)) // 2, bias=True), nn.ReLU())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(1.6632e+19, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(3.0330e+14, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(1.5252e+12, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(3.8693e+12, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(5.5028e+11, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(2.6158e+10, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(4.6995e+13, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(59278587.2570, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(44851757.5932, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(6.4970e+11, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(2.2625e+08, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(10016943.9741, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(51113.1479, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(52316.1698, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(53810.7873, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(2332986.8214, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(57959.4642, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(8.9400e+11, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(7.9302e+11, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(2.3042e+08, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(80772.4505, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(95999.5821, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(21624855.9176, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(8856280.3753, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(4.9882e+08, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(1772032.5033, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(3.6369e+15, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(4.4516e+11, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(1.7604e+09, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(1.2446e+13, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(53794969.1201, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(6.2624e+17, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(8483244.7350, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(1.3607e+14, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(15071419.7332, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(19193913.5193, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(1.8019e+18, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(6.1826e+15, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(36121382.2957, grad_fn=<AddBackward0>)\n",
      "Loss: tensor(84686778.3477, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mathis/repos/backward_ica/demos.ipynb Cell 15'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mathis/repos/backward_ica/demos.ipynb#ch0000014?line=32'>33</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mathis/repos/backward_ica/demos.ipynb#ch0000014?line=33'>34</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39melbo(observations)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mathis/repos/backward_ica/demos.ipynb#ch0000014?line=34'>35</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mathis/repos/backward_ica/demos.ipynb#ch0000014?line=35'>36</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mathis/repos/backward_ica/demos.ipynb#ch0000014?line=36'>37</a>\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m-\u001b[39mloss\n",
      "File \u001b[0;32m~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/_tensor.py?line=297'>298</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/_tensor.py?line=298'>299</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/_tensor.py?line=299'>300</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/_tensor.py?line=300'>301</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/_tensor.py?line=304'>305</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/_tensor.py?line=305'>306</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/_tensor.py?line=306'>307</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/autograd/__init__.py?line=150'>151</a>\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/autograd/__init__.py?line=151'>152</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/autograd/__init__.py?line=153'>154</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/autograd/__init__.py?line=154'>155</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///~/miniconda3/envs/backward_ica/lib/python3.9/site-packages/torch/autograd/__init__.py?line=155'>156</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hmm = AdditiveGaussianHMM(state_dim=2, obs_dim=2) # we now take an hmm wih \n",
    "\n",
    "# sampling 10 sequences from the hmm \n",
    "samples = [hmm.sample_joint_sequence(8) for _ in range(10)] \n",
    "state_sequences = [sample[0] for sample in samples]\n",
    "observation_sequences = [sample[1] for sample in samples] \n",
    "\n",
    "\n",
    "# the variational p is a random LGMM with same dimensions, and we will not learn the covariances for now\n",
    "q = LinearGaussianHMM.get_random_model(2,2)\n",
    "q.prior.parametrizations.cov.original.requires_grad = False\n",
    "q.transition.parametrizations.cov.original.requires_grad = False \n",
    "q.emission.parametrizations.cov.original.requires_grad = False \n",
    "\n",
    "\n",
    "elbo_nonlinear_emission = get_appropriate_elbo(q_description='linear_gaussian', p_description='nonlinear_emission')\n",
    "\n",
    "elbo = elbo_nonlinear_emission(hmm.model, q)\n",
    "\n",
    "# print(elbo_nonlinear_emission(observation_sequences[0]))\n",
    "\n",
    "\n",
    "\n",
    "# optimize the parameters of the ELBO (but theta deactivated above)\n",
    "optimizer = torch.optim.Adam(params=elbo.parameters(), lr=1e-2)\n",
    "\n",
    "\n",
    "eps = torch.inf\n",
    "# optimizing p \n",
    "while True:\n",
    "    epoch_loss = 0.0\n",
    "    for observations in observation_sequences: \n",
    "        optimizer.zero_grad()\n",
    "        loss = -elbo(observations)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += -loss\n",
    "    with torch.no_grad():\n",
    "        print(\"Loss:\", epoch_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. a. i. Sampling and the Johnson trick.\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c528143767b35424e5fa616681845f3b9656c31f35cafe040129ce40f24d14f5"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('backward_ica')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
