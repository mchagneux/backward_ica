{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% \n",
    "from functools import partial\n",
    "from src.eval import mse_expectation_against_true_states\n",
    "from src.kalman import Kalman, NumpyKalman\n",
    "from src.hmm import AdditiveGaussianHMM, LinearGaussianHMM\n",
    "from src.elbo import LinearGaussianELBO\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "torch.set_default_dtype(torch.float64) \n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "torch.set_printoptions(precision=10)\n",
    "#%%  A series of sanity checks before doing real work \n",
    "\n",
    "# few samples from a random LGHMM \n",
    "hmm = LinearGaussianHMM(state_dim=2, obs_dim=2)\n",
    "states, observations = hmm.sample_joint_sequence(10)\n",
    "\n",
    "for param in hmm.model.parameters():param.requires_grad = False\n",
    "likelihood_torch = Kalman(hmm.model).filter(observations)[4] #kalman with torch operators \n",
    "likelihood_numpy = NumpyKalman(hmm.model).filter(observations.numpy())[2] #kalman with numpy operators \n",
    "likelihood_via_elbo = LinearGaussianELBO(hmm.model, hmm.model)(observations) #elbo\n",
    "\n",
    "# both should be close to 0\n",
    "print(likelihood_numpy - likelihood_torch)\n",
    "print(likelihood_numpy - likelihood_via_elbo)\n",
    "#%% Recovering parameters when p and q are linear gaussian \n",
    "\n",
    "hmm = LinearGaussianHMM(state_dim=2, obs_dim=2) # pick some true model p \n",
    "for param in hmm.model.parameters(): param.requires_grad = False # not learning the parameters of the true model for now \n",
    "\n",
    "\n",
    "\n",
    "# sampling 10 sequences from the hmm \n",
    "samples = [hmm.sample_joint_sequence(8) for _ in range(10)] \n",
    "state_sequences = [sample[0] for sample in samples]\n",
    "observation_sequences = [sample[1] for sample in samples] \n",
    "\n",
    "\n",
    "# the variational model is a random LGMM with same dimensions, and we will not learn the covariances for now \n",
    "v_model = LinearGaussianHMM.get_random_model(2,2)\n",
    "v_model.prior.parametrizations.cov.original.requires_grad = False\n",
    "v_model.transition.parametrizations.cov.original.requires_grad = False \n",
    "v_model.emission.parametrizations.cov.original.requires_grad = False \n",
    "\n",
    "# the elbo object with p and q as arguments\n",
    "elbo = LinearGaussianELBO(hmm.model, v_model)\n",
    "\n",
    "# optimize the parameters of the ELBO (but theta deactivated above)\n",
    "optimizer = torch.optim.Adam(params=elbo.parameters(), lr=1e-2)\n",
    "\n",
    "\n",
    "true_evidence_all_sequences = sum(Kalman(hmm.model).filter(observations)[2] for observations in observation_sequences)\n",
    "\n",
    "\n",
    "# optimizing model \n",
    "eps = torch.inf\n",
    "while eps > 0.1:\n",
    "    epoch_loss = 0.0\n",
    "    for observations in observation_sequences: \n",
    "        optimizer.zero_grad()\n",
    "        loss = -elbo(observations)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += -loss\n",
    "    with torch.no_grad():\n",
    "        eps = torch.abs(true_evidence_all_sequences - epoch_loss)\n",
    "        print('Average of \"L(theta, phi) - log(p_theta(x))\":', eps)\n",
    "\n",
    "# checking expectations under approximate model \n",
    "with torch.no_grad():\n",
    "    additive_functional = partial(torch.sum, dim=0)\n",
    "    smoothed_with_true_model = mse_expectation_against_true_states(state_sequences, observation_sequences, hmm.model, additive_functional)\n",
    "    smoothed_with_approximate_model = mse_expectation_against_true_states(state_sequences, observation_sequences, v_model, additive_functional)\n",
    "\n",
    "    print('Expectation when smooth with true model:',smoothed_with_true_model)\n",
    "    print('Expectation when smooth with variational model:',smoothed_with_approximate_model)\n",
    "\n",
    "#%% Approximating the model when p is not linear gaussian but q still is \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
