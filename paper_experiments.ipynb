{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax, jax.numpy as jnp\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dill\n",
    "import math\n",
    "# import os\n",
    "# import matplotlib\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "jax.config.update('jax_platform_name', 'cpu') # switch to 'cpu' if unavailable or depending on computation type\n",
    "jax.config.update('jax_enable_x64', False)\n",
    "from src.stats.hmm import get_generative_model\n",
    "from src.variational import get_variational_model\n",
    "from src.utils.misc import tree_get_idx, tree_dropfirst, tree_droplast\n",
    "from src.training import SVITrainer\n",
    "from src.stats.ula import ULA\n",
    "\n",
    "def set_defaults(args, default_std=0.1):\n",
    "    args.default_prior_mean = 0.0 # default value for the mean of Gaussian prior\n",
    "    args.default_prior_base_scale = default_std # default value for the diagonal components of the covariance matrix of the prior\n",
    "    args.default_transition_base_scale = default_std # default value for the diagonal components  of the covariance matrix of the transition kernel\n",
    "    args.default_transition_bias = 0.0\n",
    "    args.default_emission_base_scale = default_std\n",
    "    args.parametrization = 'cov_chol'\n",
    "    return args\n",
    "\n",
    "\n",
    "def plot_x_true_against_x_pred(x_true, x_pred, y=None, save=False):\n",
    "    dims = x_true.shape[-1]\n",
    "    _ , axes = plt.subplots(dims, 1, figsize=(15,2*dims))\n",
    "    for dim in range(dims):\n",
    "        axes[dim].plot(x_true[:,dim], c='red', label='True', alpha=0.7)\n",
    "        axes[dim].plot(x_pred[:,dim], c='green', label='Pred', alpha=0.7)\n",
    "        axes[dim].legend()\n",
    "        if y is not None:\n",
    "            axes[dim].plot(y[:,dim], c='black', label='Obs', alpha=0.5)\n",
    "    if save: plt.savefig('test.pdf', format='pdf')   \n",
    "    \n",
    "def plot_data(y):\n",
    "    dims = y.shape[-1]\n",
    "    _ , axes = plt.subplots(dims, 1, figsize=(15,2*dims))\n",
    "    for dim in range(dims):\n",
    "        axes[dim].plot(y[:,dim], label='Data')\n",
    "\n",
    "def compute_rmse_x_true_against_x_pred(x_true, x_pred):\n",
    "    return jnp.mean(jnp.sqrt(jnp.mean((x_true-x_pred)**2, axis=-1)), \n",
    "                    axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear-Gaussian HMM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning in an offline setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0) # seed\n",
    "p_args = argparse.Namespace() # will contain all configuration information for the generative model \n",
    "p_args.state_dim, p_args.obs_dim = 10,10 # dimensions of state and observation spaces\n",
    "p_args.model = 'linear'\n",
    "p_args.seq_length = 5_00\n",
    "p_args.emission_bias = False # no bias in the emission kernel, i.e. only Y_t = B X_t + noise\n",
    "p_args.transition_bias = False # no bias in the transition kernel, i.e. only X_t = A X_{t-1} + noise\n",
    "p_args.transition_matrix_conditionning = 'diagonal' # the transition matrix will be diagonal\n",
    "p_args.range_transition_map_params = [0.6,0.9] # the diagonal values are constrained to [0.9 1]\n",
    "p_args.num_seqs = 1 #\n",
    "p_args = set_defaults(p_args, math.sqrt(0.1)) # setting default values for the covariances with realistic Signal to noise ratios\n",
    "p = get_generative_model(p_args) # getting the model and its parameters \n",
    "# save_args(p_args, 'args', exp_path) # saving the params of the generative model\n",
    "# save_params(theta, 'theta', exp_path) # saving the parameters\n",
    "# jnp.save(os.path.join(exp_path, 'xs'), xs)\n",
    "# jnp.save(os.path.join(exp_path, 'ys'), ys)\n",
    "\n",
    "q_args = argparse.Namespace() # will contain all configuration information for the variational model \n",
    "q_args.state_dim, q_args.obs_dim = p_args.state_dim, p_args.obs_dim \n",
    "q_args.model = 'linear' \n",
    "q_args.emission_bias = False \n",
    "q_args.transition_bias = False \n",
    "q_args.transition_matrix_conditionning = 'diagonal' \n",
    "q_args.range_transition_map_params = [-0.9,0.9] \n",
    "q_args = set_defaults(q_args, math.sqrt(0.1)) \n",
    "# sub_exp_path = os.path.join(exp_path, q_args.model)\n",
    "# os.makedirs(sub_exp_path, exist_ok=True)\n",
    "# save_args(p_args, 'args', sub_exp_path) # saving the params of the generative model\n",
    "q = get_variational_model(q_args) # getting the corresponding model\n",
    "\n",
    "\n",
    "learning_key = jax.random.PRNGKey(0)\n",
    "\n",
    "elbos_list = dict()\n",
    "params_list = dict()\n",
    "num_fits = 10\n",
    "\n",
    "generative_settings = []\n",
    "key, key_generative, key_learning = jax.random.split(key, 3)\n",
    "\n",
    "for fit_nb in range(num_fits):\n",
    "    key_generative, key_theta, key_sequences = jax.random.split(key_generative, 3)\n",
    "    theta = p.get_random_params(key_theta, p_args)\n",
    "    xs, ys = p.sample_multiple_sequences(key_sequences, \n",
    "                                     theta, \n",
    "                                     p_args.num_seqs, \n",
    "                                     p_args.seq_length,\n",
    "                                     single_split_seq=False, # not sampling a sequence and splitting into subsequences\n",
    "                                     load_from='', # not loading from external folder\n",
    "                                     loaded_seq=False) # not loading sequences\n",
    "    generative_settings.append((theta, (xs, ys)))\n",
    "    \n",
    "elbo_modes = [\n",
    "        'closed_form', \n",
    "        'score,resampling,bptt_depth_2']\n",
    "\n",
    "num_epochs = 500\n",
    "key_params, key_mc = jax.random.split(key_learning, 2)\n",
    "keys_params = jax.random.split(key_params, num_fits)\n",
    "keys_mc = jax.random.split(key_mc, num_fits)\n",
    "\n",
    "for elbo_mode in elbo_modes:\n",
    "    \n",
    "    elbos_list[elbo_mode] = dict()\n",
    "    params_list[elbo_mode] = dict()\n",
    "\n",
    "    for fit_nb in range(num_fits):\n",
    "        trainer = SVITrainer(p=p,\n",
    "                            theta_star=generative_settings[fit_nb][0],\n",
    "                            q=q,\n",
    "                            optimizer='adam',\n",
    "                            learning_rate=1e-2, \n",
    "                            optim_options='cst', # learning rate schedule\n",
    "                            num_epochs=num_epochs, # number of full sweeps through the sequence\n",
    "                            seq_length=p_args.seq_length,\n",
    "                            num_samples=10, # number of monte carlo samples (or trajectories in the offline case)\n",
    "                            frozen_params='', # which parameters to hold fixed\n",
    "                            num_seqs=p_args.num_seqs, \n",
    "                            training_mode=f'reset,{p_args.seq_length},1', \n",
    "                            elbo_mode=elbo_mode)\n",
    "\n",
    "        fitted_variational_params, elbos, aux_results, all_params = trainer.fit(keys_params[fit_nb], \n",
    "                                                                                keys_mc[fit_nb], \n",
    "                                                                                data=generative_settings[fit_nb][1][1], \n",
    "                                                                                args=q_args)\n",
    "\n",
    "        \n",
    "        elbos_list[elbo_mode][fit_nb] = elbos.flatten().tolist()\n",
    "        params_list[elbo_mode][fit_nb] = fitted_variational_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('elbos', 'wb') as f: \n",
    "#     dill.dump(elbos_list, f)\n",
    "# with open('params', 'wb') as f: \n",
    "#     dill.dump(params_list, f)\n",
    "\n",
    "with open('elbos', 'rb') as f: \n",
    "    elbos_list = dill.load(f)\n",
    "with open('params', 'rb') as f: \n",
    "    params_list = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbos = np.zeros((len(elbo_modes), num_fits, num_epochs))\n",
    "for i, elbo_mode in enumerate(elbo_modes):\n",
    "    for j, fit_nb in enumerate(range(num_fits)):\n",
    "        elbos[i,j] = elbos_list[elbo_mode][fit_nb]\n",
    "\n",
    "elbos_mean = np.mean(elbos, axis=1)\n",
    "eblos_std = np.std(elbos, axis=1)\n",
    "\n",
    "for idx, (elbo_mode, name, color) in enumerate(zip(\n",
    "                            elbo_modes, \n",
    "                            ['Analytical', 'Score'], \n",
    "                            ['black', 'red'])):\n",
    "\n",
    "    plt.plot(elbos_mean[idx], \n",
    "                color=color, \n",
    "                label=name, \n",
    "                alpha=1)\n",
    "    plt.fill_between(range(num_epochs), \n",
    "                elbos_mean[idx]-eblos_std[idx], \n",
    "                elbos_mean[idx]+eblos_std[idx],\n",
    "                color=color, \n",
    "                alpha=0.3)\n",
    "\n",
    "plt.xlabel('Epoch', weight='bold')\n",
    "plt.ylabel('ELBO', weight='bold')\n",
    "plt.autoscale(True)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('training_lgm_dim_10.pdf',format='pdf')\n",
    "# handles, labels = plt.gca().get_legend_handles_labels()\n",
    "# by_label = dict(zip(labels, handles))\n",
    "# plt.legend(by_label.values(), by_label.keys())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_values = dict()\n",
    "for elbo_mode in elbo_modes:\n",
    "    rmse_values[elbo_mode] = []\n",
    "    for fit_nb in range(num_fits):\n",
    "        theta, (xs, ys) = generative_settings[fit_nb]\n",
    "        x_smoothed_kalman = p.smooth_seq(ys[0], theta)[0]\n",
    "        x_smoothed_variational = q.smooth_seq(ys[0], params_list[elbo_mode][fit_nb])[0]\n",
    "        rmse_values[elbo_mode].append(compute_rmse_x_true_against_x_pred(x_smoothed_kalman, \n",
    "                                                                         x_smoothed_variational))\n",
    "        if fit_nb == 0:\n",
    "            plt.close()\n",
    "            plt.figure()\n",
    "            print(f'Plotting {elbo_mode}')\n",
    "            plot_x_true_against_x_pred(x_smoothed_kalman, x_smoothed_variational)\n",
    "\n",
    "rmse_values_stats = {k:(jnp.mean(jnp.array(v)), \n",
    "                    jnp.std(jnp.array(v))) for k,v in rmse_values.items()}\n",
    "rmse_values_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(fitted_variational_params)\n",
    "x_smoothed_kalman = p.smooth_seq(ys[0], theta)[0]\n",
    "x_smoothed_variational = q.smooth_seq(ys[0], fitted_variational_params)[0]\n",
    "plot_x_true_against_x_pred(x_smoothed_kalman, x_smoothed_variational)\n",
    "# compute_rmse_x_true_against_x_pred(x_smoothed_kalman, x_smoothed_variational)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0) # seed\n",
    "p_args = argparse.Namespace() # will contain all configuration information for the generative model \n",
    "p_args.state_dim, p_args.obs_dim = 10,10 # dimensions of state and observation spaces\n",
    "p_args.model = 'linear'\n",
    "p_args.seq_length = 500_000\n",
    "p_args.emission_bias = False # no bias in the emission kernel, i.e. only Y_t = B X_t + noise\n",
    "p_args.transition_bias = False # no bias in the transition kernel, i.e. only X_t = A X_{t-1} + noise\n",
    "p_args.transition_matrix_conditionning = 'diagonal' # the transition matrix will be diagonal\n",
    "p_args.range_transition_map_params = [0.6,0.9] # the diagonal values are constrained to [0.9 1]\n",
    "p_args.num_seqs = 1 #\n",
    "p_args = set_defaults(p_args, math.sqrt(0.1)) # setting default values for the covariances with realistic Signal to noise ratios\n",
    "key, key_theta, key_sequences = jax.random.split(key, 3)\n",
    "p,theta = get_generative_model(p_args, key_theta) # getting the model and its parameters \n",
    "xs, ys = p.sample_multiple_sequences(key_sequences, \n",
    "                                     theta, \n",
    "                                     num_seqs=1, \n",
    "                                     seq_length=p_args.seq_length,\n",
    "                                     single_split_seq=False,\n",
    "                                     load_from='')\n",
    "\n",
    "q_args = argparse.Namespace() # will contain all configuration information for the variational model \n",
    "q_args.state_dim, q_args.obs_dim = p_args.state_dim, p_args.obs_dim \n",
    "q_args.model = 'linear' \n",
    "q_args.emission_bias = False \n",
    "q_args.transition_bias = False \n",
    "q_args.transition_matrix_conditionning = 'diagonal' \n",
    "q_args.range_transition_map_params = [-0.9,0.9] \n",
    "q_args = set_defaults(q_args, math.sqrt(0.1))\n",
    "# sub_exp_path = os.path.join(exp_path, q_args.model)\n",
    "# os.makedirs(sub_exp_path, exist_ok=True)\n",
    "# save_args(p_args, 'args', sub_exp_path) # saving the params of the generative model\n",
    "q = get_variational_model(q_args) # getting the corresponding model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo_mode = 'score,resampling,bptt_depth_2'\n",
    "trainer = SVITrainer(p=p,\n",
    "                    theta_star=theta,\n",
    "                    q=q,\n",
    "                    optimizer='adam',\n",
    "                    learning_rate=1e-3, \n",
    "                    optim_options='cst', # learning rate schedule\n",
    "                    num_epochs=1, # number of full sweeps through the sequence\n",
    "                    seq_length=p_args.seq_length,\n",
    "                    num_samples=10, # number of monte carlo samples (or trajectories in the online case)\n",
    "                    frozen_params='', # which parameters to hold fixed\n",
    "                    num_seqs=p_args.num_seqs, \n",
    "                    training_mode=f'streaming,1,difference', # L_t - L_{t-1} / t\n",
    "                    elbo_mode=elbo_mode)\n",
    "\n",
    "key_init_params, key_montecarlo = jax.random.split(key, 2)\n",
    "fitted_variational_params, elbos, aux_results, all_params = trainer.fit(key_init_params, \n",
    "                                key_montecarlo, \n",
    "                                data=ys, \n",
    "                                args=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo_for_all_epochs = elbos.flatten()\n",
    "# plt.plot(elbo_for_all_epochs)\n",
    "# plt.xlabel('Iteration')\n",
    "# plt.ylabel('ELBO')\n",
    "\n",
    "steps = jnp.array([0, \n",
    "                   10_000, \n",
    "                   p_args.seq_length-1]) #jnp.arange(0, p_args.seq_length, p_args.seq_length // num_points)\n",
    "colors = ['red', 'green', 'blue']\n",
    "\n",
    "with open('online_500k_all_params','wb') as f: \n",
    "    dill.dump(all_params, f)\n",
    "\n",
    "all_params_at_steps = tree_get_idx(-1, \n",
    "                          jax.tree_map(lambda x: jnp.take(x, indices=steps, axis=1), \n",
    "                                       all_params))\n",
    "\n",
    "eval_length = 5_00\n",
    "# x_true = xs[0][:eval_length]\n",
    "# y = ys[0][:eval_length]\n",
    "dim = 9\n",
    "\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(2,1)\n",
    "\n",
    "ax0.plot(elbo_for_all_epochs, label='ELBO', c='black')\n",
    "ax0.set_xlabel('Iteration')\n",
    "ax0.set_ylabel('ELBO')\n",
    "# plt.vlines(x=steps, ymin=-100, ymax=0, colors=colors)\n",
    "# plt.tight_layout()\n",
    "# plt.autoscale(True)\n",
    "\n",
    "\n",
    "# _ , axes = plt.subplots(dims, 1, figsize=(10,1.5*dims))\n",
    "key_test = jax.random.PRNGKey(5)\n",
    "x_true, y = p.sample_seq(key_test, theta, seq_length=eval_length)\n",
    "x_smoothed_kalman = p.smooth_seq(y, theta)[0]\n",
    "ax1.plot(x_smoothed_kalman[:,dim], c='black', label='Optimal smoothing', alpha=0.7)\n",
    "ax1.set_xlabel('Timestep')\n",
    "ax1.set_ylabel(f'States dim x')\n",
    "rmse_values = []\n",
    "\n",
    "for idx, (step, color) in enumerate(zip(steps,colors)):\n",
    "    params = tree_get_idx(idx, all_params_at_steps)\n",
    "    x_smoothed_variational = q.smooth_seq(y, params)[0]\n",
    "    rmse_values.append(compute_rmse_x_true_against_x_pred(x_smoothed_kalman, \n",
    "                                                          x_smoothed_variational))\n",
    "    ax1.plot(x_smoothed_variational[:,dim],c=color, label=f'Step {step}', alpha=0.4)\n",
    "    # ax1.legend()\n",
    "\n",
    "\n",
    "plt.autoscale(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.legend()\n",
    "plt.savefig('training_100k_lgm.pdf', format='pdf')\n",
    "\n",
    "\n",
    "# fig, (ax0, ax1) = plt.subplots(2,1)\n",
    "\n",
    "# eval_length = 1000\n",
    "# key_eval = jax.random.PRNGKey(2)\n",
    "# x, y = p.sample_seq(key_eval, theta, eval_length)\n",
    "# x_smoothed_kalman = p.smooth_seq(y, theta)[0]\n",
    "# x_smoothed_variational = q.smooth_seq(y, fitted_variational_params)[0]\n",
    "# plot_x_true_against_x_pred(x_smoothed_kalman, x_smoothed_variational)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo_for_all_epochs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaotic RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side-by-side comparison with Campbell et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "key, key_params, key_sequences = jax.random.split(key, 3)\n",
    "import os \n",
    "data_dirs = [os.path.join('data/crnn/dim_5_500_obs', name) for name in os.listdir('data/crnn/dim_5_500_obs')]\n",
    "p_args = argparse.Namespace()\n",
    "p_args.state_dim, p_args.obs_dim = 5,5\n",
    "p_args.model = 'chaotic_rnn' \n",
    "p_args.seq_length = 5_00\n",
    "p_args.loaded_seq = True\n",
    "p_args.range_transition_map_params = [0.8,1] # range of the components of the transition matrix\n",
    "p_args.transition_matrix_conditionning = 'init_scale_by_dim' # constraint\n",
    "p_args.gamma = 2.5 # gamma for the chaotic rnn\n",
    "p_args.tau = 0.025 # tau for the chaotic rnn\n",
    "p_args.grid_size = 0.001 # discretization parameter for the chaotic rnn\n",
    "p_args.emission_matrix_conditionning = 'diagonal'\n",
    "p_args.range_emission_map_params = [-1,1]\n",
    "p_args.default_emission_df = 2 # degrees of freedom for the emission noise\n",
    "p_args.default_emission_matrix = 1.0 # diagonal values for the emission matrix\n",
    "p_args.transition_bias = False \n",
    "p_args.emission_bias = False\n",
    "p_args.num_seqs = 1\n",
    "p_args = set_defaults(p_args)\n",
    "p_args.num_particles, p_args.num_smooth_particles = None, None\n",
    "\n",
    "q_args = argparse.Namespace()\n",
    "q_args.state_dim, q_args.obs_dim = p_args.state_dim, p_args.obs_dim \n",
    "q_args.model = 'nonamortized' # we compare to the nonamortized scenario of Campbell \n",
    "q_args.backwd_layers = (100,) # the number of neurons in the hidden layers of the NN that predicts backward parameters\n",
    "\n",
    "q_args = set_defaults(q_args)\n",
    "\n",
    "q = get_variational_model(q_args)\n",
    "elbo_mode = 'score,resampling,truncated,bptt_depth_2'\n",
    "elbo_list = []\n",
    "params_list = []\n",
    "key, key_init_params, key_montecarlo = jax.random.split(key, 3)\n",
    "\n",
    "filter_means_campbell = []\n",
    "smoothing_means_campbell = []\n",
    "data_campbell = []\n",
    "for data_dir in data_dirs: \n",
    "\n",
    "    p_args.load_from = data_dir\n",
    "    p_args.default_transition_matrix = os.path.join(data_dir, 'W.npy')\n",
    "    p, theta = get_generative_model(p_args, key_params) # no randomness here actually because we load params from Campbell et al. experiments\n",
    "    xs, ys = p.sample_multiple_sequences(key_sequences, \n",
    "                                        theta, \n",
    "                                        1, \n",
    "                                        p_args.seq_length,\n",
    "                                        single_split_seq=False,\n",
    "                                        load_from=p_args.load_from,\n",
    "                                        loaded_seq=p_args.loaded_seq)\n",
    "\n",
    "    trainer = SVITrainer(p=p,\n",
    "                        theta_star=theta,\n",
    "                        q=q,\n",
    "                        optimizer='adam',\n",
    "                        learning_rate=1e-3, \n",
    "                        optim_options='cst', # learning rate schedule\n",
    "                        num_epochs=1, # number of full sweeps through the sequence\n",
    "                        seq_length=p_args.seq_length,\n",
    "                        num_samples=100, # number of monte carlo samples (or trajectories in the offline case)\n",
    "                        frozen_params='', # which parameters to hold fixed\n",
    "                        num_seqs=p_args.num_seqs, \n",
    "                        training_mode='streaming,500', # 10 gradient steps per timestep should be enough to have stabilized q_t at each t\n",
    "                        elbo_mode=elbo_mode)\n",
    "    \n",
    "    fitted_variational_params, elbos, aux_results, all_params = trainer.fit(key_init_params, \n",
    "                                                                            key_montecarlo, \n",
    "                                                                            data=ys, \n",
    "                                                                            args=q_args)\n",
    "    \n",
    "    all_params = tree_get_idx(0, all_params)\n",
    "\n",
    "    params_list.append(all_params)\n",
    "    elbo_list.append(elbos.squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, data_dir in enumerate(data_dirs):\n",
    "    filter_means_campbell = np.load(os.path.join(data_dir, 'filter_means.npy'))\n",
    "    smoothing_means_campbell = np.load(os.path.join(data_dir, 'x_Tm1_means.npy'))\n",
    "\n",
    "    xs, ys = p.sample_multiple_sequences(key_sequences, # just reloading the sequencse\n",
    "                                        theta, \n",
    "                                        1, \n",
    "                                        p_args.seq_length,\n",
    "                                        single_split_seq=False,\n",
    "                                        load_from=data_dir,\n",
    "                                        loaded_seq=p_args.loaded_seq)\n",
    "    \n",
    "    x_true = xs[0]\n",
    "    y = ys[0]\n",
    "\n",
    "    filt_means_ours = params_list[idx].filt.mean\n",
    "    # plot_x_true_against_x_pred(x_true, all_params.filt.mean)#, y = all_params.filt.mean)\n",
    "    print(compute_rmse_x_true_against_x_pred(x_true, all_params.filt.mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing the means of $q_t^{\\lambda_{t}}$ to the true states $x_t^{*}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true = xs[0]\n",
    "plot_x_true_against_x_pred(x_true, all_params.filt.mean)#, y = all_params.filt.mean)\n",
    "compute_rmse_x_true_against_x_pred(x_true, all_params.filt.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing the 1-step smoothing means $\\hat{x}_{t-1|t} = \\int x_{t-1} q_{t-1|t}^{\\lambda_{t-1}} (x_t, x_{t-1}) q_t^{\\lambda_t}(d x_t)$ to the true states $x_{t-1}^{*}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10_000\n",
    "\n",
    "def x_tm1_t(carry, x): \n",
    "    key, filt_params_tm1, filt_params_t, backwd_params_net_t = x\n",
    "    key, subkey = jax.random.split(key, 2)\n",
    "    \n",
    "\n",
    "    filt_params_tm1 = q.filt_dist.format_params(filt_params_tm1)\n",
    "    filt_params_t = q.filt_dist.format_params(filt_params_t)\n",
    "\n",
    "    x_t = jax.vmap(q.filt_dist.sample, in_axes=(0, None))(jax.random.split(key, num_samples), \n",
    "                                                          filt_params_t)\n",
    "    \n",
    "    backwd_params_t = (backwd_params_net_t, (filt_params_tm1, filt_params_t))\n",
    "\n",
    "    x_tm1 = jax.vmap(q.backwd_kernel.sample, in_axes=(0,0, None))(jax.random.split(subkey, \n",
    "                                                                                   num_samples), \n",
    "                                                                    x_t, \n",
    "                                                                    backwd_params_t)\n",
    "\n",
    "    return None, jnp.mean(x_tm1, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "_, x_smoothed_tm1 = jax.lax.scan(x_tm1_t, \n",
    "                              init=None,\n",
    "                              xs=(jax.random.split(key, p_args.seq_length-1), \n",
    "                                   tree_droplast(all_params.filt),\n",
    "                                   tree_dropfirst(all_params.filt),\n",
    "                                   tree_dropfirst(all_params.backwd)))\n",
    "\n",
    "plot_x_true_against_x_pred(x_true[:-1], x_smoothed_tm1)#, y = all_params.filt.mean)\n",
    "compute_rmse_x_true_against_x_pred(x_true[:-1], x_smoothed_tm1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_args = argparse.Namespace()\n",
    "key = jax.random.PRNGKey(0)\n",
    "rmse_values = []\n",
    "\n",
    "for dim in [10,20,30,40,50]:\n",
    "    \n",
    "    p_args.state_dim, p_args.obs_dim = dim, dim\n",
    "    p_args.model = 'chaotic_rnn' \n",
    "    p_args.seq_length = 5_00\n",
    "    p_args.loaded_seq = False\n",
    "    p_args.load_from = '' #data/crnn/2022-10-18_15-28-00_Train_run'\n",
    "    p_args.range_transition_map_params = [0.8,1] # range of the components of the transition matrix\n",
    "    p_args.transition_matrix_conditionning = 'init_scale_by_dim' # constraint\n",
    "    p_args.default_transition_matrix = None ##os.path.join(p_args.load_from, \n",
    "                                                    # 'W.npy')\n",
    "\n",
    "    p_args.gamma = 2.5 # gamma for the chaotic rnn\n",
    "    p_args.tau = 0.025 # tau for the chaotic rnn\n",
    "    p_args.grid_size = 0.001 # discretization parameter for the chaotic rnn\n",
    "\n",
    "    p_args.emission_matrix_conditionning = 'diagonal'\n",
    "    p_args.range_emission_map_params = [-1,1]\n",
    "    p_args.default_emission_df = 2 # degrees of freedom for the emission noise\n",
    "    p_args.default_emission_matrix = 1.0 # diagonal values for the emission matrix\n",
    "    p_args.transition_bias = False \n",
    "    p_args.emission_bias = False\n",
    "    p_args.num_seqs = 1 #\n",
    "\n",
    "    p_args = set_defaults(p_args)\n",
    "    p_args.num_particles, p_args.num_smooth_particles = None, None\n",
    "\n",
    "    key, key_params, key_sequences = jax.random.split(key, 3)\n",
    "    p, theta = get_generative_model(p_args, key_params)\n",
    "    xs, ys = p.sample_multiple_sequences(key_sequences, \n",
    "                                        theta, \n",
    "                                        1, \n",
    "                                        p_args.seq_length,\n",
    "                                        single_split_seq=False,\n",
    "                                        load_from=p_args.load_from,\n",
    "                                        loaded_seq=p_args.loaded_seq)\n",
    "    q_args = argparse.Namespace()\n",
    "    q_args.state_dim, q_args.obs_dim = p_args.state_dim, p_args.obs_dim \n",
    "    q_args.model = 'nonamortized' # we compare to the nonamortized scenario of Campbell \n",
    "    q_args.backwd_layers = (100,) # the number of neurons in the hidden layers of the NN that predicts backward parameters\n",
    "\n",
    "    q_args = set_defaults(q_args)\n",
    "\n",
    "    q = get_variational_model(q_args)\n",
    "\n",
    "    for elbo_mode in ['score,truncated,resampling,bptt_depth_2']:\n",
    "        trainer = SVITrainer(p=p,\n",
    "                            theta_star=theta,\n",
    "                            q=q,\n",
    "                            optimizer='adam',\n",
    "                            learning_rate=1e-3, \n",
    "                            optim_options='cst', # learning rate schedule\n",
    "                            num_epochs=1, # number of full sweeps through the sequence\n",
    "                            seq_length=p_args.seq_length,\n",
    "                            num_samples=100, # number of monte carlo samples (or trajectories in the offline case)\n",
    "                            frozen_params='', # which parameters to hold fixed\n",
    "                            num_seqs=p_args.num_seqs, \n",
    "                            training_mode='streaming,5', # 10 gradient steps per timestep should be enough to have stabilized q_t at each t\n",
    "                            elbo_mode=elbo_mode)\n",
    "\n",
    "        key_init_params, key_montecarlo = jax.random.split(key, 2)\n",
    "        fitted_variational_params, elbos, aux_results, all_params = trainer.fit(key_init_params, \n",
    "                                        key_montecarlo, \n",
    "                                        data=ys, \n",
    "                                        args=q_args)\n",
    "        \n",
    "\n",
    "        all_params = tree_get_idx(0, all_params)\n",
    "        x_true = xs[0]\n",
    "        plot_x_true_against_x_pred(x_true, all_params.filt.mean)\n",
    "        \n",
    "        #, y = all_params.filt.mean)\n",
    "        # rmse_values.append(compute_rmse_x_true_against_x_pred(x_true, all_params.filt.mean))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive gradients for faster convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_args = argparse.Namespace()\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "p_args.state_dim, p_args.obs_dim = 5,5\n",
    "p_args.model = 'chaotic_rnn' \n",
    "p_args.seq_length = 5_00\n",
    "p_args.loaded_seq = False\n",
    "p_args.load_from = '' #data/crnn/2022-10-18_15-28-00_Train_run'\n",
    "p_args.range_transition_map_params = [0.6,0.9] # range of the components of the transition matrix\n",
    "p_args.transition_matrix_conditionning = 'init_scale_by_dim' # constraint\n",
    "p_args.default_transition_matrix = None ##os.path.join(p_args.load_from, \n",
    "                                                 # 'W.npy')\n",
    "\n",
    "p_args.gamma = 2.5 # gamma for the chaotic rnn\n",
    "p_args.tau = 0.025 # tau for the chaotic rnn\n",
    "p_args.grid_size = 0.001 # discretization parameter for the chaotic rnn\n",
    "\n",
    "p_args.emission_matrix_conditionning = 'diagonal'\n",
    "p_args.range_emission_map_params = [-1,1]\n",
    "p_args.default_emission_df = 2 # degrees of freedom for the emission noise\n",
    "p_args.default_emission_matrix = 1.0 # diagonal values for the emission matrix\n",
    "p_args.transition_bias = False \n",
    "p_args.emission_bias = False\n",
    "p_args.num_seqs = 1 #\n",
    "\n",
    "p_args = set_defaults(p_args)\n",
    "p_args.num_particles, p_args.num_smooth_particles = None, None\n",
    "\n",
    "key, key_params, key_sequences = jax.random.split(key, 3)\n",
    "p, theta = get_generative_model(p_args, key_params)\n",
    "xs, ys = p.sample_multiple_sequences(key_sequences, \n",
    "                                     theta, \n",
    "                                     1, \n",
    "                                     p_args.seq_length,\n",
    "                                     single_split_seq=False,\n",
    "                                     load_from=p_args.load_from,\n",
    "                                     loaded_seq=p_args.loaded_seq)\n",
    "\n",
    "\n",
    "q_args = argparse.Namespace()\n",
    "q_args.model = 'johnson_backward,10'\n",
    "q_args.state_dim, q_args.obs_dim = p_args.state_dim, p_args.obs_dim\n",
    "q_args.transition_bias = False\n",
    "q_args.emission_bias = False\n",
    "layers = [int(nb) for nb in q_args.model.split(',')[-1].split('_')]\n",
    "q_args.update_layers = (*layers,)\n",
    "q_args.backwd_layers = 0\n",
    "q_args.transition_matrix_conditionning = 'diagonal'\n",
    "q_args.range_transition_map_params = (0.6, 0.9)\n",
    "q_args.anisotropic = False\n",
    "\n",
    "q_args = set_defaults(q_args)\n",
    "\n",
    "q = get_variational_model(q_args)\n",
    "\n",
    "num_epochs = 50\n",
    "num_fits = 10\n",
    "\n",
    "elbos_list = dict()\n",
    "\n",
    "\n",
    "key, key_init_params, key_montecarlo = jax.random.split(key, 3)\n",
    "keys_params = jax.random.split(key, num_fits)\n",
    "keys_mc = jax.random.split(key, num_fits)\n",
    "for training_mode, elbo_mode, learning_rate in zip([f'reset,{p_args.seq_length},1', 'streaming,1,difference'],\n",
    "                                                    ['autodiff_on_batch', 'score,resampling,bptt_depth_2'],\n",
    "                                                    [1e-3, 1e-3]):\n",
    "    elbos_list[elbo_mode] = []\n",
    "    \n",
    "    trainer = SVITrainer(p=p,\n",
    "                        theta_star=theta,\n",
    "                        q=q,\n",
    "                        optimizer='adam',\n",
    "                        learning_rate=learning_rate, \n",
    "                        optim_options='cst', # learning rate schedule\n",
    "                        num_epochs=num_epochs, # number of full sweeps through the sequence\n",
    "                        seq_length=p_args.seq_length,\n",
    "                        num_samples=10, # number of monte carlo samples (or trajectories in the offline case)\n",
    "                        frozen_params='', # which parameters to hold fixed\n",
    "                        num_seqs=p_args.num_seqs, \n",
    "                        training_mode=training_mode, # 10 gradient steps per timestep should be enough to have stabilized q_t at each t\n",
    "                        elbo_mode=elbo_mode)\n",
    "    for fit_nb in range(num_fits):\n",
    "\n",
    "        fitted_variational_params, elbos, aux_results, all_params = trainer.fit(keys_params[fit_nb], \n",
    "                                        keys_mc[fit_nb], \n",
    "                                        data=ys, \n",
    "                                        args=q_args)\n",
    "        \n",
    "        elbos_list[elbo_mode].append(elbos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('elbos_faster_convergence','wb') as f:\n",
    "    dill.dump(elbos_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()\n",
    "\n",
    "plt.rcParams['font.weight'] = 'normal'\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "for idx, (method_name, elbo_mode) in enumerate(zip(['Backward MC', 'Score'], \n",
    "                                                   ['autodiff_on_batch', 'score,resampling,bptt_depth_2'])):\n",
    "    elbo_values = jnp.array(elbos_list[elbo_mode])[:,:,-1,:]\n",
    "    results[method_name] = (jnp.mean(elbo_values, axis=0).flatten(), jnp.std(elbo_values, axis=0).flatten())\n",
    "\n",
    "for method_name in ['Backward MC', 'Score']:\n",
    "    means = results[method_name][0]\n",
    "    std = results[method_name][1]\n",
    "    plt.plot(means, label=method_name)\n",
    "    plt.fill_between(range(num_epochs), means-std, means+std, alpha=0.4)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.tight_layout()\n",
    "plt.autoscale(True)\n",
    "plt.ylabel('ELBO')\n",
    "plt.savefig('faster_convergence.pdf',format='pdf')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning on streaming data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on 100k observation in 10 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "\n",
    "\n",
    "p_args = argparse.Namespace()\n",
    "p_args.state_dim, p_args.obs_dim = 10,10\n",
    "p_args.model = 'chaotic_rnn' \n",
    "p_args.seq_length = 100_000\n",
    "p_args.loaded_seq = False\n",
    "p_args.load_from = '' #data/crnn/2022-10-18_15-28-00_Train_run'\n",
    "p_args.range_transition_map_params = [0.8,1] # range of the components of the transition matrix\n",
    "p_args.transition_matrix_conditionning = 'init_scale_by_dim' # constraint\n",
    "p_args.default_transition_matrix = None ##os.path.join(p_args.load_from, \n",
    "                                                 # 'W.npy')\n",
    "\n",
    "p_args.gamma = 2.5 # gamma for the chaotic rnn\n",
    "p_args.tau = 0.025 # tau for the chaotic rnn\n",
    "p_args.grid_size = 0.001 # discretization parameter for the chaotic rnn\n",
    "\n",
    "p_args.emission_matrix_conditionning = 'diagonal'\n",
    "p_args.range_emission_map_params = [-1,1]\n",
    "p_args.default_emission_df = 2 # degrees of freedom for the emission noise\n",
    "p_args.default_emission_matrix = 1.0 # diagonal values for the emission matrix\n",
    "p_args.transition_bias = False \n",
    "p_args.emission_bias = False\n",
    "p_args.num_seqs = 1 #\n",
    "\n",
    "p_args = set_defaults(p_args)\n",
    "p_args.num_particles, p_args.num_smooth_particles = None, None\n",
    "\n",
    "key, key_params, key_sequences = jax.random.split(key, 3)\n",
    "p, theta = get_generative_model(p_args, key_params)\n",
    "xs, ys = p.sample_multiple_sequences(key_sequences, \n",
    "                                     theta, \n",
    "                                     1, \n",
    "                                     p_args.seq_length,\n",
    "                                     single_split_seq=False,\n",
    "                                     load_from=p_args.load_from,\n",
    "                                     loaded_seq=p_args.loaded_seq)\n",
    "\n",
    "q_args = argparse.Namespace()\n",
    "q_args.model = 'johnson_backward,100'\n",
    "q_args.state_dim, q_args.obs_dim = p_args.state_dim, p_args.obs_dim\n",
    "q_args.transition_bias = False\n",
    "q_args.emission_bias = False\n",
    "layers = [int(nb) for nb in q_args.model.split(',')[-1].split('_')]\n",
    "q_args.update_layers = (*layers,)\n",
    "q_args.backwd_layers = 0\n",
    "q_args.transition_matrix_conditionning = 'diagonal'\n",
    "q_args.range_transition_map_params = (0.9, 1)\n",
    "q_args.anisotropic = False\n",
    "\n",
    "q_args = set_defaults(q_args)\n",
    "\n",
    "q = get_variational_model(q_args)\n",
    "\n",
    "for elbo_mode in ['score,resampling,bptt_depth_2']:\n",
    "\n",
    "    trainer = SVITrainer(p=p,\n",
    "                        theta_star=theta,\n",
    "                        q=q,\n",
    "                        optimizer='adam',\n",
    "                        learning_rate=1e-3, \n",
    "                        optim_options='cst', # learning rate schedule\n",
    "                        num_epochs=1, # number of full sweeps through the sequence\n",
    "                        seq_length=p_args.seq_length,\n",
    "                        num_samples=100, # number of monte carlo samples (or trajectories in the offline case)\n",
    "                        frozen_params='', # which parameters to hold fixed\n",
    "                        num_seqs=p_args.num_seqs, \n",
    "                        training_mode='streaming,1,difference', \n",
    "                        elbo_mode=elbo_mode)\n",
    "\n",
    "\n",
    "    key_init_params, key_montecarlo = jax.random.split(key, 2)\n",
    "    \n",
    "    fitted_variational_params, elbos, aux_results, all_params = trainer.fit(key_init_params, \n",
    "                                    key_montecarlo, \n",
    "                                    data=ys, \n",
    "                                    args=q_args)\n",
    "    \n",
    "    elbo_for_all_epochs = elbos.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing smoothing performance on a subset of the data for different stages of the learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 5\n",
    "# plt.rcParams.update({'font.size': 22})\n",
    "#\n",
    "plt.rcParams['font.weight'] = 'normal'\n",
    "plt.rcParams['font.size'] = 20\n",
    "\n",
    "# matplotlib.rc('xtick', labelsize=20) \n",
    "# matplotlib.rc('ytick', labelsize=20) \n",
    "\n",
    "steps = jnp.array([0, \n",
    "                   10_000, \n",
    "                   p_args.seq_length-1]) #jnp.arange(0, p_args.seq_length, p_args.seq_length // num_points)\n",
    "colors = ['red', 'green', 'blue']\n",
    "\n",
    "all_params_at_steps = tree_get_idx(-1, \n",
    "                          jax.tree_map(lambda x: jnp.take(x, indices=steps, axis=1), \n",
    "                                       all_params))\n",
    "\n",
    "eval_length = 5_000\n",
    "# x_true = xs[0][:eval_length]\n",
    "# y = ys[0][:eval_length]\n",
    "dim = 9\n",
    "\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(2,1)\n",
    "\n",
    "ax0.plot(elbo_for_all_epochs, label='ELBO', c='black')\n",
    "ax0.set_xlabel('Iteration')\n",
    "ax0.set_ylabel('ELBO')\n",
    "# plt.vlines(x=steps, ymin=-100, ymax=0, colors=colors)\n",
    "# plt.tight_layout()\n",
    "# plt.autoscale(True)\n",
    "\n",
    "\n",
    "# _ , axes = plt.subplots(dims, 1, figsize=(10,1.5*dims))\n",
    "key_test = jax.random.PRNGKey(5)\n",
    "x_true, y = p.sample_seq(key_test, theta, seq_length=eval_length)\n",
    "ax1.plot(x_true[:,dim], c='black', label='True states', alpha=0.4)\n",
    "ax1.set_xlabel('Timestep')\n",
    "ax1.set_ylabel(f'States dim x')\n",
    "rmse_values = []\n",
    "\n",
    "for idx, (step, color) in enumerate(zip(steps,colors)):\n",
    "    params = tree_get_idx(idx, all_params_at_steps)\n",
    "    x_smoothed = q.smooth_seq(y, params)[0]\n",
    "    rmse_values.append(compute_rmse_x_true_against_x_pred(x_true, \n",
    "                                                          x_smoothed))\n",
    "    ax1.plot(x_smoothed[:,dim],c=color, label=f'Step {step}', alpha=0.7)\n",
    "    # ax1.legend()\n",
    "\n",
    "\n",
    "plt.autoscale(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# plt.legend()\n",
    "plt.savefig('training_100k.pdf', format='pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
