{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments for the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax, jax.numpy as jnp\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "import math\n",
    "from src.utils.misc import tree_dropfirst, tree_droplast\n",
    "\n",
    "jax.config.update('jax_platform_name', 'gpu') # switch to 'cpu' if unavailable or depending on computation\n",
    "jax.config.update('jax_enable_x64', False) # turn on float64 to prevent numerical instabilities on hard data with few particles\n",
    "\n",
    "from src.stats.hmm import get_generative_model\n",
    "from src.variational import get_variational_model\n",
    "from src.utils.misc import tree_get_idx, save_params, tree_get_slice\n",
    "from src.training import SVITrainer\n",
    "from src.stats.ula import ULA\n",
    "\n",
    "def set_defaults(args):\n",
    "    args.default_prior_mean = 0.0 # default value for the mean of Gaussian prior\n",
    "    args.default_prior_base_scale = 0.1 # default value for the diagonal components of the covariance matrix of the prior\n",
    "    args.default_transition_base_scale = 0.1 # default value for the diagonal components of the covariance matrix of the transition kernel\n",
    "    args.default_transition_bias = 0.0\n",
    "    args.default_emission_base_scale = 0.1\n",
    "    args.parametrization = 'cov_chol'\n",
    "    return args\n",
    "\n",
    "\n",
    "def plot_x_true_against_x_pred(x_true, x_pred, y=None, save=False):\n",
    "    dims = x_true.shape[-1]\n",
    "    _ , axes = plt.subplots(dims, 1, figsize=(15,2*dims))\n",
    "    for dim in range(dims):\n",
    "        axes[dim].plot(x_true[:,dim], c='red', label='True', alpha=0.7)\n",
    "        axes[dim].plot(x_pred[:,dim], c='green', label='Pred', alpha=0.7)\n",
    "        axes[dim].legend()\n",
    "        if y is not None:\n",
    "            axes[dim].plot(y[:,dim], c='black', label='Obs', alpha=0.5)\n",
    "    if save: plt.savefig('test.pdf', format='pdf')   \n",
    "    \n",
    "def plot_data(y):\n",
    "    dims = y.shape[-1]\n",
    "    _ , axes = plt.subplots(dims, 1, figsize=(15,2*dims))\n",
    "    for dim in range(dims):\n",
    "        axes[dim].plot(y[:,dim], label='Data')\n",
    "\n",
    "def compute_rmse_x_true_against_x_pred(x_true, x_pred):\n",
    "    return jnp.mean(jnp.sqrt(jnp.mean((x_true-x_pred)**2, axis=-1)), \n",
    "                    axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear-Gaussian HMM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up data and generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0) # seed\n",
    "p_args = argparse.Namespace() # will contain all configuration information for the generative model \n",
    "p_args.state_dim, p_args.obs_dim = 2,2 # dimensions of state and observation spaces\n",
    "p_args.model = 'linear'\n",
    "p_args.seq_length = 500\n",
    "p_args.emission_bias = False # no bias in the emission kernel, i.e. only Y_t = B X_t + noise\n",
    "p_args.transition_bias = False # no bias in the transition kernel, i.e. only X_t = A X_{t-1} + noise\n",
    "p_args.transition_matrix_conditionning = 'diagonal' # the transition matrix will be diagonal\n",
    "p_args.range_transition_map_params = [0.9,1] # the diagonal values are constrained to [0.9 1]\n",
    "p_args.num_seqs = 1 #\n",
    "p_args = set_defaults(p_args) # setting default values for the covariances with realistic Signal to noise ratios\n",
    "\n",
    "key, key_theta, key_sequences = jax.random.split(key, 3)\n",
    "p, theta = get_generative_model(p_args, key_theta) # getting the model and its parameters \n",
    "# save_args(p_args, 'args', exp_path) # saving the params of the generative model\n",
    "# save_params(theta, 'theta', exp_path) # saving the parameters\n",
    "# jnp.save(os.path.join(exp_path, 'xs'), xs)\n",
    "# jnp.save(os.path.join(exp_path, 'ys'), ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = p.sample_multiple_sequences(key_sequences, \n",
    "                                     theta, \n",
    "                                     p_args.num_seqs, \n",
    "                                     p_args.seq_length,\n",
    "                                     single_split_seq=False, # not sampling a sequence and splitting into subsequences\n",
    "                                     load_from='', # not loading from external folder\n",
    "                                     loaded_seq=False) # not loading sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up variational model and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_args = argparse.Namespace() # will contain all configuration information for the variational model \n",
    "q_args.state_dim, q_args.obs_dim = p_args.state_dim, p_args.obs_dim \n",
    "q_args.model = 'linear' \n",
    "q_args.emission_bias = False \n",
    "q_args.transition_bias = False \n",
    "q_args.transition_matrix_conditionning = 'diagonal' \n",
    "q_args.range_transition_map_params = [0.9,1] \n",
    "q_args = set_defaults(q_args) \n",
    "# sub_exp_path = os.path.join(exp_path, q_args.model)\n",
    "# os.makedirs(sub_exp_path, exist_ok=True)\n",
    "# save_args(p_args, 'args', sub_exp_path) # saving the params of the generative model\n",
    "q = get_variational_model(q_args) # getting the corresponding model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning in an offline setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "for elbo_mode in ['closed_form', 'autodiff_on_batch', 'score,paris,bptt_depth_2']:\n",
    "    trainer = SVITrainer(p=p,\n",
    "                        theta_star=theta,\n",
    "                        q=q,\n",
    "                        optimizer='adam',\n",
    "                        learning_rate=1e-3, \n",
    "                        optim_options='cst', # learning rate schedule\n",
    "                        num_epochs=1000, # number of full sweeps through the sequence\n",
    "                        seq_length=p_args.seq_length,\n",
    "                        num_samples=2, # number of monte carlo samples (or trajectories in the offline case)\n",
    "                        frozen_params='', # which parameters to hold fixed\n",
    "                        num_seqs=p_args.num_seqs, \n",
    "                        training_mode=f'reset,{p_args.seq_length},1', \n",
    "                        elbo_mode=elbo_mode,\n",
    "                        logging_type='basic_logging')\n",
    "\n",
    "\n",
    "    key_init_params, key_montecarlo = jax.random.split(key, 2)\n",
    "    fitted_variational_params, elbos, aux_results, all_params = trainer.fit(key_init_params, \n",
    "                                    key_montecarlo, \n",
    "                                    data=(xs, ys), \n",
    "                                    log_writer=None, \n",
    "                                    args=q_args, \n",
    "                                    log_writer_monitor=None)\n",
    "\n",
    "    elbo_for_all_epochs = elbos.flatten()\n",
    "    plt.plot(elbo_for_all_epochs, label=elbo_mode)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fitted_variational_params)\n",
    "x_smoothed_kalman = p.smooth_seq(ys[0], theta)[0]\n",
    "x_smoothed_variational = q.smooth_seq(ys[0], fitted_variational_params)[0]\n",
    "plot_x_true_against_x_pred(x_smoothed_kalman, x_smoothed_variational)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_args.seq_length = 100_000 # keeping all previous settings except the sequence length\n",
    "\n",
    "xs, ys = p.sample_multiple_sequences(key_sequences, \n",
    "                                     theta, \n",
    "                                     num_seqs=1, \n",
    "                                     seq_length=p_args.seq_length,\n",
    "                                     single_split_seq=False,\n",
    "                                     load_from='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ula = ULA(p)\n",
    "fitted_params = ula.learn_params(key, \n",
    "                                 ys[0], \n",
    "                                 p.get_random_params(key, \n",
    "                                                     p_args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo_mode = 'score,paris,bptt_depth_2'\n",
    "trainer = SVITrainer(p=p,\n",
    "                    theta_star=theta,\n",
    "                    q=q,\n",
    "                    optimizer='adam',\n",
    "                    learning_rate=1e-3, \n",
    "                    optim_options='cst', # learning rate schedule\n",
    "                    num_epochs=1, # number of full sweeps through the sequence\n",
    "                    seq_length=p_args.seq_length,\n",
    "                    num_samples=20, # number of monte carlo samples (or trajectories in the online case)\n",
    "                    frozen_params='', # which parameters to hold fixed\n",
    "                    num_seqs=p_args.num_seqs, \n",
    "                    training_mode=f'streaming,1,difference', # L_t - L_{t-1} / t\n",
    "                    elbo_mode=elbo_mode,\n",
    "                    logging_type='basic_logging')\n",
    "\n",
    "key_init_params, key_montecarlo = jax.random.split(key, 2)\n",
    "fitted_variational_params, elbos, aux_results, all_params = trainer.fit(key_init_params, \n",
    "                                key_montecarlo, \n",
    "                                data=(xs, ys), \n",
    "                                args=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo_for_all_epochs = elbos.flatten()\n",
    "plt.plot(elbo_for_all_epochs, label=elbo_mode)\n",
    "plt.legend()\n",
    "\n",
    "x_smoothed_kalman = p.smooth_seq(ys[0][:1000], theta)[0]\n",
    "x_smoothed_variational = q.smooth_seq(ys[0][:1000], fitted_variational_params)[0]\n",
    "plot_x_true_against_x_pred(x_smoothed_kalman, x_smoothed_variational)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaotic RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_args = argparse.Namespace()\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "p_args.state_dim, p_args.obs_dim = 10,10\n",
    "p_args.model = 'chaotic_rnn' \n",
    "p_args.seq_length = 5_000\n",
    "p_args.loaded_seq = False\n",
    "p_args.load_from = '' #data/crnn/2022-10-18_15-28-00_Train_run'\n",
    "p_args.range_transition_map_params = [0.8,1] # range of the components of the transition matrix\n",
    "p_args.transition_matrix_conditionning = 'init_scale_by_dim' # constraint\n",
    "p_args.default_transition_matrix = None #os.path.join(p_args.load_from, \n",
    "                                           #       'W.npy')\n",
    "\n",
    "p_args.gamma = 2.5 # gamma for the chaotic rnn\n",
    "p_args.tau = 0.025 # tau for the chaotic rnn\n",
    "p_args.grid_size = 0.001 # discretization parameter for the chaotic rnn\n",
    "\n",
    "p_args.emission_matrix_conditionning = 'diagonal'\n",
    "p_args.range_emission_map_params = [-1,1]\n",
    "p_args.default_emission_df = 2 # degrees of freedom for the emission noise\n",
    "p_args.default_emission_matrix = 1.0 # diagonal values for the emission matrix\n",
    "p_args.transition_bias = False \n",
    "p_args.emission_bias = False\n",
    "p_args.num_seqs = 1 #\n",
    "\n",
    "p_args = set_defaults(p_args)\n",
    "p_args.num_particles, p_args.num_smooth_particles = None, None\n",
    "\n",
    "key, key_params, key_sequences = jax.random.split(key, 3)\n",
    "p, theta = get_generative_model(p_args, key_params)\n",
    "xs, ys = p.sample_multiple_sequences(key_sequences, \n",
    "                                     theta, \n",
    "                                     1, \n",
    "                                     p_args.seq_length,\n",
    "                                     single_split_seq=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side-by-side comparison with Campbell et al."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonamortized training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_args = argparse.Namespace()\n",
    "q_args.state_dim, q_args.obs_dim = p_args.state_dim, p_args.obs_dim \n",
    "q_args.model = 'nonamortized' # we compare to the nonamortized scenario of Campbell \n",
    "q_args.backwd_layers = (10,) # the number of neurons in the hidden layers of the NN that predicts backward parameters\n",
    "\n",
    "q_args = set_defaults(q_args)\n",
    "\n",
    "q = get_variational_model(q_args)\n",
    "\n",
    "for elbo_mode in ['score,truncated,paris,bptt_depth_2']:\n",
    "    trainer = SVITrainer(p=p,\n",
    "                        theta_star=theta,\n",
    "                        q=q,\n",
    "                        optimizer='adam',\n",
    "                        learning_rate=1e-2, \n",
    "                        optim_options='cst', # learning rate schedule\n",
    "                        num_epochs=1, # number of full sweeps through the sequence\n",
    "                        seq_length=p_args.seq_length,\n",
    "                        num_samples=2, # number of monte carlo samples (or trajectories in the offline case)\n",
    "                        frozen_params='', # which parameters to hold fixed\n",
    "                        num_seqs=p_args.num_seqs, \n",
    "                        training_mode='streaming,10', # 10 gradient steps per timestep should be enough to have stabilized q_t at each t\n",
    "                        elbo_mode=elbo_mode)\n",
    "\n",
    "    key_init_params, key_montecarlo = jax.random.split(key, 2)\n",
    "    fitted_variational_params, elbos, aux_results, all_params = trainer.fit(key_init_params, \n",
    "                                    key_montecarlo, \n",
    "                                    data=(xs, ys), \n",
    "                                    args=q_args)\n",
    "    \n",
    "\n",
    "    all_params = tree_get_idx(0, all_params)\n",
    "\n",
    "    # elbo_for_all_epochs = elbos.flatten()\n",
    "    # plt.plot(elbo_for_all_epochs, label=elbo_mode)\n",
    "# plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing the means of $q_t^{\\lambda_{t}}$ to the true states $x_t^{*}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true = xs[0]\n",
    "plot_x_true_against_x_pred(x_true, all_params.filt.mean)#, y = all_params.filt.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing the 1-step smoothing means $\\hat{x}_{t-1|t} = \\int x_{t-1} q_{t-1|t}^{\\lambda_{t-1}} (x_t, x_{t-1}) q_t^{\\lambda_t}(d x_t)$ to the true states $x_{t-1}^{*}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10_000\n",
    "\n",
    "def x_tm1_t(carry, x): \n",
    "    key, filt_params_tm1, filt_params_t, backwd_params_net_t = x\n",
    "    key, subkey = jax.random.split(key, 2)\n",
    "    \n",
    "\n",
    "    filt_params_tm1 = q.filt_dist.format_params(filt_params_tm1)\n",
    "    filt_params_t = q.filt_dist.format_params(filt_params_t)\n",
    "\n",
    "    x_t = jax.vmap(q.filt_dist.sample, in_axes=(0, None))(jax.random.split(key, num_samples), \n",
    "                                                          filt_params_t)\n",
    "    \n",
    "    backwd_params_t = (backwd_params_net_t, (filt_params_tm1, filt_params_t))\n",
    "\n",
    "    x_tm1 = jax.vmap(q.backwd_kernel.sample, in_axes=(0,0, None))(jax.random.split(subkey, \n",
    "                                                                                   num_samples), \n",
    "                                                                    x_t, \n",
    "                                                                    backwd_params_t)\n",
    "\n",
    "    return None, jnp.mean(x_tm1, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "_, x_smoothed_tm1 = jax.lax.scan(x_tm1_t, \n",
    "                              init=None,\n",
    "                              xs=(jax.random.split(key, p_args.seq_length-1), \n",
    "                                   tree_droplast(all_params.filt),\n",
    "                                   tree_dropfirst(all_params.filt),\n",
    "                                   tree_dropfirst(all_params.backwd)))\n",
    "\n",
    "plot_x_true_against_x_pred(x_true[:-1], x_smoothed_tm1)#, y = all_params.filt.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning on streaming data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training on 100k observation in 10 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "\n",
    "\n",
    "p_args.state_dim, p_args.obs_dim = 10,10\n",
    "p_args.model = 'chaotic_rnn' \n",
    "p_args.seq_length = 100_000\n",
    "p_args.loaded_seq = False\n",
    "p_args.load_from = '' #data/crnn/2022-10-18_15-28-00_Train_run'\n",
    "p_args.range_transition_map_params = [0.8,1] # range of the components of the transition matrix\n",
    "p_args.transition_matrix_conditionning = 'init_scale_by_dim' # constraint\n",
    "p_args.default_transition_matrix = '' #os.path.join(p_args.load_from, 'W.npy')\n",
    "\n",
    "q_args = argparse.Namespace()\n",
    "q_args.model = 'johnson_backward,100'\n",
    "q_args.state_dim, q_args.obs_dim = p_args.state_dim, p_args.obs_dim\n",
    "q_args.transition_bias = False\n",
    "q_args.emission_bias = False\n",
    "layers = [int(nb) for nb in q_args.model.split(',')[-1].split('_')]\n",
    "q_args.update_layers = (*layers,)\n",
    "q_args.backwd_layers = 0\n",
    "q_args.transition_matrix_conditionning = 'diagonal'\n",
    "q_args.range_transition_map_params = (0.9, 1)\n",
    "q_args.anisotropic = False\n",
    "\n",
    "q_args = set_defaults(q_args)\n",
    "\n",
    "q = get_variational_model(q_args)\n",
    "\n",
    "for elbo_mode in ['score,paris,bptt_depth_2']:\n",
    "\n",
    "    trainer = SVITrainer(p=p,\n",
    "                        theta_star=theta,\n",
    "                        q=q,\n",
    "                        optimizer='adam',\n",
    "                        learning_rate=1e-3, \n",
    "                        optim_options='cst', # learning rate schedule\n",
    "                        num_epochs=1, # number of full sweeps through the sequence\n",
    "                        seq_length=p_args.seq_length,\n",
    "                        num_samples=100, # number of monte carlo samples (or trajectories in the offline case)\n",
    "                        frozen_params='', # which parameters to hold fixed\n",
    "                        num_seqs=p_args.num_seqs, \n",
    "                        training_mode='streaming,1,difference', \n",
    "                        elbo_mode=elbo_mode)\n",
    "\n",
    "\n",
    "    key_init_params, key_montecarlo = jax.random.split(key, 2)\n",
    "    \n",
    "    fitted_variational_params, elbos, aux_results, all_params = trainer.fit(key_init_params, \n",
    "                                    key_montecarlo, \n",
    "                                    data=(xs, ys), \n",
    "                                    args=q_args)\n",
    "    \n",
    "    elbo_for_all_epochs = elbos.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing smoothing performance on a subset of the data for different stages of the learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = jnp.arange(0, 100_000, 10_000)\n",
    "all_params = tree_get_idx(-1, \n",
    "                          jax.tree_map(lambda x: jnp.take(x, indices=steps, axis=1), \n",
    "                                       all_params))\n",
    "\n",
    "eval_length = 5_000\n",
    "x_true = xs[0][:eval_length]\n",
    "y = ys[0][:eval_length]\n",
    "\n",
    "alphas = jnp.linspace(0.2,1,len(steps)).tolist()\n",
    "\n",
    "plt.plot(elbo_for_all_epochs, label='ELBO', c='black')\n",
    "plt.xlabel('Timestep')\n",
    "plt.ylabel('ELBO')\n",
    "plt.vlines(x=steps, ymin=-100, ymax=0, alpha=alphas, linestyles='dotted', colors='black')\n",
    "\n",
    "plt.savefig('ELBO.pdf', format='pdf')\n",
    "\n",
    "\n",
    "dims = p_args.state_dim\n",
    "_ , axes = plt.subplots(dims, 1, figsize=(15,2*dims))\n",
    "for dim in range(dims): \n",
    "    axes[dim].plot(x_true[:,dim], c='red', label='True', alpha=0.7)\n",
    "    axes[dim].set_xlabel('Timestep')\n",
    "    axes[dim].set_ylabel(f'States dim {dim}')\n",
    "rmse_values = []\n",
    "for idx, (step, alpha) in enumerate(zip(steps, alphas)):\n",
    "    params = tree_get_idx(idx, all_params)\n",
    "    x_smoothed = q.smooth_seq(y, params)[0]\n",
    "    rmse_values.append(compute_rmse_x_true_against_x_pred(x_true, \n",
    "                                                          x_smoothed))\n",
    "    for dim in range(dims): \n",
    "        axes[dim].plot(x_smoothed[:,dim], linestyle='dotted', c='black', label=f'Step {step}', alpha=alpha)\n",
    "plt.savefig('smoothing_multiple_params.pdf', format='pdf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
