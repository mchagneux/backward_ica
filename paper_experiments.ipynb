{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments for the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax, jax.numpy as jnp\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "import math\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "jax.config.update('jax_enable_x64', True)\n",
    "\n",
    "from src.stats.hmm import get_generative_model\n",
    "from src.variational import get_variational_model\n",
    "from src.training import SVITrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_defaults(args):\n",
    "    args.default_prior_mean = 0.0 # default value for the mean of Gaussian prior\n",
    "    args.default_prior_base_scale = math.sqrt(0.1) # default value for the diagonal components of the covariance matrix of the prior\n",
    "    args.default_transition_base_scale = math.sqrt(0.1) # default value for the diagonal components of the covariance matrix of the transition kernel\n",
    "    args.default_transition_bias = 0.0\n",
    "    args.default_emission_base_scale = math.sqrt(0.1)\n",
    "    args.parametrization = 'cov_chol'\n",
    "    return args\n",
    "\n",
    "\n",
    "def plot_x_true_against_x_pred(x_true, x_pred):\n",
    "    dims = x_true.shape[-1]\n",
    "    _ , axes = plt.subplots(dims, 1, figsize=(15,2*dims))\n",
    "    for dim in range(dims):\n",
    "        axes[dim].plot(x_true[:,dim], c='red', label='True')\n",
    "        axes[dim].plot(x_pred[:,dim], c='green', label='Pred')\n",
    "        axes[dim].legend()\n",
    "\n",
    "def compute_rmse_x_true_against_x_pred(x_true, x_pred):\n",
    "    rmse = jax.vmap(lambda x,y: jnp.mean(jnp.sqrt(jnp.mean((x-y)**2, axis=-1)), axis=0))\n",
    "    print('RMSE:',rmse.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear-Gaussian HMM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = datetime.now().strftime('%Y_%m_%d__%H_%M_%S')\n",
    "# exp_path = os.path.join('experiments', date)\n",
    "# os.makedirs(exp_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up data and generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0) # seed\n",
    "p_args = argparse.Namespace() # will contain all configuration information for the generative model \n",
    "p_args.state_dim, p_args.obs_dim = 10,10\n",
    "p_args.model = 'linear' \n",
    "p_args.seq_length = 500\n",
    "p_args.emission_bias = False # no bias in the emission kernel, i.e. only Y_t = B X_t + noise\n",
    "p_args.transition_bias = False # no bias in the transition kernel, i.e. only X_t = A X_{t-1} + noise\n",
    "p_args.transition_matrix_conditionning = 'diagonal' # the transition matrix will be diagonal\n",
    "p_args.range_transition_map_params = [0.8,0.9] # the diagonal values are constrained to [0.9 1]\n",
    "p_args.num_seqs = 1 #\n",
    "p_args = set_defaults(p_args) # setting default values for the covariances with realistic Signal to noise ratios\n",
    "\n",
    "key, key_theta, key_sequences = jax.random.split(key, 3)\n",
    "p, theta = get_generative_model(p_args, key_theta) # getting the model and its parameters \n",
    "print(theta)\n",
    "# save_args(p_args, 'args', exp_path) # saving the params of the generative model\n",
    "# save_params(theta, 'theta', exp_path) # saving the parameters\n",
    "# jnp.save(os.path.join(exp_path, 'xs'), xs)\n",
    "# jnp.save(os.path.join(exp_path, 'ys'), ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, ys = p.sample_multiple_sequences(key_sequences, \n",
    "                                     theta, \n",
    "                                     p_args.num_seqs, \n",
    "                                     p_args.seq_length,\n",
    "                                     single_split_seq=False, # not sampling a sequence and splitting into subsequences\n",
    "                                     load_from='', # not loading from external folder\n",
    "                                     loaded_seq=False) # not loading sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up variational model and training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_args = argparse.Namespace() # will contain all configuration information for the variational model \n",
    "q_args.state_dim, q_args.obs_dim = p_args.state_dim, p_args.obs_dim \n",
    "q_args.model = 'linear' \n",
    "q_args.emission_bias = False \n",
    "q_args.transition_bias = False \n",
    "q_args.transition_matrix_conditionning = 'diagonal' \n",
    "q_args.range_transition_map_params = [0.8,0.9] \n",
    "q_args = set_defaults(q_args) \n",
    "# sub_exp_path = os.path.join(exp_path, q_args.model)\n",
    "# os.makedirs(sub_exp_path, exist_ok=True)\n",
    "# save_args(p_args, 'args', sub_exp_path) # saving the params of the generative model\n",
    "q = get_variational_model(q_args) # getting the corresponding model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning in an offline setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "for elbo_mode in ['closed_form', 'autodiff_on_batch', 'score,paris,bptt_depth_2']:\n",
    "    trainer = SVITrainer(p=p,\n",
    "                        theta_star=theta,\n",
    "                        q=q,\n",
    "                        optimizer='adam',\n",
    "                        learning_rate=1e-3, \n",
    "                        optim_options='cst', # learning rate schedule\n",
    "                        num_epochs=1000, # number of full sweeps through the sequence\n",
    "                        seq_length=p_args.seq_length,\n",
    "                        num_samples=2, # number of monte carlo samples (or trajectories in the offline case)\n",
    "                        frozen_params='', # which parameters to hold fixed\n",
    "                        num_seqs=p_args.num_seqs, \n",
    "                        training_mode=f'reset,{p_args.seq_length},1', \n",
    "                        elbo_mode=elbo_mode,\n",
    "                        logging_type='basic_logging')\n",
    "\n",
    "\n",
    "    key_init_params, key_montecarlo = jax.random.split(key, 2)\n",
    "    fitted_variational_params, elbos = trainer.fit(key_init_params, \n",
    "                                    key_montecarlo, \n",
    "                                    data=(xs, ys), \n",
    "                                    log_writer=None, \n",
    "                                    args=None, \n",
    "                                    log_writer_monitor=None)\n",
    "\n",
    "    elbo_for_all_epochs = elbos.flatten()\n",
    "    plt.plot(elbo_for_all_epochs, label=elbo_mode)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fitted_variational_params)\n",
    "x_smoothed_kalman = p.smooth_seq(ys[0], theta)[0]\n",
    "x_smoothed_variational = q.smooth_seq(ys[0], fitted_variational_params)[0]\n",
    "plot_x_true_against_x_pred(x_smoothed_kalman, x_smoothed_variational)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_args.seq_length = 100_000 # keeping all previous settings except the sequence length\n",
    "\n",
    "xs, ys = p.sample_multiple_sequences(key_sequences, \n",
    "                                     theta, \n",
    "                                     num_seqs=1, \n",
    "                                     seq_length=p_args.seq_length,\n",
    "                                     single_split_seq=False,\n",
    "                                     load_from='')\n",
    "\n",
    "\n",
    "elbo_mode = 'score,paris,bptt_depth_2'\n",
    "trainer = SVITrainer(p=p,\n",
    "                    theta_star=theta,\n",
    "                    q=q,\n",
    "                    optimizer='adam',\n",
    "                    learning_rate=1e-6, \n",
    "                    optim_options='cst', # learning rate schedule\n",
    "                    num_epochs=1, # number of full sweeps through the sequence\n",
    "                    seq_length=p_args.seq_length,\n",
    "                    num_samples=10, # number of monte carlo samples (or trajectories in the online case)\n",
    "                    frozen_params='', # which parameters to hold fixed\n",
    "                    num_seqs=p_args.num_seqs, \n",
    "                    training_mode=f'streaming,1,difference', # L_t - L_{t-1} / t\n",
    "                    elbo_mode=elbo_mode,\n",
    "                    logging_type='basic_logging')\n",
    "\n",
    "key_init_params, key_montecarlo = jax.random.split(key, 2)\n",
    "fitted_variational_params, elbos = trainer.fit(key_init_params, \n",
    "                                key_montecarlo, \n",
    "                                data=(xs, ys), \n",
    "                                log_writer=None, \n",
    "                                args=None, \n",
    "                                log_writer_monitor=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo_for_all_epochs = elbos.flatten()\n",
    "plt.plot(elbo_for_all_epochs, label=elbo_mode)\n",
    "plt.legend()\n",
    "\n",
    "x_smoothed_kalman = p.smooth_seq(ys[0], theta)[0]\n",
    "x_smoothed_variational = q.smooth_seq(ys[0], fitted_variational_params)[0]\n",
    "plot_x_true_against_x_pred(x_smoothed_kalman, x_smoothed_variational)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chaotic RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_args = argparse.Namespace()\n",
    "\n",
    "p_args.state_dim, p_args.obs_dim = 5,5 \n",
    "p_args.model = 'chaotic_rnn' \n",
    "p_args.seq_length = 500_000\n",
    "p_args.loaded_seq = True\n",
    "p_args.load_from = 'data/crnn/2022-10-18_15-28-00_Train_run'\n",
    "p_args.range_transition_map_params = [-1,1] # range of the components of the transition matrix\n",
    "p_args.transition_matrix_conditionning = 'init_scale_by_dim' # constraint\n",
    "p_args.default_transition_matrix = os.path.join(p_args.load_from, 'W.npy')\n",
    "\n",
    "p_args.gamma = 2.5 # gamma for the chaotic rnn\n",
    "p_args.tau = 0.025 # tau for the chaotic rnn\n",
    "p_args.grid_size = 0.001 # discretization parameter for the chaotic rnn\n",
    "\n",
    "p_args.emission_matrix_conditionning = 'diagonal'\n",
    "p_args.range_emission_map_params = (-1,1)\n",
    "p_args.default_emission_df = 2 # degrees of freedom for the emission noise\n",
    "p_args.default_emission_matrix = 1.0 # diagonal values for the emission matrix\n",
    "p_args.transition_bias = False \n",
    "p_args.emission_bias = False\n",
    "p_args.num_seqs = 1 #\n",
    "\n",
    "p_args.default_prior_mean = 0.0 # default value for the mean of Gaussian prior\n",
    "p_args.default_prior_base_scale = math.sqrt(0.1) # default value for the diagonal components of the covariance matrix of the prior\n",
    "p_args.default_transition_base_scale = math.sqrt(0.1) # default value for the diagonal components of the covariance matrix of the transition kernel\n",
    "p_args.default_transition_bias = 0.0\n",
    "p_args.default_emission_base_scale = math.sqrt(0.1)\n",
    "p_args.num_particles, p_args.num_smooth_particles = None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "key, key_params, key_sequences = jax.random.split(key, 3)\n",
    "p, theta = get_generative_model(p_args, key_params)\n",
    "xs, ys = p.sample_multiple_sequences(key_sequences, \n",
    "                                     theta, \n",
    "                                     1, \n",
    "                                     p_args.seq_length,\n",
    "                                     single_split_seq=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_args = argparse.Namespace()\n",
    "q_args.model = 'johnson_backward,100'\n",
    "q_args.state_dim, q_args.obs_dim = p_args.state_dim, p_args.obs_dim\n",
    "q_args.transition_bias = False\n",
    "q_args.emission_bias = False\n",
    "layers = [int(nb) for nb in q_args.model.split(',')[-1].split('_')]\n",
    "q_args.update_layers = (*layers,)\n",
    "q_args.backwd_layers = 0\n",
    "q_args.transition_matrix_conditionning = 'diagonal'\n",
    "q_args.range_transition_map_params = (0.8, 1)\n",
    "q_args.anisotropic = False\n",
    "\n",
    "q_args = set_defaults(q_args)\n",
    "\n",
    "q = get_variational_model(q_args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "for elbo_mode in ['score,paris,bptt_depth_2']:\n",
    "    trainer = SVITrainer(p=p,\n",
    "                        theta_star=theta,\n",
    "                        q=q,\n",
    "                        optimizer='adam',\n",
    "                        learning_rate=1e-3, \n",
    "                        optim_options='cst', # learning rate schedule\n",
    "                        num_epochs=1, # number of full sweeps through the sequence\n",
    "                        seq_length=p_args.seq_length,\n",
    "                        num_samples=2, # number of monte carlo samples (or trajectories in the offline case)\n",
    "                        frozen_params='', # which parameters to hold fixed\n",
    "                        num_seqs=p_args.num_seqs, \n",
    "                        training_mode='accumulate,1000,1', \n",
    "                        elbo_mode=elbo_mode,\n",
    "                        logging_type='basic_logging')\n",
    "\n",
    "\n",
    "    key_init_params, key_montecarlo = jax.random.split(key, 2)\n",
    "    fitted_variational_params, elbos = trainer.fit(key_init_params, \n",
    "                                    key_montecarlo, \n",
    "                                    data=(xs, ys), \n",
    "                                    log_writer=None, \n",
    "                                    args=None, \n",
    "                                    log_writer_monitor=None)\n",
    "\n",
    "    elbo_for_all_epochs = elbos.flatten()\n",
    "    plt.plot(elbo_for_all_epochs, label=elbo_mode)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_smoothed = q.smooth_seq(ys[0], fitted_variational_params)[0]\n",
    "plot_x_true_against_x_pred(x_true=xs[0], \n",
    "                           x_pred=x_smoothed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
